{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 导入神经网络中间层特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-24T09:50:42.368488500Z",
     "start_time": "2023-07-24T09:50:42.342511700Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 调参随机种子设置\n",
    "import random\n",
    "import numpy as np\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "# 定义数据集类\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = torch.tensor(data, dtype=torch.float32)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index], self.labels[index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从Excel读取数据集\n",
    "data_df = pd.read_excel('Pb1.xlsx', sheet_name='Sheet1',header=None)\n",
    "data = data_df.iloc[0:, :2048].values\n",
    "label_df = pd.read_excel('Pb1.xlsx',sheet_name='Sheet2',header=None)\n",
    "labels = label_df.iloc[0:, 0].values\n",
    "\n",
    "# 数据预处理：标准化\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "data_normalized = scaler.fit_transform(data)\n",
    "\n",
    "# 数据预处理：归一化\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "min_max_scaler = MinMaxScaler()\n",
    "normalized_data = min_max_scaler.fit_transform(data)\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "# 创建PCA模型并降维到十六维\n",
    "pca = PCA(n_components=5)\n",
    "data_pca = pca.fit_transform(data_normalized)\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "# 选择最好的K个特征\n",
    "k_best = SelectKBest(score_func=f_regression, k=15)\n",
    "data_selected = k_best.fit_transform(data_normalized, labels)\n",
    "\n",
    "merged_matrix = np.hstack((data_pca, data_selected))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(57,)\n",
      "(57, 20)\n"
     ]
    }
   ],
   "source": [
    "# 假设data是包含特征数据的数组，labels是包含对应标签的数组\n",
    "data_train, data_test, labels_train, labels_test = train_test_split(merged_matrix, labels, test_size=0.1, random_state=42)\n",
    "\n",
    "data_train_tensor = torch.tensor(data_train, dtype=torch.float32).clone().detach()\n",
    "labels_train_tensor = torch.tensor(labels_train, dtype=torch.float32).clone().detach()\n",
    "data_test_tensor = torch.tensor(data_test, dtype=torch.float32).clone().detach()\n",
    "labels_test_tensor = torch.tensor(labels_test, dtype=torch.float32).clone().detach()\n",
    "\n",
    "batch_size = 57\n",
    "\n",
    "# 准备数据\n",
    "train_dataset = CustomDataset(data_train, labels_train)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "print(labels.shape)\n",
    "print(merged_matrix.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-24T09:51:03.054687400Z",
     "start_time": "2023-07-24T09:51:03.039550800Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 定义神经网络模型\n",
    "# 利用神经网络提取中间层特征\n",
    "# 但是有一个问题是为什么要用这样一个三个线形层的神经网络：\n",
    "# 前馈神经网络：一个输入层，两个隐藏层和一个输出层\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(20, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        intermediate_features = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(intermediate_features))  # 提取中间层特征\n",
    "        x = self.fc3(x)\n",
    "        return x, intermediate_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-24T09:51:04.459356600Z",
     "start_time": "2023-07-24T09:51:03.909834700Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/308], Loss: 158302.5, Train RMSE: 397.27398681640625, Train R^2: -0.10150604778481886\n",
      "Epoch [2/308], Loss: 157826.609375, Train RMSE: 396.7463073730469, Train R^2: -0.09858190116005039\n",
      "Epoch [3/308], Loss: 157407.640625, Train RMSE: 396.2071838378906, Train R^2: -0.09559842966344712\n",
      "Epoch [4/308], Loss: 156980.15625, Train RMSE: 395.53753662109375, Train R^2: -0.09189780974872064\n",
      "Epoch [5/308], Loss: 156449.90625, Train RMSE: 394.73590087890625, Train R^2: -0.08747638939868896\n",
      "Epoch [6/308], Loss: 155816.40625, Train RMSE: 393.7745056152344, Train R^2: -0.08218563964463343\n",
      "Epoch [7/308], Loss: 155058.328125, Train RMSE: 392.6207580566406, Train R^2: -0.07585353894298752\n",
      "Epoch [8/308], Loss: 154151.046875, Train RMSE: 391.26153564453125, Train R^2: -0.06841729270017449\n",
      "Epoch [9/308], Loss: 153085.578125, Train RMSE: 389.6529541015625, Train R^2: -0.059650296690682\n",
      "Epoch [10/308], Loss: 151829.421875, Train RMSE: 387.7790222167969, Train R^2: -0.049482548312000274\n",
      "Epoch [11/308], Loss: 150372.5625, Train RMSE: 385.5940246582031, Train R^2: -0.037689101439140726\n",
      "Epoch [12/308], Loss: 148682.765625, Train RMSE: 383.0549011230469, Train R^2: -0.02406773952850183\n",
      "Epoch [13/308], Loss: 146731.0625, Train RMSE: 380.12872314453125, Train R^2: -0.008481743660177177\n",
      "Epoch [14/308], Loss: 144497.859375, Train RMSE: 376.7854309082031, Train R^2: 0.009179761552064125\n",
      "Epoch [15/308], Loss: 141967.28125, Train RMSE: 372.9707946777344, Train R^2: 0.029140762866753267\n",
      "Epoch [16/308], Loss: 139107.21875, Train RMSE: 368.6447448730469, Train R^2: 0.05153169748669029\n",
      "Epoch [17/308], Loss: 135898.984375, Train RMSE: 363.77301025390625, Train R^2: 0.0764346853385951\n",
      "Epoch [18/308], Loss: 132330.8125, Train RMSE: 358.3126525878906, Train R^2: 0.10395270245164856\n",
      "Epoch [19/308], Loss: 128387.953125, Train RMSE: 352.2216491699219, Train R^2: 0.13415784670444275\n",
      "Epoch [20/308], Loss: 124060.1015625, Train RMSE: 345.4475402832031, Train R^2: 0.16714207161061378\n",
      "Epoch [21/308], Loss: 119334.03125, Train RMSE: 337.9056701660156, Train R^2: 0.2031115382069616\n",
      "Epoch [22/308], Loss: 114180.234375, Train RMSE: 329.447509765625, Train R^2: 0.2425060660344226\n",
      "Epoch [23/308], Loss: 108535.6640625, Train RMSE: 319.974365234375, Train R^2: 0.2854428106649397\n",
      "Epoch [24/308], Loss: 102383.6015625, Train RMSE: 309.4864807128906, Train R^2: 0.3315175628594116\n",
      "Epoch [25/308], Loss: 95781.8828125, Train RMSE: 297.88116455078125, Train R^2: 0.3807119364418443\n",
      "Epoch [26/308], Loss: 88733.1875, Train RMSE: 285.1364440917969, Train R^2: 0.4325702929030102\n",
      "Epoch [27/308], Loss: 81302.7890625, Train RMSE: 271.2305603027344, Train R^2: 0.48656685385800236\n",
      "Epoch [28/308], Loss: 73566.0234375, Train RMSE: 256.20025634765625, Train R^2: 0.5418942145593348\n",
      "Epoch [29/308], Loss: 65638.5703125, Train RMSE: 240.11972045898438, Train R^2: 0.5975960005315738\n",
      "Epoch [30/308], Loss: 57657.48046875, Train RMSE: 223.03768920898438, Train R^2: 0.652813239833161\n",
      "Epoch [31/308], Loss: 49745.8125, Train RMSE: 205.09393310546875, Train R^2: 0.7064296296943181\n",
      "Epoch [32/308], Loss: 42063.515625, Train RMSE: 186.640869140625, Train R^2: 0.7568803045596557\n",
      "Epoch [33/308], Loss: 34834.8125, Train RMSE: 168.1526641845703, Train R^2: 0.8026604194797522\n",
      "Epoch [34/308], Loss: 28275.32421875, Train RMSE: 150.3101348876953, Train R^2: 0.8423176315705092\n",
      "Epoch [35/308], Loss: 22593.1328125, Train RMSE: 134.23497009277344, Train R^2: 0.8742413097497069\n",
      "Epoch [36/308], Loss: 18019.02734375, Train RMSE: 121.08081817626953, Train R^2: 0.8976807629417421\n",
      "Epoch [37/308], Loss: 14660.5625, Train RMSE: 112.06403350830078, Train R^2: 0.9123525737082229\n",
      "Epoch [38/308], Loss: 12558.34765625, Train RMSE: 107.46446228027344, Train R^2: 0.9193997521931466\n",
      "Epoch [39/308], Loss: 11548.6103515625, Train RMSE: 106.51424407958984, Train R^2: 0.9208188191718907\n",
      "Epoch [40/308], Loss: 11345.283203125, Train RMSE: 107.94703674316406, Train R^2: 0.9186742473956504\n",
      "Epoch [41/308], Loss: 11652.5654296875, Train RMSE: 109.8021469116211, Train R^2: 0.9158550025387355\n",
      "Epoch [42/308], Loss: 12056.5126953125, Train RMSE: 110.61651611328125, Train R^2: 0.9146022221515893\n",
      "Epoch [43/308], Loss: 12236.0126953125, Train RMSE: 109.3756103515625, Train R^2: 0.9165074908648297\n",
      "Epoch [44/308], Loss: 11963.0224609375, Train RMSE: 105.9365234375, Train R^2: 0.9216754248402835\n",
      "Epoch [45/308], Loss: 11222.546875, Train RMSE: 100.53486633300781, Train R^2: 0.9294592493392531\n",
      "Epoch [46/308], Loss: 10107.2587890625, Train RMSE: 93.72265625, Train R^2: 0.9386950166512057\n",
      "Epoch [47/308], Loss: 8783.935546875, Train RMSE: 86.30687713623047, Train R^2: 0.948012686139598\n",
      "Epoch [48/308], Loss: 7448.87548828125, Train RMSE: 79.28205871582031, Train R^2: 0.9561311289545567\n",
      "Epoch [49/308], Loss: 6285.64453125, Train RMSE: 73.408447265625, Train R^2: 0.9623903968946326\n",
      "Epoch [50/308], Loss: 5388.80029296875, Train RMSE: 69.27865600585938, Train R^2: 0.9665030326793255\n",
      "Epoch [51/308], Loss: 4799.53173828125, Train RMSE: 67.08942413330078, Train R^2: 0.9685866135474993\n",
      "Epoch [52/308], Loss: 4500.99072265625, Train RMSE: 66.65640258789062, Train R^2: 0.9689908146990349\n",
      "Epoch [53/308], Loss: 4443.0751953125, Train RMSE: 67.41719055175781, Train R^2: 0.9682789267072249\n",
      "Epoch [54/308], Loss: 4545.07666015625, Train RMSE: 68.69680786132812, Train R^2: 0.9670633302498285\n",
      "Epoch [55/308], Loss: 4719.25048828125, Train RMSE: 69.95967102050781, Train R^2: 0.9658412337306853\n",
      "Epoch [56/308], Loss: 4894.35595703125, Train RMSE: 70.82310485839844, Train R^2: 0.9649928644062463\n",
      "Epoch [57/308], Loss: 5015.91259765625, Train RMSE: 71.08789825439453, Train R^2: 0.9647306120081702\n",
      "Epoch [58/308], Loss: 5053.48876953125, Train RMSE: 70.71820068359375, Train R^2: 0.9650964960357259\n",
      "Epoch [59/308], Loss: 5001.06396484375, Train RMSE: 69.76317596435547, Train R^2: 0.9660328503497876\n",
      "Epoch [60/308], Loss: 4866.90087890625, Train RMSE: 68.38339233398438, Train R^2: 0.9673631738117674\n",
      "Epoch [61/308], Loss: 4676.28857421875, Train RMSE: 66.79071807861328, Train R^2: 0.9688657132460625\n",
      "Epoch [62/308], Loss: 4461.0009765625, Train RMSE: 65.17887878417969, Train R^2: 0.9703502877219978\n",
      "Epoch [63/308], Loss: 4248.28759765625, Train RMSE: 63.740631103515625, Train R^2: 0.9716443675649706\n",
      "Epoch [64/308], Loss: 4062.86767578125, Train RMSE: 62.62016296386719, Train R^2: 0.9726325099446886\n",
      "Epoch [65/308], Loss: 3921.284423828125, Train RMSE: 61.88835525512695, Train R^2: 0.9732684242220061\n",
      "Epoch [66/308], Loss: 3830.169189453125, Train RMSE: 61.525848388671875, Train R^2: 0.9735806667354078\n",
      "Epoch [67/308], Loss: 3785.43017578125, Train RMSE: 61.431800842285156, Train R^2: 0.9736613730978846\n",
      "Epoch [68/308], Loss: 3773.8662109375, Train RMSE: 61.47922897338867, Train R^2: 0.9736206892204912\n",
      "Epoch [69/308], Loss: 3779.695068359375, Train RMSE: 61.5147819519043, Train R^2: 0.9735901742463686\n",
      "Epoch [70/308], Loss: 3784.067626953125, Train RMSE: 61.40337371826172, Train R^2: 0.9736857442472036\n",
      "Epoch [71/308], Loss: 3770.374755859375, Train RMSE: 61.09135055541992, Train R^2: 0.9739524970923956\n",
      "Epoch [72/308], Loss: 3732.153076171875, Train RMSE: 60.58295440673828, Train R^2: 0.9743842239851426\n",
      "Epoch [73/308], Loss: 3670.294189453125, Train RMSE: 59.92753219604492, Train R^2: 0.974935477074895\n",
      "Epoch [74/308], Loss: 3591.309326171875, Train RMSE: 59.19758987426758, Train R^2: 0.9755423513918486\n",
      "Epoch [75/308], Loss: 3504.354736328125, Train RMSE: 58.480159759521484, Train R^2: 0.9761315788562726\n",
      "Epoch [76/308], Loss: 3419.9287109375, Train RMSE: 57.873714447021484, Train R^2: 0.9766240471867056\n",
      "Epoch [77/308], Loss: 3349.36669921875, Train RMSE: 57.423770904541016, Train R^2: 0.9769861072453138\n",
      "Epoch [78/308], Loss: 3297.489990234375, Train RMSE: 57.109100341796875, Train R^2: 0.9772376428666572\n",
      "Epoch [79/308], Loss: 3261.44873046875, Train RMSE: 56.893795013427734, Train R^2: 0.9774089501909083\n",
      "Epoch [80/308], Loss: 3236.903564453125, Train RMSE: 56.727142333984375, Train R^2: 0.9775411015096436\n",
      "Epoch [81/308], Loss: 3217.968505859375, Train RMSE: 56.564388275146484, Train R^2: 0.977669789091494\n",
      "Epoch [82/308], Loss: 3199.530029296875, Train RMSE: 56.347957611083984, Train R^2: 0.9778403432696549\n",
      "Epoch [83/308], Loss: 3175.0927734375, Train RMSE: 56.055442810058594, Train R^2: 0.9780698210247196\n",
      "Epoch [84/308], Loss: 3142.21240234375, Train RMSE: 55.683677673339844, Train R^2: 0.9783597400632502\n",
      "Epoch [85/308], Loss: 3100.671875, Train RMSE: 55.24907684326172, Train R^2: 0.978696220342991\n",
      "Epoch [86/308], Loss: 3052.46044921875, Train RMSE: 54.797096252441406, Train R^2: 0.9790433568273493\n",
      "Epoch [87/308], Loss: 3002.721923828125, Train RMSE: 54.3910026550293, Train R^2: 0.9793528215142955\n",
      "Epoch [88/308], Loss: 2958.380859375, Train RMSE: 53.997249603271484, Train R^2: 0.9796506782554982\n",
      "Epoch [89/308], Loss: 2915.703125, Train RMSE: 53.6877326965332, Train R^2: 0.9798832955707008\n",
      "Epoch [90/308], Loss: 2882.373046875, Train RMSE: 53.43254470825195, Train R^2: 0.9800740818258081\n",
      "Epoch [91/308], Loss: 2855.036865234375, Train RMSE: 53.21229553222656, Train R^2: 0.9802380143511593\n",
      "Epoch [92/308], Loss: 2831.54833984375, Train RMSE: 53.005271911621094, Train R^2: 0.9803914829381722\n",
      "Epoch [93/308], Loss: 2809.558837890625, Train RMSE: 52.791988372802734, Train R^2: 0.9805489681315332\n",
      "Epoch [94/308], Loss: 2786.994140625, Train RMSE: 52.55656433105469, Train R^2: 0.9807220620721602\n",
      "Epoch [95/308], Loss: 2762.1923828125, Train RMSE: 52.2869873046875, Train R^2: 0.9809193192266359\n",
      "Epoch [96/308], Loss: 2733.92919921875, Train RMSE: 51.99701690673828, Train R^2: 0.9811303654398676\n",
      "Epoch [97/308], Loss: 2703.689697265625, Train RMSE: 51.69328689575195, Train R^2: 0.9813501679221983\n",
      "Epoch [98/308], Loss: 2672.196044921875, Train RMSE: 51.38205337524414, Train R^2: 0.9815740647630223\n",
      "Epoch [99/308], Loss: 2640.115234375, Train RMSE: 51.07618713378906, Train R^2: 0.9817927799566784\n",
      "Epoch [100/308], Loss: 2608.77685546875, Train RMSE: 50.79014205932617, Train R^2: 0.9819961461013398\n",
      "Epoch [101/308], Loss: 2579.637939453125, Train RMSE: 50.52276611328125, Train R^2: 0.982185200966459\n",
      "Epoch [102/308], Loss: 2552.55029296875, Train RMSE: 50.2606201171875, Train R^2: 0.982369593002669\n",
      "Epoch [103/308], Loss: 2526.130126953125, Train RMSE: 49.998268127441406, Train R^2: 0.9825531690879935\n",
      "Epoch [104/308], Loss: 2499.826171875, Train RMSE: 49.7340087890625, Train R^2: 0.9827371067667509\n",
      "Epoch [105/308], Loss: 2473.471923828125, Train RMSE: 49.46179962158203, Train R^2: 0.9829255579362066\n",
      "Epoch [106/308], Loss: 2446.469970703125, Train RMSE: 49.188419342041016, Train R^2: 0.9831137817402426\n",
      "Epoch [107/308], Loss: 2419.500732421875, Train RMSE: 48.916446685791016, Train R^2: 0.9833000012088929\n",
      "Epoch [108/308], Loss: 2392.81884765625, Train RMSE: 48.64088439941406, Train R^2: 0.9834876236149006\n",
      "Epoch [109/308], Loss: 2365.935546875, Train RMSE: 48.36259460449219, Train R^2: 0.9836760301400194\n",
      "Epoch [110/308], Loss: 2338.940673828125, Train RMSE: 48.08013916015625, Train R^2: 0.983866147247542\n",
      "Epoch [111/308], Loss: 2311.69970703125, Train RMSE: 47.80612564086914, Train R^2: 0.9840495212555788\n",
      "Epoch [112/308], Loss: 2285.42578125, Train RMSE: 47.5385856628418, Train R^2: 0.9842275492017497\n",
      "Epoch [113/308], Loss: 2259.917236328125, Train RMSE: 47.27462387084961, Train R^2: 0.9844022222669222\n",
      "Epoch [114/308], Loss: 2234.8896484375, Train RMSE: 47.01213455200195, Train R^2: 0.9845749497878229\n",
      "Epoch [115/308], Loss: 2210.140869140625, Train RMSE: 46.74641036987305, Train R^2: 0.9847488289227257\n",
      "Epoch [116/308], Loss: 2185.227294921875, Train RMSE: 46.47822189331055, Train R^2: 0.9849233214195079\n",
      "Epoch [117/308], Loss: 2160.225341796875, Train RMSE: 46.20540237426758, Train R^2: 0.9850997987745241\n",
      "Epoch [118/308], Loss: 2134.93896484375, Train RMSE: 45.92839431762695, Train R^2: 0.9852779224000899\n",
      "Epoch [119/308], Loss: 2109.417236328125, Train RMSE: 45.652923583984375, Train R^2: 0.9854539927906475\n",
      "Epoch [120/308], Loss: 2084.189453125, Train RMSE: 45.380802154541016, Train R^2: 0.9856268845764685\n",
      "Epoch [121/308], Loss: 2059.4169921875, Train RMSE: 45.113033294677734, Train R^2: 0.9857960003634602\n",
      "Epoch [122/308], Loss: 2035.1856689453125, Train RMSE: 44.853126525878906, Train R^2: 0.9859591920756062\n",
      "Epoch [123/308], Loss: 2011.8031005859375, Train RMSE: 44.59153366088867, Train R^2: 0.9861224946724917\n",
      "Epoch [124/308], Loss: 1988.404541015625, Train RMSE: 44.32996368408203, Train R^2: 0.986284823859338\n",
      "Epoch [125/308], Loss: 1965.1455078125, Train RMSE: 44.0733757019043, Train R^2: 0.9864431353463111\n",
      "Epoch [126/308], Loss: 1942.4622802734375, Train RMSE: 43.81642532348633, Train R^2: 0.9866007489290342\n",
      "Epoch [127/308], Loss: 1919.879150390625, Train RMSE: 43.55412673950195, Train R^2: 0.9867606924579724\n",
      "Epoch [128/308], Loss: 1896.9620361328125, Train RMSE: 43.2889289855957, Train R^2: 0.9869214276307612\n",
      "Epoch [129/308], Loss: 1873.931640625, Train RMSE: 43.036521911621094, Train R^2: 0.9870734989284181\n",
      "Epoch [130/308], Loss: 1852.142578125, Train RMSE: 42.786067962646484, Train R^2: 0.9872235132341072\n",
      "Epoch [131/308], Loss: 1830.6478271484375, Train RMSE: 42.52643585205078, Train R^2: 0.9873781032944817\n",
      "Epoch [132/308], Loss: 1808.497802734375, Train RMSE: 42.27354049682617, Train R^2: 0.9875277766642258\n",
      "Epoch [133/308], Loss: 1787.05224609375, Train RMSE: 42.02632141113281, Train R^2: 0.9876732282018249\n",
      "Epoch [134/308], Loss: 1766.211669921875, Train RMSE: 41.78440475463867, Train R^2: 0.9878147313264011\n",
      "Epoch [135/308], Loss: 1745.9365234375, Train RMSE: 41.54038619995117, Train R^2: 0.9879566394822035\n",
      "Epoch [136/308], Loss: 1725.603759765625, Train RMSE: 41.306007385253906, Train R^2: 0.9880921562336934\n",
      "Epoch [137/308], Loss: 1706.1865234375, Train RMSE: 41.06753158569336, Train R^2: 0.9882292580050014\n",
      "Epoch [138/308], Loss: 1686.5421142578125, Train RMSE: 40.83340072631836, Train R^2: 0.9883630890502579\n",
      "Epoch [139/308], Loss: 1667.366455078125, Train RMSE: 40.60303497314453, Train R^2: 0.9884940192655761\n",
      "Epoch [140/308], Loss: 1648.6064453125, Train RMSE: 40.4079704284668, Train R^2: 0.9886043084578297\n",
      "Epoch [141/308], Loss: 1632.803955078125, Train RMSE: 40.15154266357422, Train R^2: 0.9887484812917171\n",
      "Epoch [142/308], Loss: 1612.1466064453125, Train RMSE: 39.93376541137695, Train R^2: 0.9888702051188357\n",
      "Epoch [143/308], Loss: 1594.70556640625, Train RMSE: 39.71406555175781, Train R^2: 0.9889923310925783\n",
      "Epoch [144/308], Loss: 1577.2071533203125, Train RMSE: 39.493980407714844, Train R^2: 0.9891139971168199\n",
      "Epoch [145/308], Loss: 1559.774658203125, Train RMSE: 39.278900146484375, Train R^2: 0.9892322437480922\n",
      "Epoch [146/308], Loss: 1542.831787109375, Train RMSE: 39.06425094604492, Train R^2: 0.9893496065475695\n",
      "Epoch [147/308], Loss: 1526.0157470703125, Train RMSE: 38.849205017089844, Train R^2: 0.9894665432273596\n",
      "Epoch [148/308], Loss: 1509.2607421875, Train RMSE: 38.635520935058594, Train R^2: 0.9895820991158586\n",
      "Epoch [149/308], Loss: 1492.70361328125, Train RMSE: 38.41986846923828, Train R^2: 0.989698074757025\n",
      "Epoch [150/308], Loss: 1476.0863037109375, Train RMSE: 38.21916198730469, Train R^2: 0.9898054301262043\n",
      "Epoch [151/308], Loss: 1460.704345703125, Train RMSE: 38.01216506958008, Train R^2: 0.9899155577411605\n",
      "Epoch [152/308], Loss: 1444.9248046875, Train RMSE: 37.81892013549805, Train R^2: 0.9900178329402683\n",
      "Epoch [153/308], Loss: 1430.2706298828125, Train RMSE: 37.62382507324219, Train R^2: 0.9901205562427554\n",
      "Epoch [154/308], Loss: 1415.5521240234375, Train RMSE: 37.426387786865234, Train R^2: 0.9902239727207613\n",
      "Epoch [155/308], Loss: 1400.734375, Train RMSE: 37.224815368652344, Train R^2: 0.9903289907514395\n",
      "Epoch [156/308], Loss: 1385.68701171875, Train RMSE: 37.021175384521484, Train R^2: 0.9904345137932281\n",
      "Epoch [157/308], Loss: 1370.5673828125, Train RMSE: 36.81697082519531, Train R^2: 0.9905397491676224\n",
      "Epoch [158/308], Loss: 1355.4891357421875, Train RMSE: 36.62360763549805, Train R^2: 0.9906388569323533\n",
      "Epoch [159/308], Loss: 1341.28857421875, Train RMSE: 36.463069915771484, Train R^2: 0.990720746302588\n",
      "Epoch [160/308], Loss: 1329.555419921875, Train RMSE: 36.23143005371094, Train R^2: 0.9908382676403807\n",
      "Epoch [161/308], Loss: 1312.716552734375, Train RMSE: 36.04758071899414, Train R^2: 0.9909310101416487\n",
      "Epoch [162/308], Loss: 1299.4281005859375, Train RMSE: 35.84125900268555, Train R^2: 0.9910345283978116\n",
      "Epoch [163/308], Loss: 1284.5958251953125, Train RMSE: 35.617061614990234, Train R^2: 0.9911463418548208\n",
      "Epoch [164/308], Loss: 1268.574951171875, Train RMSE: 35.403202056884766, Train R^2: 0.9912523429609748\n",
      "Epoch [165/308], Loss: 1253.38671875, Train RMSE: 35.21430587768555, Train R^2: 0.9913454429015117\n",
      "Epoch [166/308], Loss: 1240.0472412109375, Train RMSE: 34.9931640625, Train R^2: 0.9914538003431478\n",
      "Epoch [167/308], Loss: 1224.521484375, Train RMSE: 34.79187774658203, Train R^2: 0.99155183509166\n",
      "Epoch [168/308], Loss: 1210.4747314453125, Train RMSE: 34.58314895629883, Train R^2: 0.9916528984384166\n",
      "Epoch [169/308], Loss: 1195.994140625, Train RMSE: 34.37267303466797, Train R^2: 0.9917541926809275\n",
      "Epoch [170/308], Loss: 1181.48046875, Train RMSE: 34.17109680175781, Train R^2: 0.991850621426595\n",
      "Epoch [171/308], Loss: 1167.6639404296875, Train RMSE: 33.96141052246094, Train R^2: 0.9919503308087184\n",
      "Epoch [172/308], Loss: 1153.3773193359375, Train RMSE: 33.75067138671875, Train R^2: 0.9920499200804043\n",
      "Epoch [173/308], Loss: 1139.1077880859375, Train RMSE: 33.56132507324219, Train R^2: 0.9921388721500948\n",
      "Epoch [174/308], Loss: 1126.362548828125, Train RMSE: 33.337791442871094, Train R^2: 0.9922432404000169\n",
      "Epoch [175/308], Loss: 1111.4085693359375, Train RMSE: 33.13600158691406, Train R^2: 0.9923368597288136\n",
      "Epoch [176/308], Loss: 1097.994384765625, Train RMSE: 32.9304084777832, Train R^2: 0.9924316543503531\n",
      "Epoch [177/308], Loss: 1084.412109375, Train RMSE: 32.7192268371582, Train R^2: 0.9925284158188263\n",
      "Epoch [178/308], Loss: 1070.5478515625, Train RMSE: 32.512081146240234, Train R^2: 0.9926227219165081\n",
      "Epoch [179/308], Loss: 1057.035400390625, Train RMSE: 32.31321716308594, Train R^2: 0.9927126935875397\n",
      "Epoch [180/308], Loss: 1044.1439208984375, Train RMSE: 32.1093864440918, Train R^2: 0.9928043382652507\n",
      "Epoch [181/308], Loss: 1031.012939453125, Train RMSE: 31.899587631225586, Train R^2: 0.9928980635450518\n",
      "Epoch [182/308], Loss: 1017.5836181640625, Train RMSE: 31.697917938232422, Train R^2: 0.992987577261958\n",
      "Epoch [183/308], Loss: 1004.7579956054688, Train RMSE: 31.491371154785156, Train R^2: 0.9930786669242889\n",
      "Epoch [184/308], Loss: 991.7063598632812, Train RMSE: 31.30585289001465, Train R^2: 0.9931599752256827\n",
      "Epoch [185/308], Loss: 980.056396484375, Train RMSE: 31.134366989135742, Train R^2: 0.993234705274637\n",
      "Epoch [186/308], Loss: 969.348876953125, Train RMSE: 30.954566955566406, Train R^2: 0.9933126194328582\n",
      "Epoch [187/308], Loss: 958.1851196289062, Train RMSE: 30.799711227416992, Train R^2: 0.9933793615230687\n",
      "Epoch [188/308], Loss: 948.6220703125, Train RMSE: 30.60433006286621, Train R^2: 0.9934630915749133\n",
      "Epoch [189/308], Loss: 936.6251831054688, Train RMSE: 30.434478759765625, Train R^2: 0.9935354492557609\n",
      "Epoch [190/308], Loss: 926.2574462890625, Train RMSE: 30.25665855407715, Train R^2: 0.9936107707289412\n",
      "Epoch [191/308], Loss: 915.4652099609375, Train RMSE: 30.085039138793945, Train R^2: 0.9936830451550254\n",
      "Epoch [192/308], Loss: 905.1094360351562, Train RMSE: 29.906599044799805, Train R^2: 0.9937577562552881\n",
      "Epoch [193/308], Loss: 894.4047241210938, Train RMSE: 29.72547149658203, Train R^2: 0.993833139438045\n",
      "Epoch [194/308], Loss: 883.6036376953125, Train RMSE: 29.55248260498047, Train R^2: 0.9939047069310585\n",
      "Epoch [195/308], Loss: 873.3492431640625, Train RMSE: 29.38518524169922, Train R^2: 0.9939735226908825\n",
      "Epoch [196/308], Loss: 863.4891357421875, Train RMSE: 29.214296340942383, Train R^2: 0.9940434127053552\n",
      "Epoch [197/308], Loss: 853.4752807617188, Train RMSE: 29.045400619506836, Train R^2: 0.9941120878795267\n",
      "Epoch [198/308], Loss: 843.6351928710938, Train RMSE: 28.868772506713867, Train R^2: 0.9941834789369436\n",
      "Epoch [199/308], Loss: 833.4059448242188, Train RMSE: 28.690214157104492, Train R^2: 0.9942552082203737\n",
      "Epoch [200/308], Loss: 823.1286010742188, Train RMSE: 28.5135440826416, Train R^2: 0.9943257424537332\n",
      "Epoch [201/308], Loss: 813.022216796875, Train RMSE: 28.342369079589844, Train R^2: 0.9943936669610469\n",
      "Epoch [202/308], Loss: 803.2896728515625, Train RMSE: 28.165306091308594, Train R^2: 0.9944634964320763\n",
      "Epoch [203/308], Loss: 793.2844848632812, Train RMSE: 27.993776321411133, Train R^2: 0.9945307262641156\n",
      "Epoch [204/308], Loss: 783.6515502929688, Train RMSE: 27.823387145996094, Train R^2: 0.9945971037908208\n",
      "Epoch [205/308], Loss: 774.140869140625, Train RMSE: 27.65406036376953, Train R^2: 0.9946626647290138\n",
      "Epoch [206/308], Loss: 764.7471923828125, Train RMSE: 27.48291015625, Train R^2: 0.9947285259750268\n",
      "Epoch [207/308], Loss: 755.3103637695312, Train RMSE: 27.322643280029297, Train R^2: 0.9947898279315046\n",
      "Epoch [208/308], Loss: 746.5267944335938, Train RMSE: 27.15972900390625, Train R^2: 0.9948517751918392\n",
      "Epoch [209/308], Loss: 737.65087890625, Train RMSE: 26.99076271057129, Train R^2: 0.9949156330559712\n",
      "Epoch [210/308], Loss: 728.501220703125, Train RMSE: 26.822813034057617, Train R^2: 0.9949787102776241\n",
      "Epoch [211/308], Loss: 719.4632568359375, Train RMSE: 26.67401885986328, Train R^2: 0.9950342640878524\n",
      "Epoch [212/308], Loss: 711.50341796875, Train RMSE: 26.522171020507812, Train R^2: 0.9950906409265375\n",
      "Epoch [213/308], Loss: 703.4255981445312, Train RMSE: 26.359107971191406, Train R^2: 0.9951508222776105\n",
      "Epoch [214/308], Loss: 694.8026123046875, Train RMSE: 26.196271896362305, Train R^2: 0.9952105503597252\n",
      "Epoch [215/308], Loss: 686.24462890625, Train RMSE: 26.038715362548828, Train R^2: 0.995267990124715\n",
      "Epoch [216/308], Loss: 678.0145263671875, Train RMSE: 25.912046432495117, Train R^2: 0.9953139162406983\n",
      "Epoch [217/308], Loss: 671.4341430664062, Train RMSE: 25.74082374572754, Train R^2: 0.995375641164676\n",
      "Epoch [218/308], Loss: 662.590087890625, Train RMSE: 25.592721939086914, Train R^2: 0.9954287013668365\n",
      "Epoch [219/308], Loss: 654.9874267578125, Train RMSE: 25.441112518310547, Train R^2: 0.9954827019305659\n",
      "Epoch [220/308], Loss: 647.2500610351562, Train RMSE: 25.28470230102539, Train R^2: 0.9955380747429957\n",
      "Epoch [221/308], Loss: 639.316162109375, Train RMSE: 25.143156051635742, Train R^2: 0.995587891462641\n",
      "Epoch [222/308], Loss: 632.17822265625, Train RMSE: 24.99034881591797, Train R^2: 0.9956413567663455\n",
      "Epoch [223/308], Loss: 624.517578125, Train RMSE: 24.832149505615234, Train R^2: 0.995696366831307\n",
      "Epoch [224/308], Loss: 616.6356201171875, Train RMSE: 24.68638038635254, Train R^2: 0.9957467443480827\n",
      "Epoch [225/308], Loss: 609.4174194335938, Train RMSE: 24.54390525817871, Train R^2: 0.9957956975979863\n",
      "Epoch [226/308], Loss: 602.4032592773438, Train RMSE: 24.402374267578125, Train R^2: 0.9958440456560378\n",
      "Epoch [227/308], Loss: 595.4757690429688, Train RMSE: 24.277360916137695, Train R^2: 0.9958865182295022\n",
      "Epoch [228/308], Loss: 589.3902587890625, Train RMSE: 24.119857788085938, Train R^2: 0.995939718734803\n",
      "Epoch [229/308], Loss: 581.7675170898438, Train RMSE: 23.981464385986328, Train R^2: 0.9959861788624697\n",
      "Epoch [230/308], Loss: 575.110595703125, Train RMSE: 23.843769073486328, Train R^2: 0.9960321391763212\n",
      "Epoch [231/308], Loss: 568.5253295898438, Train RMSE: 23.702611923217773, Train R^2: 0.9960789802152695\n",
      "Epoch [232/308], Loss: 561.813720703125, Train RMSE: 23.560575485229492, Train R^2: 0.9961258324146922\n",
      "Epoch [233/308], Loss: 555.1007080078125, Train RMSE: 23.424327850341797, Train R^2: 0.9961705103506228\n",
      "Epoch [234/308], Loss: 548.6991577148438, Train RMSE: 23.287078857421875, Train R^2: 0.9962152550717147\n",
      "Epoch [235/308], Loss: 542.2880249023438, Train RMSE: 23.14864158630371, Train R^2: 0.9962601203188167\n",
      "Epoch [236/308], Loss: 535.859619140625, Train RMSE: 23.011754989624023, Train R^2: 0.9963042201670892\n",
      "Epoch [237/308], Loss: 529.5408325195312, Train RMSE: 22.875778198242188, Train R^2: 0.9963477674041854\n",
      "Epoch [238/308], Loss: 523.30126953125, Train RMSE: 22.737503051757812, Train R^2: 0.9963917872273901\n",
      "Epoch [239/308], Loss: 516.9939575195312, Train RMSE: 22.600370407104492, Train R^2: 0.9964351787417289\n",
      "Epoch [240/308], Loss: 510.77679443359375, Train RMSE: 22.46718406677246, Train R^2: 0.9964770709440859\n",
      "Epoch [241/308], Loss: 504.77435302734375, Train RMSE: 22.335716247558594, Train R^2: 0.9965181794257302\n",
      "Epoch [242/308], Loss: 498.88421630859375, Train RMSE: 22.199438095092773, Train R^2: 0.9965605369806281\n",
      "Epoch [243/308], Loss: 492.8150634765625, Train RMSE: 22.069042205810547, Train R^2: 0.9966008240738928\n",
      "Epoch [244/308], Loss: 487.0426940917969, Train RMSE: 21.93808937072754, Train R^2: 0.9966410448533278\n",
      "Epoch [245/308], Loss: 481.27972412109375, Train RMSE: 21.804183959960938, Train R^2: 0.9966819243875157\n",
      "Epoch [246/308], Loss: 475.42242431640625, Train RMSE: 21.685367584228516, Train R^2: 0.9967179882570789\n",
      "Epoch [247/308], Loss: 470.2550964355469, Train RMSE: 21.556724548339844, Train R^2: 0.9967568117883578\n",
      "Epoch [248/308], Loss: 464.69232177734375, Train RMSE: 21.422510147094727, Train R^2: 0.9967970709966094\n",
      "Epoch [249/308], Loss: 458.9238586425781, Train RMSE: 21.30457305908203, Train R^2: 0.9968322396632167\n",
      "Epoch [250/308], Loss: 453.8847961425781, Train RMSE: 21.185190200805664, Train R^2: 0.9968676421902573\n",
      "Epoch [251/308], Loss: 448.81231689453125, Train RMSE: 21.061019897460938, Train R^2: 0.9969042537181326\n",
      "Epoch [252/308], Loss: 443.5664978027344, Train RMSE: 20.934810638427734, Train R^2: 0.9969412450395376\n",
      "Epoch [253/308], Loss: 438.2662353515625, Train RMSE: 20.812576293945312, Train R^2: 0.9969768596788128\n",
      "Epoch [254/308], Loss: 433.1632995605469, Train RMSE: 20.688343048095703, Train R^2: 0.9970128428048031\n",
      "Epoch [255/308], Loss: 428.007568359375, Train RMSE: 20.568119049072266, Train R^2: 0.9970474601251771\n",
      "Epoch [256/308], Loss: 423.0475158691406, Train RMSE: 20.467065811157227, Train R^2: 0.9970764007401973\n",
      "Epoch [257/308], Loss: 418.90081787109375, Train RMSE: 20.343385696411133, Train R^2: 0.9971116282981729\n",
      "Epoch [258/308], Loss: 413.85333251953125, Train RMSE: 20.229286193847656, Train R^2: 0.9971439374513391\n",
      "Epoch [259/308], Loss: 409.2239685058594, Train RMSE: 20.118896484375, Train R^2: 0.9971750228163062\n",
      "Epoch [260/308], Loss: 404.77001953125, Train RMSE: 20.004716873168945, Train R^2: 0.9972069973387917\n",
      "Epoch [261/308], Loss: 400.1885681152344, Train RMSE: 19.890790939331055, Train R^2: 0.9972387181054779\n",
      "Epoch [262/308], Loss: 395.6435852050781, Train RMSE: 19.77766990661621, Train R^2: 0.9972700359145732\n",
      "Epoch [263/308], Loss: 391.1562805175781, Train RMSE: 19.666282653808594, Train R^2: 0.9973007000350685\n",
      "Epoch [264/308], Loss: 386.76263427734375, Train RMSE: 19.560794830322266, Train R^2: 0.9973295796294921\n",
      "Epoch [265/308], Loss: 382.62469482421875, Train RMSE: 19.461544036865234, Train R^2: 0.997356609983325\n",
      "Epoch [266/308], Loss: 378.7516784667969, Train RMSE: 19.35808563232422, Train R^2: 0.9973846401514042\n",
      "Epoch [267/308], Loss: 374.73553466796875, Train RMSE: 19.249248504638672, Train R^2: 0.9974139660228105\n",
      "Epoch [268/308], Loss: 370.53363037109375, Train RMSE: 19.14314842224121, Train R^2: 0.9974423954654794\n",
      "Epoch [269/308], Loss: 366.46014404296875, Train RMSE: 19.038209915161133, Train R^2: 0.9974703593287052\n",
      "Epoch [270/308], Loss: 362.4534606933594, Train RMSE: 18.935977935791016, Train R^2: 0.9974974537972849\n",
      "Epoch [271/308], Loss: 358.57122802734375, Train RMSE: 18.82796859741211, Train R^2: 0.9975259209883496\n",
      "Epoch [272/308], Loss: 354.492431640625, Train RMSE: 18.732248306274414, Train R^2: 0.9975510131312152\n",
      "Epoch [273/308], Loss: 350.8971252441406, Train RMSE: 18.632139205932617, Train R^2: 0.9975771188600536\n",
      "Epoch [274/308], Loss: 347.15667724609375, Train RMSE: 18.533140182495117, Train R^2: 0.9976027979293458\n",
      "Epoch [275/308], Loss: 343.4772644042969, Train RMSE: 18.431781768798828, Train R^2: 0.9976289469375806\n",
      "Epoch [276/308], Loss: 339.7305603027344, Train RMSE: 18.329402923583984, Train R^2: 0.9976552137904487\n",
      "Epoch [277/308], Loss: 335.9670715332031, Train RMSE: 18.231979370117188, Train R^2: 0.9976800734065395\n",
      "Epoch [278/308], Loss: 332.405029296875, Train RMSE: 18.133617401123047, Train R^2: 0.9977050375553881\n",
      "Epoch [279/308], Loss: 328.8280944824219, Train RMSE: 18.034400939941406, Train R^2: 0.9977300824216166\n",
      "Epoch [280/308], Loss: 325.2396545410156, Train RMSE: 17.936691284179688, Train R^2: 0.997754612909888\n",
      "Epoch [281/308], Loss: 321.724853515625, Train RMSE: 17.838178634643555, Train R^2: 0.997779209183519\n",
      "Epoch [282/308], Loss: 318.20062255859375, Train RMSE: 17.740175247192383, Train R^2: 0.9978035441047496\n",
      "Epoch [283/308], Loss: 314.71380615234375, Train RMSE: 17.649778366088867, Train R^2: 0.9978258718988875\n",
      "Epoch [284/308], Loss: 311.5146789550781, Train RMSE: 17.55706214904785, Train R^2: 0.9978486536875522\n",
      "Epoch [285/308], Loss: 308.25042724609375, Train RMSE: 17.461254119873047, Train R^2: 0.9978720695677424\n",
      "Epoch [286/308], Loss: 304.8953857421875, Train RMSE: 17.367734909057617, Train R^2: 0.9978948020724487\n",
      "Epoch [287/308], Loss: 301.6382141113281, Train RMSE: 17.272192001342773, Train R^2: 0.9979179001709811\n",
      "Epoch [288/308], Loss: 298.32861328125, Train RMSE: 17.179719924926758, Train R^2: 0.9979401349574079\n",
      "Epoch [289/308], Loss: 295.1427307128906, Train RMSE: 17.087278366088867, Train R^2: 0.9979622426412512\n",
      "Epoch [290/308], Loss: 291.9751281738281, Train RMSE: 16.995447158813477, Train R^2: 0.9979840866537145\n",
      "Epoch [291/308], Loss: 288.8452453613281, Train RMSE: 16.91502571105957, Train R^2: 0.9980031202887601\n",
      "Epoch [292/308], Loss: 286.1180725097656, Train RMSE: 16.81153678894043, Train R^2: 0.9980274796409692\n",
      "Epoch [293/308], Loss: 282.6277770996094, Train RMSE: 16.726158142089844, Train R^2: 0.9980474640639168\n",
      "Epoch [294/308], Loss: 279.7643737792969, Train RMSE: 16.637744903564453, Train R^2: 0.9980680513450794\n",
      "Epoch [295/308], Loss: 276.8145751953125, Train RMSE: 16.54823112487793, Train R^2: 0.9980887840931306\n",
      "Epoch [296/308], Loss: 273.8439025878906, Train RMSE: 16.46106719970703, Train R^2: 0.9981088646432724\n",
      "Epoch [297/308], Loss: 270.9667053222656, Train RMSE: 16.36969757080078, Train R^2: 0.9981298006009434\n",
      "Epoch [298/308], Loss: 267.96697998046875, Train RMSE: 16.284482955932617, Train R^2: 0.9981492209308248\n",
      "Epoch [299/308], Loss: 265.18438720703125, Train RMSE: 16.19708251953125, Train R^2: 0.9981690343046914\n",
      "Epoch [300/308], Loss: 262.34552001953125, Train RMSE: 16.111007690429688, Train R^2: 0.9981884429428318\n",
      "Epoch [301/308], Loss: 259.5645446777344, Train RMSE: 16.023530960083008, Train R^2: 0.9982080612736569\n",
      "Epoch [302/308], Loss: 256.7535705566406, Train RMSE: 15.933757781982422, Train R^2: 0.9982280841481309\n",
      "Epoch [303/308], Loss: 253.88467407226562, Train RMSE: 15.843823432922363, Train R^2: 0.9982480301607805\n",
      "Epoch [304/308], Loss: 251.0267333984375, Train RMSE: 15.755167961120605, Train R^2: 0.998267581886081\n",
      "Epoch [305/308], Loss: 248.22531127929688, Train RMSE: 15.664599418640137, Train R^2: 0.9982874421853072\n",
      "Epoch [306/308], Loss: 245.3796844482422, Train RMSE: 15.573670387268066, Train R^2: 0.9983072664898683\n",
      "Epoch [307/308], Loss: 242.5392303466797, Train RMSE: 15.488311767578125, Train R^2: 0.998325771393314\n",
      "Epoch [308/308], Loss: 239.88778686523438, Train RMSE: 15.400617599487305, Train R^2: 0.9983446764480004\n"
     ]
    }
   ],
   "source": [
    "def Set_Random_State():\n",
    "    seed = 42\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    return\n",
    "\n",
    "Set_Random_State()\n",
    "# 存储最后一次训练的隐藏层特征\n",
    "last_hidden_features = None\n",
    "\n",
    "# 设置超参数\n",
    "\n",
    "learning_rate = 0.006\n",
    "batch_size = 57\n",
    "num_epochs = 309\n",
    "\n",
    "# 初始化模型和损失函数\n",
    "model = Net()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# 训练神经网络模型\n",
    "total_step = len(train_dataloader)\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    for inputs, targets in train_dataloader:\n",
    "        outputs, intermediate_features = model(inputs)\n",
    "        loss = criterion(outputs, targets.unsqueeze(1))\n",
    "        loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "        # 计算训练集上的预测精度差\n",
    "        train_outputs,_ = model(data_train_tensor)\n",
    "        train_rmse = mean_squared_error(labels_train_tensor, train_outputs.detach().numpy(), squared=False)\n",
    "        train_r2 = r2_score(labels_train_tensor, train_outputs.detach().numpy())\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item()}, Train RMSE: {train_rmse}, Train R^2: {train_r2}\")\n",
    "        # 提取最后一次训练的隐藏层特征\n",
    "    last_hidden_features = intermediate_features.detach().numpy().astype(np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-24T09:51:06.485738900Z",
     "start_time": "2023-07-24T09:51:06.473272600Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 75.9813    47.88044  628.3527   115.92144    6.569015   4.629687]\n",
      "[ 85.  19. 636.  79.  21.  58.]\n",
      "Test RMSE: 29.982589386641166\n",
      "Test R2: 0.9812540090109586\n"
     ]
    }
   ],
   "source": [
    "# 直接通过神经网路进行预测\n",
    "model_path = \"8.11/model.pth\"\n",
    "# 将模型的参数保存到文件中\n",
    "torch.save(model.state_dict(), model_path)\n",
    "model1 = Net()\n",
    "model1.load_state_dict(torch.load(model_path))\n",
    "\n",
    "test_inputs = data_test_tensor\n",
    "test_outputs,_ = model1(test_inputs)\n",
    "test_outputs = test_outputs.detach().numpy()\n",
    "test_outputs = np.squeeze(test_outputs)\n",
    "labels_test = np.squeeze(labels_test)\n",
    "\n",
    "print(test_outputs)\n",
    "print(labels_test)\n",
    "test_rmse = mean_squared_error(labels_test, test_outputs, squared=False)\n",
    "test_r2 = r2_score(labels_test, test_outputs)\n",
    "print(\"Test RMSE:\", test_rmse)\n",
    "print(\"Test R2:\",test_r2)\n",
    "\n",
    "if (test_r2>=0.96):\n",
    "    good_model_path = \"good_model.pth\"+str(test_r2)\n",
    "    torch.save(model.state_dict(),good_model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 Score: 0.8462874284895459\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.decomposition import PCA\n",
    "r2 = 0\n",
    "\n",
    "# Set_Random_State()\n",
    "new_data_train, new_data_test, new_labels_train, new_labels_test = train_test_split(data_selected, labels, test_size=0.2, random_state=42)\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "# 使用随机森林回归模型进行回归\n",
    "rf = RandomForestRegressor(n_estimators=120, random_state=42)\n",
    "rf.fit(new_data_train, new_labels_train)\n",
    "\n",
    "# 在测试集上进行预测\n",
    "predictions = rf.predict(new_data_test)\n",
    "\n",
    "# 评估模型\n",
    "mse = mean_squared_error(new_labels_test, predictions)\n",
    "r2 = r2_score(new_labels_test, predictions)\n",
    "# 打印评估结果\n",
    "# print(\"Mean Squared Error:\", mse)\n",
    "print(\"R2 Score:\", r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(57, 15)\n",
      "(57, 5)\n",
      "(57, 20)\n",
      "R2 Score: 0.7997280905400042\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Set_Random_State()\n",
    "new_data_train, new_data_test, new_labels_train, new_labels_test = train_test_split(merged_matrix, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "print(data_selected.shape)\n",
    "print(data_pca.shape)\n",
    "print(merged_matrix.shape)\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "# 使用随机森林回归模型进行回归\n",
    "rf = RandomForestRegressor(n_estimators=120, random_state=42)\n",
    "rf.fit(new_data_train, new_labels_train)\n",
    "\n",
    "# 在测试集上进行预测\n",
    "predictions = rf.predict(new_data_test)\n",
    "\n",
    "# 评估模型\n",
    "mse = mean_squared_error(new_labels_test, predictions)\n",
    "r2 = r2_score(new_labels_test, predictions)\n",
    "# 打印评估结果\n",
    "# print(\"Mean Squared Error:\", mse)\n",
    "print(\"R2 Score:\", r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 12480.519210821758\n",
      "R2 Score: 0.5527078609462119\n",
      "Mean Squared Error: 11049.507330671295\n",
      "R2 Score: 0.6039942180337339\n",
      "Mean Squared Error: 12410.040490451387\n",
      "R2 Score: 0.555233763679882\n",
      "Mean Squared Error: 11951.819016377316\n",
      "R2 Score: 0.5716560662970082\n",
      "Mean Squared Error: 11900.977219560189\n",
      "R2 Score: 0.5734781969045194\n",
      "Mean Squared Error: 11394.490909143518\n",
      "R2 Score: 0.591630273862287\n",
      "Mean Squared Error: 11967.417010127321\n",
      "R2 Score: 0.5710970462857783\n",
      "Mean Squared Error: 9359.492760821757\n",
      "R2 Score: 0.6645630308540056\n",
      "Mean Squared Error: 8315.291454918979\n",
      "R2 Score: 0.7019863966475557\n"
     ]
    }
   ],
   "source": [
    "def Set_Random_State():\n",
    "    seed = 42\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    return\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.decomposition import PCA\n",
    "r2 = 0\n",
    "while r2 < 0.7:\n",
    "    # Set_Random_State()\n",
    "    # 创建PCA模型并降维到十六维\n",
    "    pca = PCA(n_components=10)\n",
    "    data_pca = pca.fit_transform(data_normalized)\n",
    "    new_data_train, new_data_test, new_labels_train, new_labels_test = train_test_split(data_pca, labels, test_size=0.2, random_state=42)\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    # 使用随机森林回归模型进行回归\n",
    "    rf = RandomForestRegressor(n_estimators=120, random_state=42)\n",
    "    rf.fit(new_data_train, new_labels_train)\n",
    "\n",
    "    # 在测试集上进行预测\n",
    "    predictions = rf.predict(new_data_test)\n",
    "\n",
    "    # 评估模型\n",
    "    mse = mean_squared_error(new_labels_test, predictions)\n",
    "    r2 = r2_score(new_labels_test, predictions)\n",
    "    # 打印评估结果\n",
    "    print(\"Mean Squared Error:\", mse)\n",
    "    print(\"R2 Score:\", r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7019863966475557\n"
     ]
    }
   ],
   "source": [
    "print(r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 Score: 0.8462874284895459\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.decomposition import PCA\n",
    "r2 = 0\n",
    "\n",
    "# Set_Random_State()\n",
    "new_data_train, new_data_test, new_labels_train, new_labels_test = train_test_split(data_selected, labels, test_size=0.2, random_state=42)\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "# 使用随机森林回归模型进行回归\n",
    "rf = RandomForestRegressor(n_estimators=120, random_state=42)\n",
    "rf.fit(new_data_train, new_labels_train)\n",
    "\n",
    "# 在测试集上进行预测\n",
    "predictions = rf.predict(new_data_test)\n",
    "\n",
    "# 评估模型\n",
    "mse = mean_squared_error(new_labels_test, predictions)\n",
    "r2 = r2_score(new_labels_test, predictions)\n",
    "# 打印评估结果\n",
    "# print(\"Mean Squared Error:\", mse)\n",
    "print(\"R2 Score:\", r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_filename' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m load_rf \u001b[39m=\u001b[39m joblib\u001b[39m.\u001b[39mload(model_filename)\n\u001b[0;32m      2\u001b[0m pca \u001b[39m=\u001b[39m PCA(n_components\u001b[39m=\u001b[39m\u001b[39m9\u001b[39m)\n\u001b[0;32m      3\u001b[0m data_pca \u001b[39m=\u001b[39m pca\u001b[39m.\u001b[39mfit_transform(data_normalized)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model_filename' is not defined"
     ]
    }
   ],
   "source": [
    "load_rf = joblib.load(model_filename)\n",
    "pca = PCA(n_components=9)\n",
    "data_pca = pca.fit_transform(data_normalized)\n",
    "new_data_train, new_data_test, new_labels_train, new_labels_test = train_test_split(data_pca, labels, test_size=0.2, random_state=42)\n",
    "load_rf = RandomForestRegressor(n_estimators=130, random_state=42)\n",
    "load_rf.fit(new_data_train, new_labels_train)\n",
    "\n",
    "# 在测试集上进行预测\n",
    "predictions = load_rf.predict(new_data_test)\n",
    "\n",
    "# 评估模型\n",
    "mse = mean_squared_error(new_labels_test, predictions)\n",
    "r2 = r2_score(new_labels_test, predictions)\n",
    "print(r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7211554639684774\n",
      "0.6529930497943028\n",
      "0.7213502000534403\n",
      "0.7217400195790483\n",
      "0.7173203040395983\n",
      "0.6512545252856485\n",
      "0.6604211151621824\n",
      "0.7264254928327523\n",
      "0.7257901832634799\n",
      "0.6430318387018237\n",
      "0.7218502022671893\n",
      "0.660676715053709\n",
      "0.7203138855096038\n",
      "0.7222260082868253\n",
      "0.7226281229145254\n",
      "0.6747683221618219\n",
      "0.7245510368270338\n",
      "0.7198891382181134\n",
      "0.7244507975570391\n",
      "0.6612433160400841\n",
      "0.7126834595072107\n",
      "0.6517009520035519\n",
      "0.7156082201955627\n",
      "0.7133235041740352\n",
      "0.6625128817902088\n",
      "0.6516266234574101\n",
      "0.7280699517363507\n",
      "0.6637520264624096\n",
      "0.6547769300830076\n",
      "0.6558857065858558\n",
      "0.7228002962995999\n",
      "0.7472683458255862\n",
      "0.6433316947437793\n",
      "0.6271999339570684\n",
      "0.7154610402948374\n",
      "0.6634366204458\n",
      "0.6345149915023602\n",
      "0.6727478472169048\n",
      "0.6545126660769387\n",
      "0.6276890058474482\n",
      "0.6665102067090232\n",
      "0.7236368900574424\n",
      "0.6522296625166701\n",
      "0.6614200181977249\n",
      "0.6562937743516186\n",
      "0.6662008825917334\n",
      "0.7248116884341642\n",
      "0.6540386615052873\n",
      "0.655314382467451\n",
      "0.6646250632685771\n",
      "0.7283710808946267\n",
      "0.6567251744928252\n",
      "0.652182859646187\n",
      "0.7311430324696211\n",
      "0.6579188429307725\n",
      "0.6416895621614236\n",
      "0.6544315462327199\n",
      "0.6316793402976647\n",
      "0.6508033680480563\n",
      "0.6499711958948955\n",
      "0.7150696933052725\n",
      "0.6622720998719742\n",
      "0.6563563689786815\n",
      "0.715680184287526\n",
      "0.6539389905288077\n",
      "0.662434148821188\n",
      "0.6551405149325724\n",
      "0.7251289601873196\n",
      "0.6600727178026572\n",
      "0.7209048952906063\n",
      "0.6564310684311776\n",
      "0.7305602897982051\n",
      "0.7250967575864271\n",
      "0.7316525130020328\n",
      "0.6497476777696851\n",
      "0.6986406476304099\n",
      "0.7176950292571525\n",
      "0.7309048388544988\n",
      "0.7273940628822303\n",
      "0.6603864004399462\n",
      "0.6562452154967215\n",
      "0.656460970562855\n",
      "0.6163218382094722\n",
      "0.7204653687207188\n",
      "0.7251885815540285\n",
      "0.6729596366374044\n",
      "0.6483246423233375\n",
      "0.6584457076301312\n",
      "0.721363662935113\n",
      "0.6279317773568571\n",
      "0.7336312487633276\n",
      "0.6550546231251405\n",
      "0.6466066412358957\n",
      "0.721250957006867\n",
      "0.7276005225970695\n",
      "0.7268128426897736\n",
      "0.6518857493782092\n",
      "0.6629526438247619\n",
      "0.6587398892771825\n",
      "0.7171716266995856\n",
      "0.6652376694322071\n",
      "0.7081689376280407\n",
      "0.7292906329724853\n",
      "0.6529013467358229\n",
      "0.6588266863853552\n",
      "0.6393544058403601\n",
      "0.7274779684205452\n",
      "0.7125441706049063\n",
      "0.7254684258977426\n",
      "0.6603813878837136\n",
      "0.7249959917722695\n",
      "0.6678044060507762\n",
      "0.6609816743337387\n",
      "0.6539909353169682\n",
      "0.7203414175139922\n",
      "0.6563556752822837\n",
      "0.6445461729516588\n",
      "0.6592249058282942\n",
      "0.6604382592254348\n",
      "0.644141037583799\n",
      "0.6592898669222667\n",
      "0.6561593137842837\n",
      "0.6479952412861503\n",
      "0.6542174122368654\n",
      "0.6656598592633445\n",
      "0.6631131144661049\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)\n",
      "Cell \u001b[1;32mIn[10], line 14\u001b[0m\n",
      "\u001b[0;32m     12\u001b[0m \u001b[39m# 使用随机森林回归模型进行回归\u001b[39;00m\n",
      "\u001b[0;32m     13\u001b[0m rf \u001b[39m=\u001b[39m RandomForestRegressor(n_estimators\u001b[39m=\u001b[39m\u001b[39m125\u001b[39m, random_state\u001b[39m=\u001b[39m\u001b[39m42\u001b[39m)\n",
      "\u001b[1;32m---> 14\u001b[0m rf\u001b[39m.\u001b[39;49mfit(new_data_train, new_labels_train)\n",
      "\u001b[0;32m     16\u001b[0m \u001b[39m# 在测试集上进行预测\u001b[39;00m\n",
      "\u001b[0;32m     17\u001b[0m predictions \u001b[39m=\u001b[39m rf\u001b[39m.\u001b[39mpredict(new_data_test)\n",
      "\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\pytorch\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:462\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n",
      "\u001b[0;32m    457\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwarm_start \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mestimators_) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n",
      "\u001b[0;32m    458\u001b[0m     \u001b[39m# We draw from the random state to get the random state we\u001b[39;00m\n",
      "\u001b[0;32m    459\u001b[0m     \u001b[39m# would have got if we hadn't used a warm_start.\u001b[39;00m\n",
      "\u001b[0;32m    460\u001b[0m     random_state\u001b[39m.\u001b[39mrandint(MAX_INT, size\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mestimators_))\n",
      "\u001b[1;32m--> 462\u001b[0m trees \u001b[39m=\u001b[39m [\n",
      "\u001b[0;32m    463\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_make_estimator(append\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, random_state\u001b[39m=\u001b[39mrandom_state)\n",
      "\u001b[0;32m    464\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_more_estimators)\n",
      "\u001b[0;32m    465\u001b[0m ]\n",
      "\u001b[0;32m    467\u001b[0m \u001b[39m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n",
      "\u001b[0;32m    468\u001b[0m \u001b[39m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n",
      "\u001b[0;32m    469\u001b[0m \u001b[39m# making threading more efficient than multiprocessing in\u001b[39;00m\n",
      "\u001b[0;32m    470\u001b[0m \u001b[39m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n",
      "\u001b[0;32m    471\u001b[0m \u001b[39m# parallel_backend contexts set at a higher level,\u001b[39;00m\n",
      "\u001b[0;32m    472\u001b[0m \u001b[39m# since correctness does not rely on using threads.\u001b[39;00m\n",
      "\u001b[0;32m    473\u001b[0m trees \u001b[39m=\u001b[39m Parallel(\n",
      "\u001b[0;32m    474\u001b[0m     n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_jobs,\n",
      "\u001b[0;32m    475\u001b[0m     verbose\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose,\n",
      "\u001b[1;32m   (...)\u001b[0m\n",
      "\u001b[0;32m    490\u001b[0m     \u001b[39mfor\u001b[39;00m i, t \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(trees)\n",
      "\u001b[0;32m    491\u001b[0m )\n",
      "\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\pytorch\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:463\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n",
      "\u001b[0;32m    457\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwarm_start \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mestimators_) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n",
      "\u001b[0;32m    458\u001b[0m     \u001b[39m# We draw from the random state to get the random state we\u001b[39;00m\n",
      "\u001b[0;32m    459\u001b[0m     \u001b[39m# would have got if we hadn't used a warm_start.\u001b[39;00m\n",
      "\u001b[0;32m    460\u001b[0m     random_state\u001b[39m.\u001b[39mrandint(MAX_INT, size\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mestimators_))\n",
      "\u001b[0;32m    462\u001b[0m trees \u001b[39m=\u001b[39m [\n",
      "\u001b[1;32m--> 463\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_estimator(append\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, random_state\u001b[39m=\u001b[39;49mrandom_state)\n",
      "\u001b[0;32m    464\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_more_estimators)\n",
      "\u001b[0;32m    465\u001b[0m ]\n",
      "\u001b[0;32m    467\u001b[0m \u001b[39m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n",
      "\u001b[0;32m    468\u001b[0m \u001b[39m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n",
      "\u001b[0;32m    469\u001b[0m \u001b[39m# making threading more efficient than multiprocessing in\u001b[39;00m\n",
      "\u001b[0;32m    470\u001b[0m \u001b[39m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n",
      "\u001b[0;32m    471\u001b[0m \u001b[39m# parallel_backend contexts set at a higher level,\u001b[39;00m\n",
      "\u001b[0;32m    472\u001b[0m \u001b[39m# since correctness does not rely on using threads.\u001b[39;00m\n",
      "\u001b[0;32m    473\u001b[0m trees \u001b[39m=\u001b[39m Parallel(\n",
      "\u001b[0;32m    474\u001b[0m     n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_jobs,\n",
      "\u001b[0;32m    475\u001b[0m     verbose\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose,\n",
      "\u001b[1;32m   (...)\u001b[0m\n",
      "\u001b[0;32m    490\u001b[0m     \u001b[39mfor\u001b[39;00m i, t \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(trees)\n",
      "\u001b[0;32m    491\u001b[0m )\n",
      "\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\pytorch\\lib\\site-packages\\sklearn\\ensemble\\_base.py:206\u001b[0m, in \u001b[0;36mBaseEnsemble._make_estimator\u001b[1;34m(self, append, random_state)\u001b[0m\n",
      "\u001b[0;32m    203\u001b[0m             estimator\u001b[39m.\u001b[39mset_params(max_features\u001b[39m=\u001b[39m\u001b[39m1.0\u001b[39m)\n",
      "\u001b[0;32m    205\u001b[0m \u001b[39mif\u001b[39;00m random_state \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;32m--> 206\u001b[0m     _set_random_states(estimator, random_state)\n",
      "\u001b[0;32m    208\u001b[0m \u001b[39mif\u001b[39;00m append:\n",
      "\u001b[0;32m    209\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mestimators_\u001b[39m.\u001b[39mappend(estimator)\n",
      "\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\pytorch\\lib\\site-packages\\sklearn\\ensemble\\_base.py:85\u001b[0m, in \u001b[0;36m_set_random_states\u001b[1;34m(estimator, random_state)\u001b[0m\n",
      "\u001b[0;32m     82\u001b[0m         to_set[key] \u001b[39m=\u001b[39m random_state\u001b[39m.\u001b[39mrandint(np\u001b[39m.\u001b[39miinfo(np\u001b[39m.\u001b[39mint32)\u001b[39m.\u001b[39mmax)\n",
      "\u001b[0;32m     84\u001b[0m \u001b[39mif\u001b[39;00m to_set:\n",
      "\u001b[1;32m---> 85\u001b[0m     estimator\u001b[39m.\u001b[39mset_params(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mto_set)\n",
      "\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\pytorch\\lib\\site-packages\\sklearn\\base.py:198\u001b[0m, in \u001b[0;36mBaseEstimator.set_params\u001b[1;34m(self, **params)\u001b[0m\n",
      "\u001b[0;32m    195\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m params:\n",
      "\u001b[0;32m    196\u001b[0m     \u001b[39m# Simple optimization to gain speed (inspect is slow)\u001b[39;00m\n",
      "\u001b[0;32m    197\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "\u001b[1;32m--> 198\u001b[0m valid_params \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_params(deep\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;32m    200\u001b[0m nested_params \u001b[39m=\u001b[39m defaultdict(\u001b[39mdict\u001b[39m)  \u001b[39m# grouped by prefix\u001b[39;00m\n",
      "\u001b[0;32m    201\u001b[0m \u001b[39mfor\u001b[39;00m key, value \u001b[39min\u001b[39;00m params\u001b[39m.\u001b[39mitems():\n",
      "\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\pytorch\\lib\\site-packages\\sklearn\\base.py:169\u001b[0m, in \u001b[0;36mBaseEstimator.get_params\u001b[1;34m(self, deep)\u001b[0m\n",
      "\u001b[0;32m    154\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n",
      "\u001b[0;32m    155\u001b[0m \u001b[39mGet parameters for this estimator.\u001b[39;00m\n",
      "\u001b[0;32m    156\u001b[0m \n",
      "\u001b[1;32m   (...)\u001b[0m\n",
      "\u001b[0;32m    166\u001b[0m \u001b[39m    Parameter names mapped to their values.\u001b[39;00m\n",
      "\u001b[0;32m    167\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n",
      "\u001b[0;32m    168\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m()\n",
      "\u001b[1;32m--> 169\u001b[0m \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_param_names():\n",
      "\u001b[0;32m    170\u001b[0m     value \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m, key)\n",
      "\u001b[0;32m    171\u001b[0m     \u001b[39mif\u001b[39;00m deep \u001b[39mand\u001b[39;00m \u001b[39mhasattr\u001b[39m(value, \u001b[39m\"\u001b[39m\u001b[39mget_params\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(value, \u001b[39mtype\u001b[39m):\n",
      "\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\pytorch\\lib\\site-packages\\sklearn\\base.py:134\u001b[0m, in \u001b[0;36mBaseEstimator._get_param_names\u001b[1;34m(cls)\u001b[0m\n",
      "\u001b[0;32m    130\u001b[0m     \u001b[39mreturn\u001b[39;00m []\n",
      "\u001b[0;32m    132\u001b[0m \u001b[39m# introspect the constructor arguments to find the model parameters\u001b[39;00m\n",
      "\u001b[0;32m    133\u001b[0m \u001b[39m# to represent\u001b[39;00m\n",
      "\u001b[1;32m--> 134\u001b[0m init_signature \u001b[39m=\u001b[39m inspect\u001b[39m.\u001b[39;49msignature(init)\n",
      "\u001b[0;32m    135\u001b[0m \u001b[39m# Consider the constructor parameters excluding 'self'\u001b[39;00m\n",
      "\u001b[0;32m    136\u001b[0m parameters \u001b[39m=\u001b[39m [\n",
      "\u001b[0;32m    137\u001b[0m     p\n",
      "\u001b[0;32m    138\u001b[0m     \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m init_signature\u001b[39m.\u001b[39mparameters\u001b[39m.\u001b[39mvalues()\n",
      "\u001b[0;32m    139\u001b[0m     \u001b[39mif\u001b[39;00m p\u001b[39m.\u001b[39mname \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mself\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mand\u001b[39;00m p\u001b[39m.\u001b[39mkind \u001b[39m!=\u001b[39m p\u001b[39m.\u001b[39mVAR_KEYWORD\n",
      "\u001b[0;32m    140\u001b[0m ]\n",
      "\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\pytorch\\lib\\inspect.py:3113\u001b[0m, in \u001b[0;36msignature\u001b[1;34m(obj, follow_wrapped)\u001b[0m\n",
      "\u001b[0;32m   3111\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msignature\u001b[39m(obj, \u001b[39m*\u001b[39m, follow_wrapped\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n",
      "\u001b[0;32m   3112\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Get a signature object for the passed callable.\"\"\"\u001b[39;00m\n",
      "\u001b[1;32m-> 3113\u001b[0m     \u001b[39mreturn\u001b[39;00m Signature\u001b[39m.\u001b[39;49mfrom_callable(obj, follow_wrapped\u001b[39m=\u001b[39;49mfollow_wrapped)\n",
      "\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\pytorch\\lib\\inspect.py:2862\u001b[0m, in \u001b[0;36mSignature.from_callable\u001b[1;34m(cls, obj, follow_wrapped)\u001b[0m\n",
      "\u001b[0;32m   2859\u001b[0m \u001b[39m@classmethod\u001b[39m\n",
      "\u001b[0;32m   2860\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfrom_callable\u001b[39m(\u001b[39mcls\u001b[39m, obj, \u001b[39m*\u001b[39m, follow_wrapped\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n",
      "\u001b[0;32m   2861\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Constructs Signature for the given callable object.\"\"\"\u001b[39;00m\n",
      "\u001b[1;32m-> 2862\u001b[0m     \u001b[39mreturn\u001b[39;00m _signature_from_callable(obj, sigcls\u001b[39m=\u001b[39;49m\u001b[39mcls\u001b[39;49m,\n",
      "\u001b[0;32m   2863\u001b[0m                                     follow_wrapper_chains\u001b[39m=\u001b[39;49mfollow_wrapped)\n",
      "\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\pytorch\\lib\\inspect.py:2325\u001b[0m, in \u001b[0;36m_signature_from_callable\u001b[1;34m(obj, follow_wrapper_chains, skip_bound_arg, sigcls)\u001b[0m\n",
      "\u001b[0;32m   2320\u001b[0m             \u001b[39mreturn\u001b[39;00m sig\u001b[39m.\u001b[39mreplace(parameters\u001b[39m=\u001b[39mnew_params)\n",
      "\u001b[0;32m   2322\u001b[0m \u001b[39mif\u001b[39;00m isfunction(obj) \u001b[39mor\u001b[39;00m _signature_is_functionlike(obj):\n",
      "\u001b[0;32m   2323\u001b[0m     \u001b[39m# If it's a pure Python function, or an object that is duck type\u001b[39;00m\n",
      "\u001b[0;32m   2324\u001b[0m     \u001b[39m# of a Python function (Cython functions, for instance), then:\u001b[39;00m\n",
      "\u001b[1;32m-> 2325\u001b[0m     \u001b[39mreturn\u001b[39;00m _signature_from_function(sigcls, obj,\n",
      "\u001b[0;32m   2326\u001b[0m                                     skip_bound_arg\u001b[39m=\u001b[39;49mskip_bound_arg)\n",
      "\u001b[0;32m   2328\u001b[0m \u001b[39mif\u001b[39;00m _signature_is_builtin(obj):\n",
      "\u001b[0;32m   2329\u001b[0m     \u001b[39mreturn\u001b[39;00m _signature_from_builtin(sigcls, obj,\n",
      "\u001b[0;32m   2330\u001b[0m                                    skip_bound_arg\u001b[39m=\u001b[39mskip_bound_arg)\n",
      "\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\pytorch\\lib\\inspect.py:2171\u001b[0m, in \u001b[0;36m_signature_from_function\u001b[1;34m(cls, func, skip_bound_arg)\u001b[0m\n",
      "\u001b[0;32m   2168\u001b[0m Parameter \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_parameter_cls\n",
      "\u001b[0;32m   2170\u001b[0m \u001b[39m# Parameter information.\u001b[39;00m\n",
      "\u001b[1;32m-> 2171\u001b[0m func_code \u001b[39m=\u001b[39m func\u001b[39m.\u001b[39;49m\u001b[39m__code__\u001b[39;49m\n",
      "\u001b[0;32m   2172\u001b[0m pos_count \u001b[39m=\u001b[39m func_code\u001b[39m.\u001b[39mco_argcount\n",
      "\u001b[0;32m   2173\u001b[0m arg_names \u001b[39m=\u001b[39m func_code\u001b[39m.\u001b[39mco_varnames\n",
      "\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.decomposition import PCA\n",
    "r2 = 0\n",
    "while True:\n",
    "    # Set_Random_State()\n",
    "    # 创建PCA模型并降维到十六维\n",
    "    pca = PCA(n_components=9)\n",
    "    data_pca = pca.fit_transform(data_normalized)\n",
    "    new_data_train, new_data_test, new_labels_train, new_labels_test = train_test_split(data_pca, labels, test_size=0.2, random_state=42)\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    # 使用随机森林回归模型进行回归\n",
    "    rf = RandomForestRegressor(n_estimators=125, random_state=42)\n",
    "    rf.fit(new_data_train, new_labels_train)\n",
    "\n",
    "    # 在测试集上进行预测\n",
    "    predictions = rf.predict(new_data_test)\n",
    "\n",
    "    # 评估模型\n",
    "    mse = mean_squared_error(new_labels_test, predictions)\n",
    "    r2 = r2_score(new_labels_test, predictions)\n",
    "    print(r2)\n",
    "    if (r2 > 0.75):\n",
    "        model_filename = 'random_forest_model.pkl'\n",
    "        joblib.dump(rf, model_filename)\n",
    "        break\n",
    "print(r2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2744458843633376\n"
     ]
    }
   ],
   "source": [
    "load_rf = joblib.load(model_filename)\n",
    "pca = PCA(n_components=9)\n",
    "data_pca = pca.fit_transform(data_normalized)\n",
    "new_data_train, new_data_test, new_labels_train, new_labels_test = train_test_split(data_normalized, labels, test_size=0.2, random_state=42)\n",
    "load_rf = RandomForestRegressor(n_estimators=130, random_state=42)\n",
    "load_rf.fit(new_data_train, new_labels_train)\n",
    "\n",
    "# 在测试集上进行预测\n",
    "predictions = load_rf.predict(new_data_test)\n",
    "\n",
    "# 评估模型\n",
    "mse = mean_squared_error(new_labels_test, predictions)\n",
    "r2 = r2_score(new_labels_test, predictions)\n",
    "print(r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
