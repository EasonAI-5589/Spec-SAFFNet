{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 导入神经网络中间层特征"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T16:54:35.898263Z",
     "start_time": "2025-01-12T16:54:35.052117Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 定义数据集类\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = torch.tensor(data, dtype=torch.float32)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index], self.labels[index]\n"
   ],
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[1], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mpandas\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mpd\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mnumpy\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mnp\u001B[39;00m\n\u001B[1;32m----> 3\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mnn\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mnn\u001B[39;00m\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01moptim\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01moptim\u001B[39;00m\n",
      "\u001B[1;31mModuleNotFoundError\u001B[0m: No module named 'torch'"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在训练之前设置随机种子\n",
    "import random\n",
    "import numpy as np\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-24T09:51:03.054687400Z",
     "start_time": "2023-07-24T09:51:03.039550800Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 定义神经网络模型\n",
    "# 利用神经网络提取中间层特征\n",
    "# 但是有一个问题是为什么要用这样一个三个线形层的神经网络：\n",
    "# 前馈神经网络：一个输入层，两个隐藏层和一个输出层\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(2048, 30)\n",
    "        self.fc2 = nn.Linear(30, 10)\n",
    "        self.fc3 = nn.Linear(10, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        intermediate_features = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(intermediate_features))  # 提取中间层特征\n",
    "        x = self.fc3(x)\n",
    "        return x, intermediate_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-24T09:51:03.602002Z",
     "start_time": "2023-07-24T09:51:03.575464500Z"
    }
   },
   "outputs": [],
   "source": [
    "# 从Excel读取数据集\n",
    "data_df = pd.read_excel('Pb1.xlsx', sheet_name='Sheet1')\n",
    "data = data_df.iloc[:, :2048].values\n",
    "label_df = pd.read_excel('Pb1.xlsx', sheet_name=\"Sheet2\")\n",
    "labels = label_df.iloc[:, 0].values\n",
    "\n",
    "# 数据预处理：标准化\n",
    "scaler = StandardScaler()\n",
    "data_normalized = scaler.fit_transform(data)\n",
    "\n",
    "# 假设data是包含特征数据的数组，labels是包含对应标签的数组\n",
    "data_train, data_test, labels_train, labels_test = train_test_split(data_normalized, labels, test_size=0.2)\n",
    "\n",
    "data_train_tensor = torch.tensor(data_train, dtype=torch.float32).clone().detach()\n",
    "labels_train_tensor = torch.tensor(labels_train, dtype=torch.float32).clone().detach()\n",
    "data_test_tensor = torch.tensor(data_test, dtype=torch.float32).clone().detach()\n",
    "labels_test_tensor = torch.tensor(labels_test, dtype=torch.float32).clone().detach()\n",
    "\n",
    "batch_size = 57\n",
    "\n",
    "# 准备数据\n",
    "train_dataset = CustomDataset(data_train, labels_train)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-24T09:51:04.459356600Z",
     "start_time": "2023-07-24T09:51:03.909834700Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1000], Loss: 192188.34375, Train RMSE: 438.38104248046875, Train R^2: -0.13306500665677823\n",
      "Epoch [2/1000], Loss: 192177.9375, Train RMSE: 438.3682556152344, Train R^2: -0.1329988753569864\n",
      "Epoch [3/1000], Loss: 192166.734375, Train RMSE: 438.35528564453125, Train R^2: -0.13293192236604168\n",
      "Epoch [4/1000], Loss: 192155.359375, Train RMSE: 438.343017578125, Train R^2: -0.13286836453062567\n",
      "Epoch [5/1000], Loss: 192144.59375, Train RMSE: 438.33331298828125, Train R^2: -0.13281833996428238\n",
      "Epoch [6/1000], Loss: 192136.09375, Train RMSE: 438.3243713378906, Train R^2: -0.13277219492256664\n",
      "Epoch [7/1000], Loss: 192128.265625, Train RMSE: 438.31640625, Train R^2: -0.1327308662655593\n",
      "Epoch [8/1000], Loss: 192121.25, Train RMSE: 438.30841064453125, Train R^2: -0.1326896120409209\n",
      "Epoch [9/1000], Loss: 192114.25, Train RMSE: 438.30059814453125, Train R^2: -0.1326492472037093\n",
      "Epoch [10/1000], Loss: 192107.4375, Train RMSE: 438.2926940917969, Train R^2: -0.13260835258166148\n",
      "Epoch [11/1000], Loss: 192100.484375, Train RMSE: 438.28460693359375, Train R^2: -0.13256651074338777\n",
      "Epoch [12/1000], Loss: 192093.390625, Train RMSE: 438.2763977050781, Train R^2: -0.13252418452170223\n",
      "Epoch [13/1000], Loss: 192086.203125, Train RMSE: 438.2680969238281, Train R^2: -0.13248124314291854\n",
      "Epoch [14/1000], Loss: 192078.90625, Train RMSE: 438.2598571777344, Train R^2: -0.13243852714882842\n",
      "Epoch [15/1000], Loss: 192071.6875, Train RMSE: 438.25146484375, Train R^2: -0.13239517240546128\n",
      "Epoch [16/1000], Loss: 192064.34375, Train RMSE: 438.2429504394531, Train R^2: -0.132351272437093\n",
      "Epoch [17/1000], Loss: 192056.859375, Train RMSE: 438.2344055175781, Train R^2: -0.13230713483147416\n",
      "Epoch [18/1000], Loss: 192049.390625, Train RMSE: 438.225830078125, Train R^2: -0.13226285617780897\n",
      "Epoch [19/1000], Loss: 192041.890625, Train RMSE: 438.2171325683594, Train R^2: -0.13221786055171902\n",
      "Epoch [20/1000], Loss: 192034.25, Train RMSE: 438.2083435058594, Train R^2: -0.13217240311616152\n",
      "Epoch [21/1000], Loss: 192026.515625, Train RMSE: 438.199462890625, Train R^2: -0.13212651829484168\n",
      "Epoch [22/1000], Loss: 192018.75, Train RMSE: 438.19061279296875, Train R^2: -0.13208063485365962\n",
      "Epoch [23/1000], Loss: 192010.984375, Train RMSE: 438.181640625, Train R^2: -0.13203442995485326\n",
      "Epoch [24/1000], Loss: 192003.140625, Train RMSE: 438.1723327636719, Train R^2: -0.13198637734381768\n",
      "Epoch [25/1000], Loss: 191995.0, Train RMSE: 438.16253662109375, Train R^2: -0.13193561776000218\n",
      "Epoch [26/1000], Loss: 191986.390625, Train RMSE: 438.15234375, Train R^2: -0.13188319258490688\n",
      "Epoch [27/1000], Loss: 191977.484375, Train RMSE: 438.1419372558594, Train R^2: -0.1318294131377653\n",
      "Epoch [28/1000], Loss: 191968.359375, Train RMSE: 438.1313171386719, Train R^2: -0.13177458438304335\n",
      "Epoch [29/1000], Loss: 191959.0625, Train RMSE: 438.12042236328125, Train R^2: -0.13171846009698784\n",
      "Epoch [30/1000], Loss: 191949.515625, Train RMSE: 438.1094055175781, Train R^2: -0.13166141568527934\n",
      "Epoch [31/1000], Loss: 191939.859375, Train RMSE: 438.097900390625, Train R^2: -0.1316019993635078\n",
      "Epoch [32/1000], Loss: 191929.796875, Train RMSE: 438.0860595703125, Train R^2: -0.13154077920664609\n",
      "Epoch [33/1000], Loss: 191919.4375, Train RMSE: 438.0738525390625, Train R^2: -0.1314777790154269\n",
      "Epoch [34/1000], Loss: 191908.734375, Train RMSE: 438.0611877441406, Train R^2: -0.13141215025654973\n",
      "Epoch [35/1000], Loss: 191897.59375, Train RMSE: 438.04803466796875, Train R^2: -0.13134445992847898\n",
      "Epoch [36/1000], Loss: 191886.109375, Train RMSE: 438.0347595214844, Train R^2: -0.13127543189512836\n",
      "Epoch [37/1000], Loss: 191874.40625, Train RMSE: 438.0211181640625, Train R^2: -0.13120518420224592\n",
      "Epoch [38/1000], Loss: 191862.5, Train RMSE: 438.00732421875, Train R^2: -0.13113389883685622\n",
      "Epoch [39/1000], Loss: 191850.40625, Train RMSE: 437.99322509765625, Train R^2: -0.13106129384905985\n",
      "Epoch [40/1000], Loss: 191838.09375, Train RMSE: 437.9791259765625, Train R^2: -0.13098839851910293\n",
      "Epoch [41/1000], Loss: 191825.734375, Train RMSE: 437.96490478515625, Train R^2: -0.1309149740136153\n",
      "Epoch [42/1000], Loss: 191813.265625, Train RMSE: 437.9505310058594, Train R^2: -0.1308406437992926\n",
      "Epoch [43/1000], Loss: 191800.6875, Train RMSE: 437.9360656738281, Train R^2: -0.13076574576046673\n",
      "Epoch [44/1000], Loss: 191787.953125, Train RMSE: 437.9214172363281, Train R^2: -0.1306902236508969\n",
      "Epoch [45/1000], Loss: 191775.140625, Train RMSE: 437.90655517578125, Train R^2: -0.13061362555108103\n",
      "Epoch [46/1000], Loss: 191762.15625, Train RMSE: 437.8915710449219, Train R^2: -0.130536208706451\n",
      "Epoch [47/1000], Loss: 191749.015625, Train RMSE: 437.87640380859375, Train R^2: -0.13045802639544046\n",
      "Epoch [48/1000], Loss: 191735.765625, Train RMSE: 437.8610534667969, Train R^2: -0.1303787747014662\n",
      "Epoch [49/1000], Loss: 191722.3125, Train RMSE: 437.8456726074219, Train R^2: -0.13029932375238662\n",
      "Epoch [50/1000], Loss: 191708.859375, Train RMSE: 437.8299865722656, Train R^2: -0.13021819167065218\n",
      "Epoch [51/1000], Loss: 191695.09375, Train RMSE: 437.8142395019531, Train R^2: -0.13013707812672526\n",
      "Epoch [52/1000], Loss: 191681.3125, Train RMSE: 437.7984619140625, Train R^2: -0.13005546179243654\n",
      "Epoch [53/1000], Loss: 191667.5, Train RMSE: 437.782470703125, Train R^2: -0.12997290831999542\n",
      "Epoch [54/1000], Loss: 191653.484375, Train RMSE: 437.766357421875, Train R^2: -0.1298897735621063\n",
      "Epoch [55/1000], Loss: 191639.40625, Train RMSE: 437.7500305175781, Train R^2: -0.12980554224582774\n",
      "Epoch [56/1000], Loss: 191625.09375, Train RMSE: 437.733154296875, Train R^2: -0.12971829085153508\n",
      "Epoch [57/1000], Loss: 191610.296875, Train RMSE: 437.716064453125, Train R^2: -0.1296300593170603\n",
      "Epoch [58/1000], Loss: 191595.34375, Train RMSE: 437.6986083984375, Train R^2: -0.12954004631186278\n",
      "Epoch [59/1000], Loss: 191580.0625, Train RMSE: 437.68096923828125, Train R^2: -0.12944906246759036\n",
      "Epoch [60/1000], Loss: 191564.640625, Train RMSE: 437.6631774902344, Train R^2: -0.12935713019461526\n",
      "Epoch [61/1000], Loss: 191549.015625, Train RMSE: 437.6451110839844, Train R^2: -0.12926381894819317\n",
      "Epoch [62/1000], Loss: 191533.234375, Train RMSE: 437.62677001953125, Train R^2: -0.12916937576073528\n",
      "Epoch [63/1000], Loss: 191517.1875, Train RMSE: 437.60833740234375, Train R^2: -0.12907412339639723\n",
      "Epoch [64/1000], Loss: 191501.046875, Train RMSE: 437.5903625488281, Train R^2: -0.1289815062775399\n",
      "Epoch [65/1000], Loss: 191485.3125, Train RMSE: 437.57257080078125, Train R^2: -0.12888964092182742\n",
      "Epoch [66/1000], Loss: 191469.75, Train RMSE: 437.55438232421875, Train R^2: -0.1287958242842504\n",
      "Epoch [67/1000], Loss: 191453.8125, Train RMSE: 437.535888671875, Train R^2: -0.12870039972879943\n",
      "Epoch [68/1000], Loss: 191437.65625, Train RMSE: 437.5170593261719, Train R^2: -0.12860344721759565\n",
      "Epoch [69/1000], Loss: 191421.203125, Train RMSE: 437.4980773925781, Train R^2: -0.12850526991683675\n",
      "Epoch [70/1000], Loss: 191404.546875, Train RMSE: 437.4787902832031, Train R^2: -0.12840596628555834\n",
      "Epoch [71/1000], Loss: 191387.734375, Train RMSE: 437.4593505859375, Train R^2: -0.12830539841833932\n",
      "Epoch [72/1000], Loss: 191370.65625, Train RMSE: 437.4396057128906, Train R^2: -0.12820368609172328\n",
      "Epoch [73/1000], Loss: 191353.40625, Train RMSE: 437.42041015625, Train R^2: -0.12810460094138199\n",
      "Epoch [74/1000], Loss: 191336.609375, Train RMSE: 437.4009094238281, Train R^2: -0.12800409144070501\n",
      "Epoch [75/1000], Loss: 191319.5625, Train RMSE: 437.3812561035156, Train R^2: -0.12790272128093783\n",
      "Epoch [76/1000], Loss: 191302.359375, Train RMSE: 437.361328125, Train R^2: -0.12780000964555827\n",
      "Epoch [77/1000], Loss: 191284.9375, Train RMSE: 437.3411560058594, Train R^2: -0.12769610935569986\n",
      "Epoch [78/1000], Loss: 191267.3125, Train RMSE: 437.32080078125, Train R^2: -0.1275907971030099\n",
      "Epoch [79/1000], Loss: 191249.453125, Train RMSE: 437.30010986328125, Train R^2: -0.12748420452094966\n",
      "Epoch [80/1000], Loss: 191231.390625, Train RMSE: 437.2798767089844, Train R^2: -0.12737990084132633\n",
      "Epoch [81/1000], Loss: 191213.6875, Train RMSE: 437.2594299316406, Train R^2: -0.12727444119502307\n",
      "Epoch [82/1000], Loss: 191195.765625, Train RMSE: 437.2387390136719, Train R^2: -0.12716768539172363\n",
      "Epoch [83/1000], Loss: 191177.6875, Train RMSE: 437.2177429199219, Train R^2: -0.12705954434506173\n",
      "Epoch [84/1000], Loss: 191159.34375, Train RMSE: 437.19647216796875, Train R^2: -0.12695004079153116\n",
      "Epoch [85/1000], Loss: 191140.765625, Train RMSE: 437.1750183105469, Train R^2: -0.12683948178216164\n",
      "Epoch [86/1000], Loss: 191122.046875, Train RMSE: 437.1533508300781, Train R^2: -0.1267278053405836\n",
      "Epoch [87/1000], Loss: 191103.0625, Train RMSE: 437.1321716308594, Train R^2: -0.12661844316326532\n",
      "Epoch [88/1000], Loss: 191084.546875, Train RMSE: 437.1106262207031, Train R^2: -0.12650746727865037\n",
      "Epoch [89/1000], Loss: 191065.703125, Train RMSE: 437.0888366699219, Train R^2: -0.12639496870938705\n",
      "Epoch [90/1000], Loss: 191046.640625, Train RMSE: 437.066650390625, Train R^2: -0.12628085624471908\n",
      "Epoch [91/1000], Loss: 191027.265625, Train RMSE: 437.0442199707031, Train R^2: -0.1261650645673218\n",
      "Epoch [92/1000], Loss: 191007.609375, Train RMSE: 437.0215759277344, Train R^2: -0.12604847495350002\n",
      "Epoch [93/1000], Loss: 190987.859375, Train RMSE: 436.9991455078125, Train R^2: -0.12593304528013305\n",
      "Epoch [94/1000], Loss: 190968.265625, Train RMSE: 436.97650146484375, Train R^2: -0.12581607062522382\n",
      "Epoch [95/1000], Loss: 190948.453125, Train RMSE: 436.9535217285156, Train R^2: -0.12569775969424413\n",
      "Epoch [96/1000], Loss: 190928.359375, Train RMSE: 436.93035888671875, Train R^2: -0.12557834350898367\n",
      "Epoch [97/1000], Loss: 190908.140625, Train RMSE: 436.9071044921875, Train R^2: -0.12545848456118058\n",
      "Epoch [98/1000], Loss: 190887.796875, Train RMSE: 436.8837585449219, Train R^2: -0.12533820010869223\n",
      "Epoch [99/1000], Loss: 190867.390625, Train RMSE: 436.86004638671875, Train R^2: -0.1252161796870932\n",
      "Epoch [100/1000], Loss: 190846.703125, Train RMSE: 436.83636474609375, Train R^2: -0.12509422630236888\n",
      "Epoch [101/1000], Loss: 190826.015625, Train RMSE: 436.8125305175781, Train R^2: -0.12497129387237216\n",
      "Epoch [102/1000], Loss: 190805.15625, Train RMSE: 436.78839111328125, Train R^2: -0.12484712171257173\n",
      "Epoch [103/1000], Loss: 190784.109375, Train RMSE: 436.7640686035156, Train R^2: -0.12472188801308026\n",
      "Epoch [104/1000], Loss: 190762.859375, Train RMSE: 436.73944091796875, Train R^2: -0.12459504724833326\n",
      "Epoch [105/1000], Loss: 190741.359375, Train RMSE: 436.71484375, Train R^2: -0.12446824859312611\n",
      "Epoch [106/1000], Loss: 190719.84375, Train RMSE: 436.6900939941406, Train R^2: -0.12434083961437059\n",
      "Epoch [107/1000], Loss: 190698.25, Train RMSE: 436.66510009765625, Train R^2: -0.12421223299490491\n",
      "Epoch [108/1000], Loss: 190676.40625, Train RMSE: 436.639892578125, Train R^2: -0.12408221432979949\n",
      "Epoch [109/1000], Loss: 190654.359375, Train RMSE: 436.6146545410156, Train R^2: -0.12395228731249408\n",
      "Epoch [110/1000], Loss: 190632.3125, Train RMSE: 436.5890808105469, Train R^2: -0.12382066567807581\n",
      "Epoch [111/1000], Loss: 190610.015625, Train RMSE: 436.5632629394531, Train R^2: -0.12368777810452447\n",
      "Epoch [112/1000], Loss: 190587.46875, Train RMSE: 436.53729248046875, Train R^2: -0.12355422195589427\n",
      "Epoch [113/1000], Loss: 190564.8125, Train RMSE: 436.5107421875, Train R^2: -0.12341754693744722\n",
      "Epoch [114/1000], Loss: 190541.625, Train RMSE: 436.4837341308594, Train R^2: -0.12327860456153772\n",
      "Epoch [115/1000], Loss: 190518.0625, Train RMSE: 436.4563293457031, Train R^2: -0.1231375356420994\n",
      "Epoch [116/1000], Loss: 190494.109375, Train RMSE: 436.4286193847656, Train R^2: -0.12299477796857827\n",
      "Epoch [117/1000], Loss: 190469.921875, Train RMSE: 436.4004821777344, Train R^2: -0.1228501686770378\n",
      "Epoch [118/1000], Loss: 190445.390625, Train RMSE: 436.37213134765625, Train R^2: -0.12270403590241363\n",
      "Epoch [119/1000], Loss: 190420.640625, Train RMSE: 436.34326171875, Train R^2: -0.12255560804437948\n",
      "Epoch [120/1000], Loss: 190395.4375, Train RMSE: 436.3141174316406, Train R^2: -0.12240567669941482\n",
      "Epoch [121/1000], Loss: 190370.015625, Train RMSE: 436.2848205566406, Train R^2: -0.12225488860063916\n",
      "Epoch [122/1000], Loss: 190344.453125, Train RMSE: 436.2550964355469, Train R^2: -0.12210204473538311\n",
      "Epoch [123/1000], Loss: 190318.53125, Train RMSE: 436.2251281738281, Train R^2: -0.1219479479288259\n",
      "Epoch [124/1000], Loss: 190292.390625, Train RMSE: 436.1948547363281, Train R^2: -0.12179216864934683\n",
      "Epoch [125/1000], Loss: 190265.953125, Train RMSE: 436.16448974609375, Train R^2: -0.12163603890634556\n",
      "Epoch [126/1000], Loss: 190239.484375, Train RMSE: 436.1336669921875, Train R^2: -0.12147742698793262\n",
      "Epoch [127/1000], Loss: 190212.5625, Train RMSE: 436.102783203125, Train R^2: -0.12131852974134905\n",
      "Epoch [128/1000], Loss: 190185.640625, Train RMSE: 436.0716247558594, Train R^2: -0.12115835450702894\n",
      "Epoch [129/1000], Loss: 190158.453125, Train RMSE: 436.0401611328125, Train R^2: -0.12099664312967184\n",
      "Epoch [130/1000], Loss: 190131.015625, Train RMSE: 436.0084533691406, Train R^2: -0.12083339125673165\n",
      "Epoch [131/1000], Loss: 190103.359375, Train RMSE: 435.9764099121094, Train R^2: -0.1206687480458426\n",
      "Epoch [132/1000], Loss: 190075.390625, Train RMSE: 435.9440612792969, Train R^2: -0.12050261637210169\n",
      "Epoch [133/1000], Loss: 190047.234375, Train RMSE: 435.9117736816406, Train R^2: -0.12033666889616201\n",
      "Epoch [134/1000], Loss: 190019.09375, Train RMSE: 435.87908935546875, Train R^2: -0.12016864223229096\n",
      "Epoch [135/1000], Loss: 189990.609375, Train RMSE: 435.8459777832031, Train R^2: -0.1199983395590265\n",
      "Epoch [136/1000], Loss: 189961.71875, Train RMSE: 435.8128662109375, Train R^2: -0.11982811368378954\n",
      "Epoch [137/1000], Loss: 189932.828125, Train RMSE: 435.7793884277344, Train R^2: -0.1196562027849315\n",
      "Epoch [138/1000], Loss: 189903.6875, Train RMSE: 435.74560546875, Train R^2: -0.11948254426911031\n",
      "Epoch [139/1000], Loss: 189874.234375, Train RMSE: 435.7115173339844, Train R^2: -0.1193075375625392\n",
      "Epoch [140/1000], Loss: 189844.5625, Train RMSE: 435.6770935058594, Train R^2: -0.11913067112171571\n",
      "Epoch [141/1000], Loss: 189814.546875, Train RMSE: 435.6419982910156, Train R^2: -0.11895021920690652\n",
      "Epoch [142/1000], Loss: 189783.9375, Train RMSE: 435.60662841796875, Train R^2: -0.11876845895512878\n",
      "Epoch [143/1000], Loss: 189753.109375, Train RMSE: 435.5708312988281, Train R^2: -0.11858474635418004\n",
      "Epoch [144/1000], Loss: 189721.953125, Train RMSE: 435.5346374511719, Train R^2: -0.11839881331023316\n",
      "Epoch [145/1000], Loss: 189690.421875, Train RMSE: 435.49810791015625, Train R^2: -0.11821132292790937\n",
      "Epoch [146/1000], Loss: 189658.609375, Train RMSE: 435.46124267578125, Train R^2: -0.11802193029235242\n",
      "Epoch [147/1000], Loss: 189626.5, Train RMSE: 435.424072265625, Train R^2: -0.1178310457655789\n",
      "Epoch [148/1000], Loss: 189594.125, Train RMSE: 435.3865966796875, Train R^2: -0.11763863872437419\n",
      "Epoch [149/1000], Loss: 189561.5, Train RMSE: 435.34893798828125, Train R^2: -0.11744534971014997\n",
      "Epoch [150/1000], Loss: 189528.703125, Train RMSE: 435.31072998046875, Train R^2: -0.1172491923390242\n",
      "Epoch [151/1000], Loss: 189495.421875, Train RMSE: 435.2724609375, Train R^2: -0.11705276063738168\n",
      "Epoch [152/1000], Loss: 189462.109375, Train RMSE: 435.2336730957031, Train R^2: -0.11685379285111996\n",
      "Epoch [153/1000], Loss: 189428.359375, Train RMSE: 435.1947937011719, Train R^2: -0.11665422125351577\n",
      "Epoch [154/1000], Loss: 189394.515625, Train RMSE: 435.1556396484375, Train R^2: -0.11645326553148316\n",
      "Epoch [155/1000], Loss: 189360.4375, Train RMSE: 435.11614990234375, Train R^2: -0.11625058505268226\n",
      "Epoch [156/1000], Loss: 189326.0625, Train RMSE: 435.076416015625, Train R^2: -0.11604675043665069\n",
      "Epoch [157/1000], Loss: 189291.484375, Train RMSE: 435.036376953125, Train R^2: -0.11584143881137843\n",
      "Epoch [158/1000], Loss: 189256.640625, Train RMSE: 434.99603271484375, Train R^2: -0.11563440780523027\n",
      "Epoch [159/1000], Loss: 189221.546875, Train RMSE: 434.9554138183594, Train R^2: -0.11542605636801673\n",
      "Epoch [160/1000], Loss: 189186.21875, Train RMSE: 434.9145812988281, Train R^2: -0.1152166610210954\n",
      "Epoch [161/1000], Loss: 189150.703125, Train RMSE: 434.8734130859375, Train R^2: -0.11500558749910361\n",
      "Epoch [162/1000], Loss: 189114.890625, Train RMSE: 434.83209228515625, Train R^2: -0.11479360538787775\n",
      "Epoch [163/1000], Loss: 189078.9375, Train RMSE: 434.7904052734375, Train R^2: -0.11457985327078557\n",
      "Epoch [164/1000], Loss: 189042.6875, Train RMSE: 434.7483215332031, Train R^2: -0.114364125530797\n",
      "Epoch [165/1000], Loss: 189006.109375, Train RMSE: 434.7056579589844, Train R^2: -0.11414542791607607\n",
      "Epoch [166/1000], Loss: 188969.0, Train RMSE: 434.66278076171875, Train R^2: -0.11392574069342776\n",
      "Epoch [167/1000], Loss: 188931.75, Train RMSE: 434.6194763183594, Train R^2: -0.11370361418698582\n",
      "Epoch [168/1000], Loss: 188894.0625, Train RMSE: 434.57586669921875, Train R^2: -0.11348026793206989\n",
      "Epoch [169/1000], Loss: 188856.1875, Train RMSE: 434.5320739746094, Train R^2: -0.11325583669812112\n",
      "Epoch [170/1000], Loss: 188818.109375, Train RMSE: 434.4877624511719, Train R^2: -0.11302868857535753\n",
      "Epoch [171/1000], Loss: 188779.59375, Train RMSE: 434.4432067871094, Train R^2: -0.1128004865680674\n",
      "Epoch [172/1000], Loss: 188740.890625, Train RMSE: 434.3982849121094, Train R^2: -0.11257035000311655\n",
      "Epoch [173/1000], Loss: 188701.859375, Train RMSE: 434.35302734375, Train R^2: -0.11233864086408918\n",
      "Epoch [174/1000], Loss: 188662.5625, Train RMSE: 434.3074645996094, Train R^2: -0.11210534098581504\n",
      "Epoch [175/1000], Loss: 188622.984375, Train RMSE: 434.2616271972656, Train R^2: -0.11187047123572325\n",
      "Epoch [176/1000], Loss: 188583.140625, Train RMSE: 434.2154846191406, Train R^2: -0.11163425381320025\n",
      "Epoch [177/1000], Loss: 188543.09375, Train RMSE: 434.1689758300781, Train R^2: -0.11139620277272266\n",
      "Epoch [178/1000], Loss: 188502.71875, Train RMSE: 434.1221618652344, Train R^2: -0.1111566576752343\n",
      "Epoch [179/1000], Loss: 188462.09375, Train RMSE: 434.0750732421875, Train R^2: -0.1109154629682072\n",
      "Epoch [180/1000], Loss: 188421.171875, Train RMSE: 434.02777099609375, Train R^2: -0.11067329734338616\n",
      "Epoch [181/1000], Loss: 188380.109375, Train RMSE: 433.9800109863281, Train R^2: -0.11042898255866596\n",
      "Epoch [182/1000], Loss: 188338.671875, Train RMSE: 433.9320373535156, Train R^2: -0.11018346512435784\n",
      "Epoch [183/1000], Loss: 188297.015625, Train RMSE: 433.8837585449219, Train R^2: -0.10993631309692176\n",
      "Epoch [184/1000], Loss: 188255.09375, Train RMSE: 433.83514404296875, Train R^2: -0.10968765775737288\n",
      "Epoch [185/1000], Loss: 188212.921875, Train RMSE: 433.7862243652344, Train R^2: -0.1094375119021449\n",
      "Epoch [186/1000], Loss: 188170.5, Train RMSE: 433.737060546875, Train R^2: -0.10918601283246554\n",
      "Epoch [187/1000], Loss: 188127.859375, Train RMSE: 433.68756103515625, Train R^2: -0.1089327408924945\n",
      "Epoch [188/1000], Loss: 188084.890625, Train RMSE: 433.6377258300781, Train R^2: -0.10867784946896641\n",
      "Epoch [189/1000], Loss: 188041.671875, Train RMSE: 433.5874938964844, Train R^2: -0.10842114826476479\n",
      "Epoch [190/1000], Loss: 187998.125, Train RMSE: 433.5370788574219, Train R^2: -0.10816333688506896\n",
      "Epoch [191/1000], Loss: 187954.375, Train RMSE: 433.4862976074219, Train R^2: -0.10790368091826164\n",
      "Epoch [192/1000], Loss: 187910.34375, Train RMSE: 433.4351806640625, Train R^2: -0.10764253743156882\n",
      "Epoch [193/1000], Loss: 187866.03125, Train RMSE: 433.3837890625, Train R^2: -0.10737991590385465\n",
      "Epoch [194/1000], Loss: 187821.515625, Train RMSE: 433.3320617675781, Train R^2: -0.1071155843445486\n",
      "Epoch [195/1000], Loss: 187776.6875, Train RMSE: 433.2800598144531, Train R^2: -0.10684978220015995\n",
      "Epoch [196/1000], Loss: 187731.59375, Train RMSE: 433.22772216796875, Train R^2: -0.10658251385888873\n",
      "Epoch [197/1000], Loss: 187686.265625, Train RMSE: 433.17510986328125, Train R^2: -0.1063136943784877\n",
      "Epoch [198/1000], Loss: 187640.671875, Train RMSE: 433.12213134765625, Train R^2: -0.10604317915584183\n",
      "Epoch [199/1000], Loss: 187594.8125, Train RMSE: 433.0688781738281, Train R^2: -0.1057712410007603\n",
      "Epoch [200/1000], Loss: 187548.65625, Train RMSE: 433.0152893066406, Train R^2: -0.10549749742486747\n",
      "Epoch [201/1000], Loss: 187502.234375, Train RMSE: 432.9615783691406, Train R^2: -0.10522326158634931\n",
      "Epoch [202/1000], Loss: 187455.734375, Train RMSE: 432.9071960449219, Train R^2: -0.10494577827437102\n",
      "Epoch [203/1000], Loss: 187408.65625, Train RMSE: 432.85272216796875, Train R^2: -0.10466760243652273\n",
      "Epoch [204/1000], Loss: 187361.484375, Train RMSE: 432.79791259765625, Train R^2: -0.10438782204284158\n",
      "Epoch [205/1000], Loss: 187314.015625, Train RMSE: 432.7427978515625, Train R^2: -0.10410655676439617\n",
      "Epoch [206/1000], Loss: 187266.3125, Train RMSE: 432.6873474121094, Train R^2: -0.10382359246435313\n",
      "Epoch [207/1000], Loss: 187218.328125, Train RMSE: 432.6315612792969, Train R^2: -0.10353896552697228\n",
      "Epoch [208/1000], Loss: 187170.046875, Train RMSE: 432.575439453125, Train R^2: -0.10325262978248473\n",
      "Epoch [209/1000], Loss: 187121.484375, Train RMSE: 432.5190124511719, Train R^2: -0.10296498105104246\n",
      "Epoch [210/1000], Loss: 187072.6875, Train RMSE: 432.46221923828125, Train R^2: -0.10267536813198763\n",
      "Epoch [211/1000], Loss: 187023.5625, Train RMSE: 432.4051208496094, Train R^2: -0.10238426705360038\n",
      "Epoch [212/1000], Loss: 186974.21875, Train RMSE: 432.34765625, Train R^2: -0.10209121777261143\n",
      "Epoch [213/1000], Loss: 186924.5, Train RMSE: 432.28851318359375, Train R^2: -0.10178970703588308\n",
      "Epoch [214/1000], Loss: 186873.359375, Train RMSE: 432.2283935546875, Train R^2: -0.10148320188884208\n",
      "Epoch [215/1000], Loss: 186821.375, Train RMSE: 432.16741943359375, Train R^2: -0.10117245377317663\n",
      "Epoch [216/1000], Loss: 186768.6875, Train RMSE: 432.10577392578125, Train R^2: -0.10085825634400658\n",
      "Epoch [217/1000], Loss: 186715.390625, Train RMSE: 432.04345703125, Train R^2: -0.10054080754699957\n",
      "Epoch [218/1000], Loss: 186661.546875, Train RMSE: 431.9805603027344, Train R^2: -0.10022044414042952\n",
      "Epoch [219/1000], Loss: 186607.203125, Train RMSE: 431.9171142578125, Train R^2: -0.09989740232822442\n",
      "Epoch [220/1000], Loss: 186552.40625, Train RMSE: 431.8532409667969, Train R^2: -0.09957199487433832\n",
      "Epoch [221/1000], Loss: 186497.234375, Train RMSE: 431.7888488769531, Train R^2: -0.09924413230109419\n",
      "Epoch [222/1000], Loss: 186441.609375, Train RMSE: 431.7240295410156, Train R^2: -0.09891412727973381\n",
      "Epoch [223/1000], Loss: 186385.625, Train RMSE: 431.6587829589844, Train R^2: -0.09858199017681812\n",
      "Epoch [224/1000], Loss: 186329.296875, Train RMSE: 431.59307861328125, Train R^2: -0.0982475986002822\n",
      "Epoch [225/1000], Loss: 186272.59375, Train RMSE: 431.5269775390625, Train R^2: -0.09791120755152449\n",
      "Epoch [226/1000], Loss: 186215.53125, Train RMSE: 431.4604797363281, Train R^2: -0.09757275722367598\n",
      "Epoch [227/1000], Loss: 186158.125, Train RMSE: 431.3935546875, Train R^2: -0.09723239529364536\n",
      "Epoch [228/1000], Loss: 186100.421875, Train RMSE: 431.3262939453125, Train R^2: -0.09689019273382482\n",
      "Epoch [229/1000], Loss: 186042.359375, Train RMSE: 431.2585754394531, Train R^2: -0.09654589550742765\n",
      "Epoch [230/1000], Loss: 185983.984375, Train RMSE: 431.19049072265625, Train R^2: -0.09619967159228016\n",
      "Epoch [231/1000], Loss: 185925.25, Train RMSE: 431.1220397949219, Train R^2: -0.09585161669988973\n",
      "Epoch [232/1000], Loss: 185866.203125, Train RMSE: 431.05316162109375, Train R^2: -0.0955014888979655\n",
      "Epoch [233/1000], Loss: 185806.828125, Train RMSE: 430.9839782714844, Train R^2: -0.09514973116724423\n",
      "Epoch [234/1000], Loss: 185747.171875, Train RMSE: 430.91436767578125, Train R^2: -0.09479601644962532\n",
      "Epoch [235/1000], Loss: 185687.171875, Train RMSE: 430.8443603515625, Train R^2: -0.09444045789901989\n",
      "Epoch [236/1000], Loss: 185626.859375, Train RMSE: 430.77398681640625, Train R^2: -0.0940830529140897\n",
      "Epoch [237/1000], Loss: 185566.234375, Train RMSE: 430.7032470703125, Train R^2: -0.0937237508046278\n",
      "Epoch [238/1000], Loss: 185505.3125, Train RMSE: 430.63214111328125, Train R^2: -0.09336257704313855\n",
      "Epoch [239/1000], Loss: 185444.0625, Train RMSE: 430.5606689453125, Train R^2: -0.09299954169972446\n",
      "Epoch [240/1000], Loss: 185382.46875, Train RMSE: 430.48876953125, Train R^2: -0.09263466270592624\n",
      "Epoch [241/1000], Loss: 185320.59375, Train RMSE: 430.4165344238281, Train R^2: -0.0922679707154408\n",
      "Epoch [242/1000], Loss: 185258.375, Train RMSE: 430.3439025878906, Train R^2: -0.09189938038110212\n",
      "Epoch [243/1000], Loss: 185195.875, Train RMSE: 430.2709045410156, Train R^2: -0.09152903386371669\n",
      "Epoch [244/1000], Loss: 185133.0625, Train RMSE: 430.1975402832031, Train R^2: -0.09115673224501886\n",
      "Epoch [245/1000], Loss: 185069.890625, Train RMSE: 430.1237487792969, Train R^2: -0.09078250958580103\n",
      "Epoch [246/1000], Loss: 185006.4375, Train RMSE: 430.04962158203125, Train R^2: -0.09040664228473405\n",
      "Epoch [247/1000], Loss: 184942.6875, Train RMSE: 429.97509765625, Train R^2: -0.09002869703803551\n",
      "Epoch [248/1000], Loss: 184878.59375, Train RMSE: 429.9001770019531, Train R^2: -0.08964883693439374\n",
      "Epoch [249/1000], Loss: 184814.171875, Train RMSE: 429.82489013671875, Train R^2: -0.0892672657175888\n",
      "Epoch [250/1000], Loss: 184749.453125, Train RMSE: 429.74920654296875, Train R^2: -0.08888364726382725\n",
      "Epoch [251/1000], Loss: 184684.390625, Train RMSE: 429.6731262207031, Train R^2: -0.08849813519304517\n",
      "Epoch [252/1000], Loss: 184618.984375, Train RMSE: 429.5966796875, Train R^2: -0.08811073336866992\n",
      "Epoch [253/1000], Loss: 184553.296875, Train RMSE: 429.5198059082031, Train R^2: -0.08772150862854255\n",
      "Epoch [254/1000], Loss: 184487.265625, Train RMSE: 429.4425964355469, Train R^2: -0.08733038575144292\n",
      "Epoch [255/1000], Loss: 184420.9375, Train RMSE: 429.36492919921875, Train R^2: -0.08693720596324317\n",
      "Epoch [256/1000], Loss: 184354.234375, Train RMSE: 429.2868957519531, Train R^2: -0.0865422030529095\n",
      "Epoch [257/1000], Loss: 184287.25, Train RMSE: 429.20849609375, Train R^2: -0.0861453931407079\n",
      "Epoch [258/1000], Loss: 184219.9375, Train RMSE: 429.1296691894531, Train R^2: -0.08574647691517923\n",
      "Epoch [259/1000], Loss: 184152.3125, Train RMSE: 429.0504455566406, Train R^2: -0.0853455960079248\n",
      "Epoch [260/1000], Loss: 184084.296875, Train RMSE: 428.97088623046875, Train R^2: -0.08494295226340642\n",
      "Epoch [261/1000], Loss: 184016.0, Train RMSE: 428.8908386230469, Train R^2: -0.08453823946762196\n",
      "Epoch [262/1000], Loss: 183947.359375, Train RMSE: 428.81048583984375, Train R^2: -0.08413176080469165\n",
      "Epoch [263/1000], Loss: 183878.40625, Train RMSE: 428.7296447753906, Train R^2: -0.08372311569739233\n",
      "Epoch [264/1000], Loss: 183809.125, Train RMSE: 428.6484680175781, Train R^2: -0.0833127742501718\n",
      "Epoch [265/1000], Loss: 183739.5, Train RMSE: 428.5668640136719, Train R^2: -0.08290025644670052\n",
      "Epoch [266/1000], Loss: 183669.546875, Train RMSE: 428.48486328125, Train R^2: -0.08248592164798096\n",
      "Epoch [267/1000], Loss: 183599.265625, Train RMSE: 428.4024658203125, Train R^2: -0.0820696926049489\n",
      "Epoch [268/1000], Loss: 183528.6875, Train RMSE: 428.3196716308594, Train R^2: -0.08165139025487456\n",
      "Epoch [269/1000], Loss: 183457.703125, Train RMSE: 428.23638916015625, Train R^2: -0.08123093578111895\n",
      "Epoch [270/1000], Loss: 183386.390625, Train RMSE: 428.1527404785156, Train R^2: -0.08080852206980227\n",
      "Epoch [271/1000], Loss: 183314.765625, Train RMSE: 428.06842041015625, Train R^2: -0.0803827939632904\n",
      "Epoch [272/1000], Loss: 183242.5625, Train RMSE: 427.98358154296875, Train R^2: -0.07995461507129287\n",
      "Epoch [273/1000], Loss: 183169.9375, Train RMSE: 427.8982238769531, Train R^2: -0.07952393899707277\n",
      "Epoch [274/1000], Loss: 183096.890625, Train RMSE: 427.8124694824219, Train R^2: -0.07909122563825566\n",
      "Epoch [275/1000], Loss: 183023.515625, Train RMSE: 427.7262268066406, Train R^2: -0.07865619377794775\n",
      "Epoch [276/1000], Loss: 182949.71875, Train RMSE: 427.6394958496094, Train R^2: -0.07821878955722728\n",
      "Epoch [277/1000], Loss: 182875.515625, Train RMSE: 427.55230712890625, Train R^2: -0.07777934026472133\n",
      "Epoch [278/1000], Loss: 182800.984375, Train RMSE: 427.4646301269531, Train R^2: -0.07733719046606824\n",
      "Epoch [279/1000], Loss: 182726.0, Train RMSE: 427.37640380859375, Train R^2: -0.07689258821560485\n",
      "Epoch [280/1000], Loss: 182650.59375, Train RMSE: 427.2877502441406, Train R^2: -0.07644593721566406\n",
      "Epoch [281/1000], Loss: 182574.84375, Train RMSE: 427.1986389160156, Train R^2: -0.07599691947733556\n",
      "Epoch [282/1000], Loss: 182498.671875, Train RMSE: 427.1091003417969, Train R^2: -0.07554589982922066\n",
      "Epoch [283/1000], Loss: 182422.1875, Train RMSE: 427.0190734863281, Train R^2: -0.0750925219803964\n",
      "Epoch [284/1000], Loss: 182345.28125, Train RMSE: 426.9285888671875, Train R^2: -0.07463695290695527\n",
      "Epoch [285/1000], Loss: 182268.015625, Train RMSE: 426.837646484375, Train R^2: -0.07417920859349003\n",
      "Epoch [286/1000], Loss: 182190.375, Train RMSE: 426.7462158203125, Train R^2: -0.0737191573526037\n",
      "Epoch [287/1000], Loss: 182112.359375, Train RMSE: 426.6544189453125, Train R^2: -0.07325720950832348\n",
      "Epoch [288/1000], Loss: 182034.0, Train RMSE: 426.5621032714844, Train R^2: -0.07279296048557105\n",
      "Epoch [289/1000], Loss: 181955.25, Train RMSE: 426.4693908691406, Train R^2: -0.07232647186461727\n",
      "Epoch [290/1000], Loss: 181876.140625, Train RMSE: 426.376220703125, Train R^2: -0.0718580276585481\n",
      "Epoch [291/1000], Loss: 181796.6875, Train RMSE: 426.2825622558594, Train R^2: -0.07138722947564236\n",
      "Epoch [292/1000], Loss: 181716.828125, Train RMSE: 426.1885070800781, Train R^2: -0.07091448923346277\n",
      "Epoch [293/1000], Loss: 181636.640625, Train RMSE: 426.0939636230469, Train R^2: -0.07043927151652851\n",
      "Epoch [294/1000], Loss: 181556.046875, Train RMSE: 425.9989929199219, Train R^2: -0.06996217228299306\n",
      "Epoch [295/1000], Loss: 181475.125, Train RMSE: 425.903564453125, Train R^2: -0.06948285758916417\n",
      "Epoch [296/1000], Loss: 181393.859375, Train RMSE: 425.8076477050781, Train R^2: -0.06900133704077449\n",
      "Epoch [297/1000], Loss: 181312.15625, Train RMSE: 425.7113037109375, Train R^2: -0.0685175994193652\n",
      "Epoch [298/1000], Loss: 181230.140625, Train RMSE: 425.6145324707031, Train R^2: -0.0680318159810902\n",
      "Epoch [299/1000], Loss: 181147.71875, Train RMSE: 425.5173034667969, Train R^2: -0.06754397743591922\n",
      "Epoch [300/1000], Loss: 181064.984375, Train RMSE: 425.4195861816406, Train R^2: -0.06705362692360617\n",
      "Epoch [301/1000], Loss: 180981.8125, Train RMSE: 425.3214111328125, Train R^2: -0.06656131128523013\n",
      "Epoch [302/1000], Loss: 180898.296875, Train RMSE: 425.2227783203125, Train R^2: -0.06606660973749667\n",
      "Epoch [303/1000], Loss: 180814.40625, Train RMSE: 425.12371826171875, Train R^2: -0.06557000050102202\n",
      "Epoch [304/1000], Loss: 180730.171875, Train RMSE: 425.0242004394531, Train R^2: -0.06507110789251969\n",
      "Epoch [305/1000], Loss: 180645.5625, Train RMSE: 424.9241943359375, Train R^2: -0.06456994786295289\n",
      "Epoch [306/1000], Loss: 180560.5625, Train RMSE: 424.8237609863281, Train R^2: -0.06406689077571892\n",
      "Epoch [307/1000], Loss: 180475.234375, Train RMSE: 424.72283935546875, Train R^2: -0.0635614081024003\n",
      "Epoch [308/1000], Loss: 180389.515625, Train RMSE: 424.6214599609375, Train R^2: -0.06305379491563379\n",
      "Epoch [309/1000], Loss: 180303.390625, Train RMSE: 424.5196533203125, Train R^2: -0.0625439199226494\n",
      "Epoch [310/1000], Loss: 180216.921875, Train RMSE: 424.4173889160156, Train R^2: -0.0620320568545536\n",
      "Epoch [311/1000], Loss: 180130.109375, Train RMSE: 424.3146057128906, Train R^2: -0.06151787236944184\n",
      "Epoch [312/1000], Loss: 180042.890625, Train RMSE: 424.2113952636719, Train R^2: -0.06100149173492553\n",
      "Epoch [313/1000], Loss: 179955.3125, Train RMSE: 424.1077575683594, Train R^2: -0.060483053237337936\n",
      "Epoch [314/1000], Loss: 179867.390625, Train RMSE: 424.0036315917969, Train R^2: -0.05996236431422908\n",
      "Epoch [315/1000], Loss: 179779.0625, Train RMSE: 423.8990173339844, Train R^2: -0.059439484901349315\n",
      "Epoch [316/1000], Loss: 179690.390625, Train RMSE: 423.7939758300781, Train R^2: -0.05891449195214271\n",
      "Epoch [317/1000], Loss: 179601.359375, Train RMSE: 423.6884460449219, Train R^2: -0.05838719284261851\n",
      "Epoch [318/1000], Loss: 179511.90625, Train RMSE: 423.58251953125, Train R^2: -0.057857846266863966\n",
      "Epoch [319/1000], Loss: 179422.125, Train RMSE: 423.4760437011719, Train R^2: -0.05732614738835706\n",
      "Epoch [320/1000], Loss: 179331.953125, Train RMSE: 423.3692932128906, Train R^2: -0.05679323507298317\n",
      "Epoch [321/1000], Loss: 179241.5625, Train RMSE: 423.2618408203125, Train R^2: -0.05625680973464808\n",
      "Epoch [322/1000], Loss: 179150.578125, Train RMSE: 423.1540222167969, Train R^2: -0.05571879788613443\n",
      "Epoch [323/1000], Loss: 179059.3125, Train RMSE: 423.0457763671875, Train R^2: -0.05517872427762027\n",
      "Epoch [324/1000], Loss: 178967.71875, Train RMSE: 422.9370422363281, Train R^2: -0.054636380425430175\n",
      "Epoch [325/1000], Loss: 178875.75, Train RMSE: 422.82781982421875, Train R^2: -0.0540917177665059\n",
      "Epoch [326/1000], Loss: 178783.375, Train RMSE: 422.7181396484375, Train R^2: -0.053544985102582876\n",
      "Epoch [327/1000], Loss: 178690.625, Train RMSE: 422.6080017089844, Train R^2: -0.052996002293657085\n",
      "Epoch [328/1000], Loss: 178597.515625, Train RMSE: 422.49737548828125, Train R^2: -0.05244482094659797\n",
      "Epoch [329/1000], Loss: 178504.046875, Train RMSE: 422.38629150390625, Train R^2: -0.05189149259703174\n",
      "Epoch [330/1000], Loss: 178410.1875, Train RMSE: 422.27471923828125, Train R^2: -0.051335804754330416\n",
      "Epoch [331/1000], Loss: 178315.9375, Train RMSE: 422.1626892089844, Train R^2: -0.05077808403531048\n",
      "Epoch [332/1000], Loss: 178221.34375, Train RMSE: 422.05023193359375, Train R^2: -0.05021827828644643\n",
      "Epoch [333/1000], Loss: 178126.390625, Train RMSE: 421.9372863769531, Train R^2: -0.04965622015206006\n",
      "Epoch [334/1000], Loss: 178031.0625, Train RMSE: 421.8238220214844, Train R^2: -0.04909184184674342\n",
      "Epoch [335/1000], Loss: 177935.328125, Train RMSE: 421.7100524902344, Train R^2: -0.048526024761877284\n",
      "Epoch [336/1000], Loss: 177839.375, Train RMSE: 421.5955810546875, Train R^2: -0.04795689241172174\n",
      "Epoch [337/1000], Loss: 177742.84375, Train RMSE: 421.48077392578125, Train R^2: -0.047386151716424685\n",
      "Epoch [338/1000], Loss: 177646.046875, Train RMSE: 421.3654479980469, Train R^2: -0.04681303667876335\n",
      "Epoch [339/1000], Loss: 177548.828125, Train RMSE: 421.2494201660156, Train R^2: -0.046236604317618335\n",
      "Epoch [340/1000], Loss: 177451.0625, Train RMSE: 421.13287353515625, Train R^2: -0.04565777773993185\n",
      "Epoch [341/1000], Loss: 177352.890625, Train RMSE: 421.01580810546875, Train R^2: -0.04507659237231021\n",
      "Epoch [342/1000], Loss: 177254.3125, Train RMSE: 420.8982238769531, Train R^2: -0.04449287275143532\n",
      "Epoch [343/1000], Loss: 177155.296875, Train RMSE: 420.78009033203125, Train R^2: -0.04390668844034584\n",
      "Epoch [344/1000], Loss: 177055.890625, Train RMSE: 420.6614990234375, Train R^2: -0.04331821753049914\n",
      "Epoch [345/1000], Loss: 176956.078125, Train RMSE: 420.5423278808594, Train R^2: -0.0427273587297432\n",
      "Epoch [346/1000], Loss: 176855.859375, Train RMSE: 420.42254638671875, Train R^2: -0.042133413625696114\n",
      "Epoch [347/1000], Loss: 176755.109375, Train RMSE: 420.3016052246094, Train R^2: -0.04153396128707576\n",
      "Epoch [348/1000], Loss: 176653.453125, Train RMSE: 420.1798095703125, Train R^2: -0.040930415779890206\n",
      "Epoch [349/1000], Loss: 176551.078125, Train RMSE: 420.0572814941406, Train R^2: -0.0403232178005648\n",
      "Epoch [350/1000], Loss: 176448.109375, Train RMSE: 419.9338073730469, Train R^2: -0.039711866581789934\n",
      "Epoch [351/1000], Loss: 176344.40625, Train RMSE: 419.8092346191406, Train R^2: -0.039095052296687305\n",
      "Epoch [352/1000], Loss: 176239.78125, Train RMSE: 419.68182373046875, Train R^2: -0.038464496935535664\n",
      "Epoch [353/1000], Loss: 176132.84375, Train RMSE: 419.5533447265625, Train R^2: -0.03782874792886015\n",
      "Epoch [354/1000], Loss: 176025.015625, Train RMSE: 419.42388916015625, Train R^2: -0.03718840302638626\n",
      "Epoch [355/1000], Loss: 175916.390625, Train RMSE: 419.29351806640625, Train R^2: -0.03654377497809125\n",
      "Epoch [356/1000], Loss: 175807.078125, Train RMSE: 419.1619567871094, Train R^2: -0.03589336163053858\n",
      "Epoch [357/1000], Loss: 175696.765625, Train RMSE: 419.0290222167969, Train R^2: -0.03523637996416684\n",
      "Epoch [358/1000], Loss: 175585.3125, Train RMSE: 418.8953552246094, Train R^2: -0.03457592918553387\n",
      "Epoch [359/1000], Loss: 175473.3125, Train RMSE: 418.7614440917969, Train R^2: -0.03391457028090761\n",
      "Epoch [360/1000], Loss: 175361.140625, Train RMSE: 418.62689208984375, Train R^2: -0.03325046076162508\n",
      "Epoch [361/1000], Loss: 175248.5, Train RMSE: 418.4918518066406, Train R^2: -0.032583864839236476\n",
      "Epoch [362/1000], Loss: 175135.4375, Train RMSE: 418.3560791015625, Train R^2: -0.03191388736570899\n",
      "Epoch [363/1000], Loss: 175021.796875, Train RMSE: 418.2198791503906, Train R^2: -0.031242115691028305\n",
      "Epoch [364/1000], Loss: 174907.859375, Train RMSE: 418.0831298828125, Train R^2: -0.030568052889332842\n",
      "Epoch [365/1000], Loss: 174793.53125, Train RMSE: 417.9459533691406, Train R^2: -0.029891626536290294\n",
      "Epoch [366/1000], Loss: 174678.8125, Train RMSE: 417.8082580566406, Train R^2: -0.029213151228929535\n",
      "Epoch [367/1000], Loss: 174563.734375, Train RMSE: 417.6697998046875, Train R^2: -0.028531133272103437\n",
      "Epoch [368/1000], Loss: 174448.046875, Train RMSE: 417.531005859375, Train R^2: -0.027847685607267136\n",
      "Epoch [369/1000], Loss: 174332.140625, Train RMSE: 417.39166259765625, Train R^2: -0.02716181198460732\n",
      "Epoch [370/1000], Loss: 174215.8125, Train RMSE: 417.2518310546875, Train R^2: -0.026473727476808095\n",
      "Epoch [371/1000], Loss: 174099.09375, Train RMSE: 417.11163330078125, Train R^2: -0.025783876595232957\n",
      "Epoch [372/1000], Loss: 173982.09375, Train RMSE: 416.97100830078125, Train R^2: -0.02509241823931152\n",
      "Epoch [373/1000], Loss: 173864.828125, Train RMSE: 416.82965087890625, Train R^2: -0.024397460088284806\n",
      "Epoch [374/1000], Loss: 173746.953125, Train RMSE: 416.68798828125, Train R^2: -0.02370126064107403\n",
      "Epoch [375/1000], Loss: 173628.859375, Train RMSE: 416.5458068847656, Train R^2: -0.023002850252028084\n",
      "Epoch [376/1000], Loss: 173510.40625, Train RMSE: 416.4031677246094, Train R^2: -0.02230235247608947\n",
      "Epoch [377/1000], Loss: 173391.59375, Train RMSE: 416.26007080078125, Train R^2: -0.021599739597645717\n",
      "Epoch [378/1000], Loss: 173272.453125, Train RMSE: 416.1165771484375, Train R^2: -0.02089579000212627\n",
      "Epoch [379/1000], Loss: 173153.03125, Train RMSE: 415.9725341796875, Train R^2: -0.02018888378078265\n",
      "Epoch [380/1000], Loss: 173033.140625, Train RMSE: 415.82806396484375, Train R^2: -0.019480392633576038\n",
      "Epoch [381/1000], Loss: 172912.984375, Train RMSE: 415.68316650390625, Train R^2: -0.018770126322492864\n",
      "Epoch [382/1000], Loss: 172792.515625, Train RMSE: 415.5379943847656, Train R^2: -0.018058609878561782\n",
      "Epoch [383/1000], Loss: 172671.8125, Train RMSE: 415.3919982910156, Train R^2: -0.017343361369811516\n",
      "Epoch [384/1000], Loss: 172550.515625, Train RMSE: 415.245849609375, Train R^2: -0.016627527829945032\n",
      "Epoch [385/1000], Loss: 172429.109375, Train RMSE: 415.0990905761719, Train R^2: -0.015909242042062344\n",
      "Epoch [386/1000], Loss: 172307.265625, Train RMSE: 414.9521484375, Train R^2: -0.015190057147259628\n",
      "Epoch [387/1000], Loss: 172185.28125, Train RMSE: 414.804443359375, Train R^2: -0.014467412816751235\n",
      "Epoch [388/1000], Loss: 172062.734375, Train RMSE: 414.6563415527344, Train R^2: -0.013743283503255377\n",
      "Epoch [389/1000], Loss: 171939.90625, Train RMSE: 414.5078430175781, Train R^2: -0.013017143338460713\n",
      "Epoch [390/1000], Loss: 171816.75, Train RMSE: 414.3587646484375, Train R^2: -0.012288638208508118\n",
      "Epoch [391/1000], Loss: 171693.1875, Train RMSE: 414.209228515625, Train R^2: -0.011558188490103927\n",
      "Epoch [392/1000], Loss: 171569.28125, Train RMSE: 414.0593566894531, Train R^2: -0.010826178245715523\n",
      "Epoch [393/1000], Loss: 171445.125, Train RMSE: 413.9088134765625, Train R^2: -0.010091487401448829\n",
      "Epoch [394/1000], Loss: 171320.515625, Train RMSE: 413.75787353515625, Train R^2: -0.00935485792913715\n",
      "Epoch [395/1000], Loss: 171195.59375, Train RMSE: 413.60650634765625, Train R^2: -0.008616402548218716\n",
      "Epoch [396/1000], Loss: 171070.34375, Train RMSE: 413.4549255371094, Train R^2: -0.007877305245239663\n",
      "Epoch [397/1000], Loss: 170944.984375, Train RMSE: 413.3025207519531, Train R^2: -0.007134473247512041\n",
      "Epoch [398/1000], Loss: 170818.984375, Train RMSE: 413.149658203125, Train R^2: -0.006389435674449606\n",
      "Epoch [399/1000], Loss: 170692.625, Train RMSE: 412.9963073730469, Train R^2: -0.0056425829890369705\n",
      "Epoch [400/1000], Loss: 170565.953125, Train RMSE: 412.8427429199219, Train R^2: -0.004894930275396447\n",
      "Epoch [401/1000], Loss: 170439.140625, Train RMSE: 412.68865966796875, Train R^2: -0.004144991528344244\n",
      "Epoch [402/1000], Loss: 170311.953125, Train RMSE: 412.53411865234375, Train R^2: -0.0033928675182641754\n",
      "Epoch [403/1000], Loss: 170184.390625, Train RMSE: 412.37908935546875, Train R^2: -0.0026390077036675397\n",
      "Epoch [404/1000], Loss: 170056.5, Train RMSE: 412.22369384765625, Train R^2: -0.001883436715862441\n",
      "Epoch [405/1000], Loss: 169928.359375, Train RMSE: 412.06781005859375, Train R^2: -0.0011259691802880667\n",
      "Epoch [406/1000], Loss: 169799.90625, Train RMSE: 411.91156005859375, Train R^2: -0.00036685840214722987\n",
      "Epoch [407/1000], Loss: 169671.140625, Train RMSE: 411.7548522949219, Train R^2: 0.0003942449699855244\n",
      "Epoch [408/1000], Loss: 169542.0625, Train RMSE: 411.597412109375, Train R^2: 0.001158453864084974\n",
      "Epoch [409/1000], Loss: 169412.421875, Train RMSE: 411.43963623046875, Train R^2: 0.0019242287548816295\n",
      "Epoch [410/1000], Loss: 169282.5625, Train RMSE: 411.28155517578125, Train R^2: 0.002690837857141637\n",
      "Epoch [411/1000], Loss: 169152.53125, Train RMSE: 411.1230163574219, Train R^2: 0.0034595460199445816\n",
      "Epoch [412/1000], Loss: 169022.15625, Train RMSE: 410.96405029296875, Train R^2: 0.004230117816911738\n",
      "Epoch [413/1000], Loss: 168891.453125, Train RMSE: 410.8044738769531, Train R^2: 0.005003165751682581\n",
      "Epoch [414/1000], Loss: 168760.34375, Train RMSE: 410.6446838378906, Train R^2: 0.00577712860884283\n",
      "Epoch [415/1000], Loss: 168629.078125, Train RMSE: 410.4842529296875, Train R^2: 0.006553981673099285\n",
      "Epoch [416/1000], Loss: 168497.3125, Train RMSE: 410.3233947753906, Train R^2: 0.007332350881548155\n",
      "Epoch [417/1000], Loss: 168365.3125, Train RMSE: 410.1619873046875, Train R^2: 0.008113164311906562\n",
      "Epoch [418/1000], Loss: 168232.859375, Train RMSE: 409.9999694824219, Train R^2: 0.008896536582326697\n",
      "Epoch [419/1000], Loss: 168100.015625, Train RMSE: 409.8375244140625, Train R^2: 0.009681866461744848\n",
      "Epoch [420/1000], Loss: 167966.796875, Train RMSE: 409.6741638183594, Train R^2: 0.010471119715513844\n",
      "Epoch [421/1000], Loss: 167832.9375, Train RMSE: 409.5100402832031, Train R^2: 0.01126389874456546\n",
      "Epoch [422/1000], Loss: 167698.46875, Train RMSE: 409.3450622558594, Train R^2: 0.012060275571866619\n",
      "Epoch [423/1000], Loss: 167563.390625, Train RMSE: 409.17919921875, Train R^2: 0.012860883531917056\n",
      "Epoch [424/1000], Loss: 167427.609375, Train RMSE: 409.0128479003906, Train R^2: 0.013663189566094247\n",
      "Epoch [425/1000], Loss: 167291.515625, Train RMSE: 408.8458251953125, Train R^2: 0.014468653116229024\n",
      "Epoch [426/1000], Loss: 167154.90625, Train RMSE: 408.6783752441406, Train R^2: 0.01527581746193385\n",
      "Epoch [427/1000], Loss: 167018.0, Train RMSE: 408.5099792480469, Train R^2: 0.016087100860721515\n",
      "Epoch [428/1000], Loss: 166880.40625, Train RMSE: 408.3411865234375, Train R^2: 0.016900049594281596\n",
      "Epoch [429/1000], Loss: 166742.515625, Train RMSE: 408.17181396484375, Train R^2: 0.017715355030110902\n",
      "Epoch [430/1000], Loss: 166604.234375, Train RMSE: 408.0016784667969, Train R^2: 0.018534099932616344\n",
      "Epoch [431/1000], Loss: 166465.359375, Train RMSE: 407.8310241699219, Train R^2: 0.0193549747236319\n",
      "Epoch [432/1000], Loss: 166326.140625, Train RMSE: 407.6597900390625, Train R^2: 0.020178197020102817\n",
      "Epoch [433/1000], Loss: 166186.515625, Train RMSE: 407.4880676269531, Train R^2: 0.021003653213679807\n",
      "Epoch [434/1000], Loss: 166046.515625, Train RMSE: 407.31573486328125, Train R^2: 0.021831464797803957\n",
      "Epoch [435/1000], Loss: 165906.109375, Train RMSE: 407.14263916015625, Train R^2: 0.0226626094181539\n",
      "Epoch [436/1000], Loss: 165765.140625, Train RMSE: 406.9692687988281, Train R^2: 0.023494887554174282\n",
      "Epoch [437/1000], Loss: 165623.984375, Train RMSE: 406.79547119140625, Train R^2: 0.02432875974017512\n",
      "Epoch [438/1000], Loss: 165482.5625, Train RMSE: 406.6208190917969, Train R^2: 0.025166365546461233\n",
      "Epoch [439/1000], Loss: 165340.484375, Train RMSE: 406.4454650878906, Train R^2: 0.026006954065317722\n",
      "Epoch [440/1000], Loss: 165197.90625, Train RMSE: 406.2697448730469, Train R^2: 0.0268489328019752\n",
      "Epoch [441/1000], Loss: 165055.109375, Train RMSE: 406.09381103515625, Train R^2: 0.02769158274210748\n",
      "Epoch [442/1000], Loss: 164912.1875, Train RMSE: 405.9169006347656, Train R^2: 0.028538446866852585\n",
      "Epoch [443/1000], Loss: 164768.546875, Train RMSE: 405.73944091796875, Train R^2: 0.02938793791772243\n",
      "Epoch [444/1000], Loss: 164624.484375, Train RMSE: 405.5613708496094, Train R^2: 0.030239560827249212\n",
      "Epoch [445/1000], Loss: 164480.015625, Train RMSE: 405.38287353515625, Train R^2: 0.03109295799324352\n",
      "Epoch [446/1000], Loss: 164335.28125, Train RMSE: 405.2037048339844, Train R^2: 0.031949305337821254\n",
      "Epoch [447/1000], Loss: 164190.03125, Train RMSE: 405.0240173339844, Train R^2: 0.032807611486957544\n",
      "Epoch [448/1000], Loss: 164044.453125, Train RMSE: 404.8437194824219, Train R^2: 0.03366849968197194\n",
      "Epoch [449/1000], Loss: 163898.4375, Train RMSE: 404.6624450683594, Train R^2: 0.03453376636800343\n",
      "Epoch [450/1000], Loss: 163751.6875, Train RMSE: 404.4805603027344, Train R^2: 0.03540130756436555\n",
      "Epoch [451/1000], Loss: 163604.53125, Train RMSE: 404.2982482910156, Train R^2: 0.03627079224658247\n",
      "Epoch [452/1000], Loss: 163457.0625, Train RMSE: 404.1153259277344, Train R^2: 0.03714261948888564\n",
      "Epoch [453/1000], Loss: 163309.203125, Train RMSE: 403.93182373046875, Train R^2: 0.03801695457511556\n",
      "Epoch [454/1000], Loss: 163160.890625, Train RMSE: 403.74774169921875, Train R^2: 0.03889343998938588\n",
      "Epoch [455/1000], Loss: 163012.234375, Train RMSE: 403.5632019042969, Train R^2: 0.03977193433099424\n",
      "Epoch [456/1000], Loss: 162863.234375, Train RMSE: 403.37786865234375, Train R^2: 0.040653677790964515\n",
      "Epoch [457/1000], Loss: 162713.6875, Train RMSE: 403.1919860839844, Train R^2: 0.04153760797577488\n",
      "Epoch [458/1000], Loss: 162563.765625, Train RMSE: 403.005615234375, Train R^2: 0.04242347628921683\n",
      "Epoch [459/1000], Loss: 162413.53125, Train RMSE: 402.8184814453125, Train R^2: 0.04331253401800739\n",
      "Epoch [460/1000], Loss: 162262.734375, Train RMSE: 402.6307373046875, Train R^2: 0.044204071430006375\n",
      "Epoch [461/1000], Loss: 162111.515625, Train RMSE: 402.4423828125, Train R^2: 0.04509822856159007\n",
      "Epoch [462/1000], Loss: 161959.859375, Train RMSE: 402.2533874511719, Train R^2: 0.045994759631192705\n",
      "Epoch [463/1000], Loss: 161807.8125, Train RMSE: 402.0638427734375, Train R^2: 0.0468937153179243\n",
      "Epoch [464/1000], Loss: 161655.3125, Train RMSE: 401.87359619140625, Train R^2: 0.04779541730462411\n",
      "Epoch [465/1000], Loss: 161502.390625, Train RMSE: 401.6829833984375, Train R^2: 0.04869859159932621\n",
      "Epoch [466/1000], Loss: 161349.1875, Train RMSE: 401.491455078125, Train R^2: 0.049605458297055205\n",
      "Epoch [467/1000], Loss: 161195.40625, Train RMSE: 401.2987976074219, Train R^2: 0.05051739803800637\n",
      "Epoch [468/1000], Loss: 161040.71875, Train RMSE: 401.1055908203125, Train R^2: 0.05143141102273141\n",
      "Epoch [469/1000], Loss: 160885.6875, Train RMSE: 400.911865234375, Train R^2: 0.052347545594429334\n",
      "Epoch [470/1000], Loss: 160730.3125, Train RMSE: 400.7174987792969, Train R^2: 0.05326612056202795\n",
      "Epoch [471/1000], Loss: 160574.515625, Train RMSE: 400.522216796875, Train R^2: 0.054188599302471\n",
      "Epoch [472/1000], Loss: 160418.046875, Train RMSE: 400.326416015625, Train R^2: 0.055113290185605734\n",
      "Epoch [473/1000], Loss: 160261.203125, Train RMSE: 400.13018798828125, Train R^2: 0.05603920466443091\n",
      "Epoch [474/1000], Loss: 160104.171875, Train RMSE: 399.93304443359375, Train R^2: 0.05696917431120985\n",
      "Epoch [475/1000], Loss: 159946.4375, Train RMSE: 399.73529052734375, Train R^2: 0.05790150781381487\n",
      "Epoch [476/1000], Loss: 159788.3125, Train RMSE: 399.536865234375, Train R^2: 0.058836583438699575\n",
      "Epoch [477/1000], Loss: 159629.71875, Train RMSE: 399.3379211425781, Train R^2: 0.059773714687856194\n",
      "Epoch [478/1000], Loss: 159470.765625, Train RMSE: 399.1385192871094, Train R^2: 0.06071250380015958\n",
      "Epoch [479/1000], Loss: 159311.515625, Train RMSE: 398.93841552734375, Train R^2: 0.061653981067068875\n",
      "Epoch [480/1000], Loss: 159151.859375, Train RMSE: 398.737548828125, Train R^2: 0.06259852866321569\n",
      "Epoch [481/1000], Loss: 158991.65625, Train RMSE: 398.5362243652344, Train R^2: 0.06354501265514234\n",
      "Epoch [482/1000], Loss: 158831.125, Train RMSE: 398.3343200683594, Train R^2: 0.06449357224458518\n",
      "Epoch [483/1000], Loss: 158670.21875, Train RMSE: 398.13177490234375, Train R^2: 0.06544471222275428\n",
      "Epoch [484/1000], Loss: 158508.90625, Train RMSE: 397.9286804199219, Train R^2: 0.06639793765148183\n",
      "Epoch [485/1000], Loss: 158347.265625, Train RMSE: 397.7252197265625, Train R^2: 0.06735237574501596\n",
      "Epoch [486/1000], Loss: 158185.359375, Train RMSE: 397.52142333984375, Train R^2: 0.06830793362952925\n",
      "Epoch [487/1000], Loss: 158023.28125, Train RMSE: 397.3154602050781, Train R^2: 0.06927313353750963\n",
      "Epoch [488/1000], Loss: 157859.5625, Train RMSE: 397.105712890625, Train R^2: 0.0702555943916674\n",
      "Epoch [489/1000], Loss: 157692.953125, Train RMSE: 396.8946838378906, Train R^2: 0.07124340839623933\n",
      "Epoch [490/1000], Loss: 157525.40625, Train RMSE: 396.6827697753906, Train R^2: 0.07223508435348713\n",
      "Epoch [491/1000], Loss: 157357.21875, Train RMSE: 396.46978759765625, Train R^2: 0.07323093236577305\n",
      "Epoch [492/1000], Loss: 157188.3125, Train RMSE: 396.2559814453125, Train R^2: 0.07423035186101778\n",
      "Epoch [493/1000], Loss: 157018.796875, Train RMSE: 396.04132080078125, Train R^2: 0.07523305931692958\n",
      "Epoch [494/1000], Loss: 156848.734375, Train RMSE: 395.8263244628906, Train R^2: 0.07623667101886611\n",
      "Epoch [495/1000], Loss: 156678.5, Train RMSE: 395.6089782714844, Train R^2: 0.0772510185150258\n",
      "Epoch [496/1000], Loss: 156506.4375, Train RMSE: 395.390380859375, Train R^2: 0.0782704010925811\n",
      "Epoch [497/1000], Loss: 156333.5625, Train RMSE: 395.1706848144531, Train R^2: 0.07929457373140614\n",
      "Epoch [498/1000], Loss: 156159.84375, Train RMSE: 394.9508361816406, Train R^2: 0.08031865857587517\n",
      "Epoch [499/1000], Loss: 155986.171875, Train RMSE: 394.7305908203125, Train R^2: 0.08134401031694782\n",
      "Epoch [500/1000], Loss: 155812.234375, Train RMSE: 394.5113220214844, Train R^2: 0.08236436376339307\n",
      "Epoch [501/1000], Loss: 155639.203125, Train RMSE: 394.29156494140625, Train R^2: 0.08338649047885238\n",
      "Epoch [502/1000], Loss: 155465.8125, Train RMSE: 394.0721435546875, Train R^2: 0.0844064000313135\n",
      "Epoch [503/1000], Loss: 155292.84375, Train RMSE: 393.8526611328125, Train R^2: 0.08542592764687906\n",
      "Epoch [504/1000], Loss: 155119.9375, Train RMSE: 393.6325988769531, Train R^2: 0.08644771521089911\n",
      "Epoch [505/1000], Loss: 154946.609375, Train RMSE: 393.4123229980469, Train R^2: 0.08746984412052206\n",
      "Epoch [506/1000], Loss: 154773.25, Train RMSE: 393.1915283203125, Train R^2: 0.08849383390505583\n",
      "Epoch [507/1000], Loss: 154599.5625, Train RMSE: 392.97039794921875, Train R^2: 0.08951877445673151\n",
      "Epoch [508/1000], Loss: 154425.734375, Train RMSE: 392.7486572265625, Train R^2: 0.09054590764865555\n",
      "Epoch [509/1000], Loss: 154251.515625, Train RMSE: 392.5263671875, Train R^2: 0.09157517912160595\n",
      "Epoch [510/1000], Loss: 154076.96875, Train RMSE: 392.3039245605469, Train R^2: 0.09260453224268228\n",
      "Epoch [511/1000], Loss: 153902.359375, Train RMSE: 392.0809020996094, Train R^2: 0.09363586991020478\n",
      "Epoch [512/1000], Loss: 153727.4375, Train RMSE: 391.85736083984375, Train R^2: 0.09466925305527973\n",
      "Epoch [513/1000], Loss: 153552.171875, Train RMSE: 391.63140869140625, Train R^2: 0.09571294617467319\n",
      "Epoch [514/1000], Loss: 153375.171875, Train RMSE: 391.40380859375, Train R^2: 0.09676362088616319\n",
      "Epoch [515/1000], Loss: 153196.9375, Train RMSE: 391.17523193359375, Train R^2: 0.09781826774416436\n",
      "Epoch [516/1000], Loss: 153018.0625, Train RMSE: 390.9454040527344, Train R^2: 0.09887809322329122\n",
      "Epoch [517/1000], Loss: 152838.3125, Train RMSE: 390.71453857421875, Train R^2: 0.09994209758636274\n",
      "Epoch [518/1000], Loss: 152657.859375, Train RMSE: 390.48260498046875, Train R^2: 0.10101041256057441\n",
      "Epoch [519/1000], Loss: 152476.640625, Train RMSE: 390.2499694824219, Train R^2: 0.10208121491830124\n",
      "Epoch [520/1000], Loss: 152295.046875, Train RMSE: 390.0165100097656, Train R^2: 0.10315522886427408\n",
      "Epoch [521/1000], Loss: 152112.875, Train RMSE: 389.7821960449219, Train R^2: 0.10423251570722025\n",
      "Epoch [522/1000], Loss: 151930.140625, Train RMSE: 389.5471496582031, Train R^2: 0.10531244053068456\n",
      "Epoch [523/1000], Loss: 151746.984375, Train RMSE: 389.3116149902344, Train R^2: 0.1063940865374462\n",
      "Epoch [524/1000], Loss: 151563.53125, Train RMSE: 389.0753173828125, Train R^2: 0.10747860177411095\n",
      "Epoch [525/1000], Loss: 151379.578125, Train RMSE: 388.8381042480469, Train R^2: 0.10856639283134106\n",
      "Epoch [526/1000], Loss: 151195.109375, Train RMSE: 388.6004943847656, Train R^2: 0.10965562169723198\n",
      "Epoch [527/1000], Loss: 151010.328125, Train RMSE: 388.3619384765625, Train R^2: 0.11074845265141287\n",
      "Epoch [528/1000], Loss: 150825.015625, Train RMSE: 388.1204833984375, Train R^2: 0.11185393797718202\n",
      "Epoch [529/1000], Loss: 150637.5, Train RMSE: 387.8770446777344, Train R^2: 0.11296754359281658\n",
      "Epoch [530/1000], Loss: 150448.625, Train RMSE: 387.6318664550781, Train R^2: 0.11408863704067895\n",
      "Epoch [531/1000], Loss: 150258.453125, Train RMSE: 387.3855285644531, Train R^2: 0.11521434185900759\n",
      "Epoch [532/1000], Loss: 150067.53125, Train RMSE: 387.1377258300781, Train R^2: 0.11634598527042417\n",
      "Epoch [533/1000], Loss: 149875.609375, Train RMSE: 386.88885498046875, Train R^2: 0.11748169937847064\n",
      "Epoch [534/1000], Loss: 149682.96875, Train RMSE: 386.63946533203125, Train R^2: 0.11861901235653316\n",
      "Epoch [535/1000], Loss: 149490.078125, Train RMSE: 386.3888854980469, Train R^2: 0.11976117849826884\n",
      "Epoch [536/1000], Loss: 149296.359375, Train RMSE: 386.13702392578125, Train R^2: 0.12090823458701982\n",
      "Epoch [537/1000], Loss: 149101.796875, Train RMSE: 385.8848876953125, Train R^2: 0.1220559671952205\n",
      "Epoch [538/1000], Loss: 148907.140625, Train RMSE: 385.6318664550781, Train R^2: 0.12320679157926662\n",
      "Epoch [539/1000], Loss: 148711.9375, Train RMSE: 385.37786865234375, Train R^2: 0.12436143731072813\n",
      "Epoch [540/1000], Loss: 148516.125, Train RMSE: 385.1235046386719, Train R^2: 0.12551709134908973\n",
      "Epoch [541/1000], Loss: 148320.109375, Train RMSE: 384.8686828613281, Train R^2: 0.1266738173732176\n",
      "Epoch [542/1000], Loss: 148123.921875, Train RMSE: 384.6123962402344, Train R^2: 0.12783653938648254\n",
      "Epoch [543/1000], Loss: 147926.703125, Train RMSE: 384.3559265136719, Train R^2: 0.12899936488499797\n",
      "Epoch [544/1000], Loss: 147729.484375, Train RMSE: 384.0988464355469, Train R^2: 0.13016413668329696\n",
      "Epoch [545/1000], Loss: 147531.90625, Train RMSE: 383.8415222167969, Train R^2: 0.13132918342910915\n",
      "Epoch [546/1000], Loss: 147334.3125, Train RMSE: 383.5829162597656, Train R^2: 0.13249930014948508\n",
      "Epoch [547/1000], Loss: 147135.859375, Train RMSE: 383.32379150390625, Train R^2: 0.13367100169716584\n",
      "Epoch [548/1000], Loss: 146937.125, Train RMSE: 383.064208984375, Train R^2: 0.13484384174628117\n",
      "Epoch [549/1000], Loss: 146738.203125, Train RMSE: 382.8040466308594, Train R^2: 0.13601868183472088\n",
      "Epoch [550/1000], Loss: 146538.9375, Train RMSE: 382.5430908203125, Train R^2: 0.13719621900030388\n",
      "Epoch [551/1000], Loss: 146339.21875, Train RMSE: 382.2814636230469, Train R^2: 0.13837604467792375\n",
      "Epoch [552/1000], Loss: 146139.109375, Train RMSE: 382.0196838378906, Train R^2: 0.1395555470091112\n",
      "Epoch [553/1000], Loss: 145939.0625, Train RMSE: 381.7567443847656, Train R^2: 0.14073970113466783\n",
      "Epoch [554/1000], Loss: 145738.21875, Train RMSE: 381.4933166503906, Train R^2: 0.141925169249688\n",
      "Epoch [555/1000], Loss: 145537.15625, Train RMSE: 381.2293395996094, Train R^2: 0.1431122621432186\n",
      "Epoch [556/1000], Loss: 145335.796875, Train RMSE: 380.96466064453125, Train R^2: 0.14430165486487856\n",
      "Epoch [557/1000], Loss: 145134.078125, Train RMSE: 380.6993713378906, Train R^2: 0.14549293371639105\n",
      "Epoch [558/1000], Loss: 144932.015625, Train RMSE: 380.4334411621094, Train R^2: 0.14668634944463954\n",
      "Epoch [559/1000], Loss: 144729.609375, Train RMSE: 380.167236328125, Train R^2: 0.14788020749918573\n",
      "Epoch [560/1000], Loss: 144527.109375, Train RMSE: 379.8999938964844, Train R^2: 0.14907778215579914\n",
      "Epoch [561/1000], Loss: 144324.0, Train RMSE: 379.6321105957031, Train R^2: 0.15027732103490687\n",
      "Epoch [562/1000], Loss: 144120.546875, Train RMSE: 379.3637390136719, Train R^2: 0.15147839161160193\n",
      "Epoch [563/1000], Loss: 143916.8125, Train RMSE: 379.0947570800781, Train R^2: 0.15268119846372963\n",
      "Epoch [564/1000], Loss: 143712.828125, Train RMSE: 378.8249816894531, Train R^2: 0.1538866502305858\n",
      "Epoch [565/1000], Loss: 143508.375, Train RMSE: 378.5548095703125, Train R^2: 0.1550931541808368\n",
      "Epoch [566/1000], Loss: 143303.734375, Train RMSE: 378.2838439941406, Train R^2: 0.15630222021299212\n",
      "Epoch [567/1000], Loss: 143098.671875, Train RMSE: 378.01226806640625, Train R^2: 0.15751321661809092\n",
      "Epoch [568/1000], Loss: 142893.265625, Train RMSE: 377.7399597167969, Train R^2: 0.15872649942542083\n",
      "Epoch [569/1000], Loss: 142687.5, Train RMSE: 377.46722412109375, Train R^2: 0.15994103073481591\n",
      "Epoch [570/1000], Loss: 142481.484375, Train RMSE: 377.193603515625, Train R^2: 0.16115834353116587\n",
      "Epoch [571/1000], Loss: 142275.03125, Train RMSE: 376.91949462890625, Train R^2: 0.16237716611070618\n",
      "Epoch [572/1000], Loss: 142068.296875, Train RMSE: 376.644775390625, Train R^2: 0.16359779071531155\n",
      "Epoch [573/1000], Loss: 141861.265625, Train RMSE: 376.369140625, Train R^2: 0.16482150101293647\n",
      "Epoch [574/1000], Loss: 141653.703125, Train RMSE: 376.09307861328125, Train R^2: 0.16604613834538995\n",
      "Epoch [575/1000], Loss: 141446.015625, Train RMSE: 375.8162536621094, Train R^2: 0.1672734067926288\n",
      "Epoch [576/1000], Loss: 141237.859375, Train RMSE: 375.5387878417969, Train R^2: 0.16850255768227007\n",
      "Epoch [577/1000], Loss: 141029.359375, Train RMSE: 375.2607116699219, Train R^2: 0.16973345624035818\n",
      "Epoch [578/1000], Loss: 140820.609375, Train RMSE: 374.9819030761719, Train R^2: 0.17096664806094108\n",
      "Epoch [579/1000], Loss: 140611.4375, Train RMSE: 374.7026672363281, Train R^2: 0.17220100641894487\n",
      "Epoch [580/1000], Loss: 140402.109375, Train RMSE: 374.4225769042969, Train R^2: 0.17343818630521146\n",
      "Epoch [581/1000], Loss: 140192.265625, Train RMSE: 374.1417541503906, Train R^2: 0.17467745153899483\n",
      "Epoch [582/1000], Loss: 139982.0625, Train RMSE: 373.8604431152344, Train R^2: 0.17591815345322104\n",
      "Epoch [583/1000], Loss: 139771.640625, Train RMSE: 373.5782775878906, Train R^2: 0.17716155335366746\n",
      "Epoch [584/1000], Loss: 139560.734375, Train RMSE: 373.2955322265625, Train R^2: 0.17840672081617148\n",
      "Epoch [585/1000], Loss: 139349.5625, Train RMSE: 373.0121154785156, Train R^2: 0.1796537765701216\n",
      "Epoch [586/1000], Loss: 139138.03125, Train RMSE: 372.72802734375, Train R^2: 0.18090285126500916\n",
      "Epoch [587/1000], Loss: 138926.203125, Train RMSE: 372.4432678222656, Train R^2: 0.18215403148686082\n",
      "Epoch [588/1000], Loss: 138713.984375, Train RMSE: 372.1578063964844, Train R^2: 0.183407183483567\n",
      "Epoch [589/1000], Loss: 138501.453125, Train RMSE: 371.8717041015625, Train R^2: 0.18466232253582482\n",
      "Epoch [590/1000], Loss: 138288.53125, Train RMSE: 371.5848693847656, Train R^2: 0.18591950552548786\n",
      "Epoch [591/1000], Loss: 138075.3125, Train RMSE: 371.2975158691406, Train R^2: 0.18717816340585813\n",
      "Epoch [592/1000], Loss: 137861.84375, Train RMSE: 371.0092468261719, Train R^2: 0.18843969442641795\n",
      "Epoch [593/1000], Loss: 137647.859375, Train RMSE: 370.7204284667969, Train R^2: 0.18970282644962522\n",
      "Epoch [594/1000], Loss: 137433.625, Train RMSE: 370.4309997558594, Train R^2: 0.19096756183030883\n",
      "Epoch [595/1000], Loss: 137219.125, Train RMSE: 370.1407775878906, Train R^2: 0.19223470818368826\n",
      "Epoch [596/1000], Loss: 137004.203125, Train RMSE: 369.8499755859375, Train R^2: 0.19350343422582594\n",
      "Epoch [597/1000], Loss: 136789.015625, Train RMSE: 369.5583801269531, Train R^2: 0.1947745407888708\n",
      "Epoch [598/1000], Loss: 136573.421875, Train RMSE: 369.26617431640625, Train R^2: 0.19604747768263808\n",
      "Epoch [599/1000], Loss: 136357.53125, Train RMSE: 368.9732666015625, Train R^2: 0.19732228418301256\n",
      "Epoch [600/1000], Loss: 136141.296875, Train RMSE: 368.6797180175781, Train R^2: 0.19859913186619893\n",
      "Epoch [601/1000], Loss: 135924.75, Train RMSE: 368.3854675292969, Train R^2: 0.19987780248851106\n",
      "Epoch [602/1000], Loss: 135707.859375, Train RMSE: 368.0905456542969, Train R^2: 0.20115841511150023\n",
      "Epoch [603/1000], Loss: 135490.671875, Train RMSE: 367.7948913574219, Train R^2: 0.2024412645251089\n",
      "Epoch [604/1000], Loss: 135273.0625, Train RMSE: 367.4983215332031, Train R^2: 0.20372689172598812\n",
      "Epoch [605/1000], Loss: 135055.015625, Train RMSE: 367.19891357421875, Train R^2: 0.20502386960972474\n",
      "Epoch [606/1000], Loss: 134835.046875, Train RMSE: 366.8975524902344, Train R^2: 0.20632819585276518\n",
      "Epoch [607/1000], Loss: 134613.8125, Train RMSE: 366.59454345703125, Train R^2: 0.20763863540758365\n",
      "Epoch [608/1000], Loss: 134391.546875, Train RMSE: 366.2901306152344, Train R^2: 0.208953991601867\n",
      "Epoch [609/1000], Loss: 134168.46875, Train RMSE: 365.98455810546875, Train R^2: 0.2102732851357919\n",
      "Epoch [610/1000], Loss: 133944.6875, Train RMSE: 365.6776123046875, Train R^2: 0.21159731320322384\n",
      "Epoch [611/1000], Loss: 133720.125, Train RMSE: 365.3697509765625, Train R^2: 0.21292425434115747\n",
      "Epoch [612/1000], Loss: 133495.0625, Train RMSE: 365.0608215332031, Train R^2: 0.2142546717503383\n",
      "Epoch [613/1000], Loss: 133269.40625, Train RMSE: 364.75103759765625, Train R^2: 0.21558763676523873\n",
      "Epoch [614/1000], Loss: 133043.328125, Train RMSE: 364.4402770996094, Train R^2: 0.21692371278248324\n",
      "Epoch [615/1000], Loss: 132816.71875, Train RMSE: 364.128173828125, Train R^2: 0.21826442545741864\n",
      "Epoch [616/1000], Loss: 132589.34375, Train RMSE: 363.8096923828125, Train R^2: 0.21963136010471163\n",
      "Epoch [617/1000], Loss: 132357.484375, Train RMSE: 363.4873352050781, Train R^2: 0.22101359966683187\n",
      "Epoch [618/1000], Loss: 132123.046875, Train RMSE: 363.161865234375, Train R^2: 0.22240804924785218\n",
      "Epoch [619/1000], Loss: 131886.53125, Train RMSE: 362.83380126953125, Train R^2: 0.22381224457465\n",
      "Epoch [620/1000], Loss: 131648.375, Train RMSE: 362.5034484863281, Train R^2: 0.2252249868268329\n",
      "Epoch [621/1000], Loss: 131408.765625, Train RMSE: 362.17120361328125, Train R^2: 0.2266445731132496\n",
      "Epoch [622/1000], Loss: 131167.984375, Train RMSE: 361.8371887207031, Train R^2: 0.2280703198419627\n",
      "Epoch [623/1000], Loss: 130926.15625, Train RMSE: 361.501708984375, Train R^2: 0.22950121411378022\n",
      "Epoch [624/1000], Loss: 130683.46875, Train RMSE: 361.1648864746094, Train R^2: 0.23093629839981122\n",
      "Epoch [625/1000], Loss: 130440.0703125, Train RMSE: 360.8267822265625, Train R^2: 0.2323754833227103\n",
      "Epoch [626/1000], Loss: 130195.96875, Train RMSE: 360.487548828125, Train R^2: 0.23381808134800997\n",
      "Epoch [627/1000], Loss: 129951.28125, Train RMSE: 360.14727783203125, Train R^2: 0.2352639145957458\n",
      "Epoch [628/1000], Loss: 129706.0546875, Train RMSE: 359.8060302734375, Train R^2: 0.2367123616810115\n",
      "Epoch [629/1000], Loss: 129460.3828125, Train RMSE: 359.4638977050781, Train R^2: 0.23816322382647837\n",
      "Epoch [630/1000], Loss: 129214.3046875, Train RMSE: 359.1209411621094, Train R^2: 0.23961637010800274\n",
      "Epoch [631/1000], Loss: 128967.8515625, Train RMSE: 358.77716064453125, Train R^2: 0.24107149313916865\n",
      "Epoch [632/1000], Loss: 128721.0546875, Train RMSE: 358.4326477050781, Train R^2: 0.2425282791363974\n",
      "Epoch [633/1000], Loss: 128473.953125, Train RMSE: 358.0873107910156, Train R^2: 0.2439871480094008\n",
      "Epoch [634/1000], Loss: 128226.5078125, Train RMSE: 357.74078369140625, Train R^2: 0.24544971860737586\n",
      "Epoch [635/1000], Loss: 127978.453125, Train RMSE: 357.3904724121094, Train R^2: 0.2469266852677836\n",
      "Epoch [636/1000], Loss: 127727.953125, Train RMSE: 357.0377502441406, Train R^2: 0.248412503245365\n",
      "Epoch [637/1000], Loss: 127475.9453125, Train RMSE: 356.682861328125, Train R^2: 0.24990580222751702\n",
      "Epoch [638/1000], Loss: 127222.671875, Train RMSE: 356.32611083984375, Train R^2: 0.251405486666917\n",
      "Epoch [639/1000], Loss: 126968.328125, Train RMSE: 355.9678649902344, Train R^2: 0.2529100705268116\n",
      "Epoch [640/1000], Loss: 126713.1171875, Train RMSE: 355.6082458496094, Train R^2: 0.2544188873223937\n",
      "Epoch [641/1000], Loss: 126457.203125, Train RMSE: 355.24725341796875, Train R^2: 0.25593180406624616\n",
      "Epoch [642/1000], Loss: 126200.6015625, Train RMSE: 354.8851623535156, Train R^2: 0.25744787917445144\n",
      "Epoch [643/1000], Loss: 125943.453125, Train RMSE: 354.52203369140625, Train R^2: 0.2589666278531575\n",
      "Epoch [644/1000], Loss: 125685.8671875, Train RMSE: 354.15802001953125, Train R^2: 0.2604875737977027\n",
      "Epoch [645/1000], Loss: 125427.90625, Train RMSE: 353.7930908203125, Train R^2: 0.2620107969794466\n",
      "Epoch [646/1000], Loss: 125169.5546875, Train RMSE: 353.42724609375, Train R^2: 0.26353618811457\n",
      "Epoch [647/1000], Loss: 124910.828125, Train RMSE: 353.06072998046875, Train R^2: 0.2650630792392503\n",
      "Epoch [648/1000], Loss: 124651.8671875, Train RMSE: 352.6933898925781, Train R^2: 0.2665915117779841\n",
      "Epoch [649/1000], Loss: 124392.625, Train RMSE: 352.32525634765625, Train R^2: 0.26812181535874835\n",
      "Epoch [650/1000], Loss: 124133.0546875, Train RMSE: 351.9563903808594, Train R^2: 0.26965345631983717\n",
      "Epoch [651/1000], Loss: 123873.3046875, Train RMSE: 351.5868225097656, Train R^2: 0.27118631675686267\n",
      "Epoch [652/1000], Loss: 123613.3046875, Train RMSE: 351.21673583984375, Train R^2: 0.27271976986613633\n",
      "Epoch [653/1000], Loss: 123353.1953125, Train RMSE: 350.8459167480469, Train R^2: 0.27425480687737547\n",
      "Epoch [654/1000], Loss: 123092.875, Train RMSE: 350.4743957519531, Train R^2: 0.2757910378950892\n",
      "Epoch [655/1000], Loss: 122832.296875, Train RMSE: 350.1022644042969, Train R^2: 0.27732822447833994\n",
      "Epoch [656/1000], Loss: 122571.578125, Train RMSE: 349.7292785644531, Train R^2: 0.2788670944747147\n",
      "Epoch [657/1000], Loss: 122310.5703125, Train RMSE: 349.3556823730469, Train R^2: 0.2804069675440888\n",
      "Epoch [658/1000], Loss: 122049.40625, Train RMSE: 348.98138427734375, Train R^2: 0.2819480939909418\n",
      "Epoch [659/1000], Loss: 121788.0078125, Train RMSE: 348.6064758300781, Train R^2: 0.28349007652826896\n",
      "Epoch [660/1000], Loss: 121526.4765625, Train RMSE: 348.2309875488281, Train R^2: 0.2850327590287046\n",
      "Epoch [661/1000], Loss: 121264.8203125, Train RMSE: 347.8548278808594, Train R^2: 0.286576611923435\n",
      "Epoch [662/1000], Loss: 121002.9765625, Train RMSE: 347.4775085449219, Train R^2: 0.2881234139828547\n",
      "Epoch [663/1000], Loss: 120740.6015625, Train RMSE: 347.0968322753906, Train R^2: 0.28968234966975626\n",
      "Epoch [664/1000], Loss: 120476.21875, Train RMSE: 346.71380615234375, Train R^2: 0.2912492213543807\n",
      "Epoch [665/1000], Loss: 120210.453125, Train RMSE: 346.3286437988281, Train R^2: 0.2928229528803541\n",
      "Epoch [666/1000], Loss: 119943.53125, Train RMSE: 345.941650390625, Train R^2: 0.29440242453764975\n",
      "Epoch [667/1000], Loss: 119675.6328125, Train RMSE: 345.5531921386719, Train R^2: 0.295986257120082\n",
      "Epoch [668/1000], Loss: 119407.0078125, Train RMSE: 345.1633605957031, Train R^2: 0.2975738050039294\n",
      "Epoch [669/1000], Loss: 119137.75, Train RMSE: 344.7723083496094, Train R^2: 0.2991644494359079\n",
      "Epoch [670/1000], Loss: 118867.953125, Train RMSE: 344.3800354003906, Train R^2: 0.3007584653771488\n",
      "Epoch [671/1000], Loss: 118597.6015625, Train RMSE: 343.9866943359375, Train R^2: 0.30235474423806996\n",
      "Epoch [672/1000], Loss: 118326.8515625, Train RMSE: 343.5923767089844, Train R^2: 0.30395329319014897\n",
      "Epoch [673/1000], Loss: 118055.7265625, Train RMSE: 343.1972961425781, Train R^2: 0.305553221392725\n",
      "Epoch [674/1000], Loss: 117784.3671875, Train RMSE: 342.8018798828125, Train R^2: 0.3071523834808948\n",
      "Epoch [675/1000], Loss: 117513.1171875, Train RMSE: 342.4056701660156, Train R^2: 0.3087530686151043\n",
      "Epoch [676/1000], Loss: 117241.6484375, Train RMSE: 342.0086364746094, Train R^2: 0.31035516747041547\n",
      "Epoch [677/1000], Loss: 116969.90625, Train RMSE: 341.61083984375, Train R^2: 0.3119585384300505\n",
      "Epoch [678/1000], Loss: 116697.9921875, Train RMSE: 341.2122802734375, Train R^2: 0.3135630878320387\n",
      "Epoch [679/1000], Loss: 116425.8203125, Train RMSE: 340.8130187988281, Train R^2: 0.31516864556409974\n",
      "Epoch [680/1000], Loss: 116153.5078125, Train RMSE: 340.4131774902344, Train R^2: 0.3167746196886775\n",
      "Epoch [681/1000], Loss: 115881.1171875, Train RMSE: 340.0125427246094, Train R^2: 0.318381771068594\n",
      "Epoch [682/1000], Loss: 115608.53125, Train RMSE: 339.6112060546875, Train R^2: 0.3199898575537712\n",
      "Epoch [683/1000], Loss: 115335.78125, Train RMSE: 339.2093200683594, Train R^2: 0.3215984505901057\n",
      "Epoch [684/1000], Loss: 115062.96875, Train RMSE: 338.806640625, Train R^2: 0.3232081794506311\n",
      "Epoch [685/1000], Loss: 114789.9296875, Train RMSE: 338.4033508300781, Train R^2: 0.32481835234898837\n",
      "Epoch [686/1000], Loss: 114516.828125, Train RMSE: 337.9994201660156, Train R^2: 0.32642926837805364\n",
      "Epoch [687/1000], Loss: 114243.6015625, Train RMSE: 337.5948486328125, Train R^2: 0.32804075996984394\n",
      "Epoch [688/1000], Loss: 113970.296875, Train RMSE: 337.18963623046875, Train R^2: 0.3296529311512647\n",
      "Epoch [689/1000], Loss: 113696.8515625, Train RMSE: 336.7843017578125, Train R^2: 0.33126362642110163\n",
      "Epoch [690/1000], Loss: 113423.65625, Train RMSE: 336.37835693359375, Train R^2: 0.3328747941292568\n",
      "Epoch [691/1000], Loss: 113150.3828125, Train RMSE: 335.9715881347656, Train R^2: 0.3344872462022481\n",
      "Epoch [692/1000], Loss: 112876.8984375, Train RMSE: 335.5640869140625, Train R^2: 0.3361007060294243\n",
      "Epoch [693/1000], Loss: 112603.25, Train RMSE: 335.1558837890625, Train R^2: 0.3377147852732131\n",
      "Epoch [694/1000], Loss: 112329.4921875, Train RMSE: 334.74700927734375, Train R^2: 0.3393298638580322\n",
      "Epoch [695/1000], Loss: 112055.546875, Train RMSE: 334.33740234375, Train R^2: 0.3409455435006271\n",
      "Epoch [696/1000], Loss: 111781.5078125, Train RMSE: 333.92718505859375, Train R^2: 0.34256181819912324\n",
      "Epoch [697/1000], Loss: 111507.3828125, Train RMSE: 333.5162658691406, Train R^2: 0.3441790127109522\n",
      "Epoch [698/1000], Loss: 111233.078125, Train RMSE: 333.1043395996094, Train R^2: 0.34579808354277664\n",
      "Epoch [699/1000], Loss: 110958.4921875, Train RMSE: 332.691650390625, Train R^2: 0.3474179496931722\n",
      "Epoch [700/1000], Loss: 110683.7265625, Train RMSE: 332.2782287597656, Train R^2: 0.3490387936117654\n",
      "Epoch [701/1000], Loss: 110408.828125, Train RMSE: 331.8638916015625, Train R^2: 0.3506612662088927\n",
      "Epoch [702/1000], Loss: 110133.6328125, Train RMSE: 331.4489440917969, Train R^2: 0.35228407409128415\n",
      "Epoch [703/1000], Loss: 109858.3984375, Train RMSE: 331.0331726074219, Train R^2: 0.35390801938791616\n",
      "Epoch [704/1000], Loss: 109582.953125, Train RMSE: 330.6166076660156, Train R^2: 0.3555330468294423\n",
      "Epoch [705/1000], Loss: 109307.3515625, Train RMSE: 330.1994323730469, Train R^2: 0.35715848152058294\n",
      "Epoch [706/1000], Loss: 109031.6484375, Train RMSE: 329.7815856933594, Train R^2: 0.35878435664413355\n",
      "Epoch [707/1000], Loss: 108755.90625, Train RMSE: 329.36297607421875, Train R^2: 0.36041119480029016\n",
      "Epoch [708/1000], Loss: 108479.96875, Train RMSE: 328.9436950683594, Train R^2: 0.3620385430200712\n",
      "Epoch [709/1000], Loss: 108203.9453125, Train RMSE: 328.52374267578125, Train R^2: 0.36366652480901185\n",
      "Epoch [710/1000], Loss: 107927.828125, Train RMSE: 328.1029968261719, Train R^2: 0.365295372683221\n",
      "Epoch [711/1000], Loss: 107651.578125, Train RMSE: 327.6815490722656, Train R^2: 0.3669249146467217\n",
      "Epoch [712/1000], Loss: 107375.171875, Train RMSE: 327.25933837890625, Train R^2: 0.36855520837049205\n",
      "Epoch [713/1000], Loss: 107098.671875, Train RMSE: 326.8364562988281, Train R^2: 0.37018601986282595\n",
      "Epoch [714/1000], Loss: 106822.0703125, Train RMSE: 326.41290283203125, Train R^2: 0.3718173274973462\n",
      "Epoch [715/1000], Loss: 106545.3828125, Train RMSE: 325.9886779785156, Train R^2: 0.3734491551298639\n",
      "Epoch [716/1000], Loss: 106268.6015625, Train RMSE: 325.5638122558594, Train R^2: 0.3750812203344591\n",
      "Epoch [717/1000], Loss: 105991.796875, Train RMSE: 325.1382751464844, Train R^2: 0.3767138755998788\n",
      "Epoch [718/1000], Loss: 105714.8828125, Train RMSE: 324.71209716796875, Train R^2: 0.37834671972547373\n",
      "Epoch [719/1000], Loss: 105437.9453125, Train RMSE: 324.2853698730469, Train R^2: 0.37997947361100926\n",
      "Epoch [720/1000], Loss: 105161.0078125, Train RMSE: 323.85797119140625, Train R^2: 0.38161280424165844\n",
      "Epoch [721/1000], Loss: 104883.9765625, Train RMSE: 323.4299011230469, Train R^2: 0.3832464675422105\n",
      "Epoch [722/1000], Loss: 104606.8984375, Train RMSE: 323.0011901855469, Train R^2: 0.3848803980692749\n",
      "Epoch [723/1000], Loss: 104329.7734375, Train RMSE: 322.5718994140625, Train R^2: 0.3865143740892617\n",
      "Epoch [724/1000], Loss: 104052.6328125, Train RMSE: 322.1419372558594, Train R^2: 0.38814879345573006\n",
      "Epoch [725/1000], Loss: 103775.421875, Train RMSE: 321.7128601074219, Train R^2: 0.3897775877379076\n",
      "Epoch [726/1000], Loss: 103499.171875, Train RMSE: 321.2833251953125, Train R^2: 0.39140590504858885\n",
      "Epoch [727/1000], Loss: 103222.9765625, Train RMSE: 320.8532409667969, Train R^2: 0.3930342511702233\n",
      "Epoch [728/1000], Loss: 102946.8046875, Train RMSE: 320.42236328125, Train R^2: 0.39466329383236365\n",
      "Epoch [729/1000], Loss: 102670.5, Train RMSE: 319.99090576171875, Train R^2: 0.39629251527462517\n",
      "Epoch [730/1000], Loss: 102394.1796875, Train RMSE: 319.5587463378906, Train R^2: 0.39792198425266534\n",
      "Epoch [731/1000], Loss: 102117.8046875, Train RMSE: 319.12579345703125, Train R^2: 0.3995523727882656\n",
      "Epoch [732/1000], Loss: 101841.28125, Train RMSE: 318.6922912597656, Train R^2: 0.40118262684866257\n",
      "Epoch [733/1000], Loss: 101564.78125, Train RMSE: 318.2580871582031, Train R^2: 0.40281315359050285\n",
      "Epoch [734/1000], Loss: 101288.21875, Train RMSE: 317.8233337402344, Train R^2: 0.40444362641048426\n",
      "Epoch [735/1000], Loss: 101011.671875, Train RMSE: 317.3879699707031, Train R^2: 0.40607408783167687\n",
      "Epoch [736/1000], Loss: 100735.1328125, Train RMSE: 316.9519348144531, Train R^2: 0.40770497252364835\n",
      "Epoch [737/1000], Loss: 100458.5234375, Train RMSE: 316.51519775390625, Train R^2: 0.4093360435093777\n",
      "Epoch [738/1000], Loss: 100181.875, Train RMSE: 316.0779113769531, Train R^2: 0.41096706384660275\n",
      "Epoch [739/1000], Loss: 99905.2421875, Train RMSE: 315.6397705078125, Train R^2: 0.4125988743057325\n",
      "Epoch [740/1000], Loss: 99628.46875, Train RMSE: 315.20098876953125, Train R^2: 0.41423081028814646\n",
      "Epoch [741/1000], Loss: 99351.671875, Train RMSE: 314.7615661621094, Train R^2: 0.4158630204196142\n",
      "Epoch [742/1000], Loss: 99074.84375, Train RMSE: 314.3215026855469, Train R^2: 0.41749530188619166\n",
      "Epoch [743/1000], Loss: 98798.0, Train RMSE: 313.88079833984375, Train R^2: 0.41912753101206257\n",
      "Epoch [744/1000], Loss: 98521.15625, Train RMSE: 313.4392395019531, Train R^2: 0.42076065171160093\n",
      "Epoch [745/1000], Loss: 98244.171875, Train RMSE: 312.9969787597656, Train R^2: 0.42239411173886554\n",
      "Epoch [746/1000], Loss: 97967.1015625, Train RMSE: 312.5541076660156, Train R^2: 0.4240274944530428\n",
      "Epoch [747/1000], Loss: 97690.078125, Train RMSE: 312.1105651855469, Train R^2: 0.42566112030610936\n",
      "Epoch [748/1000], Loss: 97413.0, Train RMSE: 311.6663513183594, Train R^2: 0.42729472083364595\n",
      "Epoch [749/1000], Loss: 97135.921875, Train RMSE: 311.2214660644531, Train R^2: 0.42892854251977885\n",
      "Epoch [750/1000], Loss: 96858.8046875, Train RMSE: 310.7760009765625, Train R^2: 0.43056229340043006\n",
      "Epoch [751/1000], Loss: 96581.71875, Train RMSE: 310.3299560546875, Train R^2: 0.4321956303603851\n",
      "Epoch [752/1000], Loss: 96304.6953125, Train RMSE: 309.88323974609375, Train R^2: 0.43382923451797184\n",
      "Epoch [753/1000], Loss: 96027.6171875, Train RMSE: 309.435791015625, Train R^2: 0.43546299225953755\n",
      "Epoch [754/1000], Loss: 95750.5078125, Train RMSE: 308.9886779785156, Train R^2: 0.43709323901996355\n",
      "Epoch [755/1000], Loss: 95474.0078125, Train RMSE: 308.5413513183594, Train R^2: 0.4387219279099569\n",
      "Epoch [756/1000], Loss: 95197.7734375, Train RMSE: 308.0932922363281, Train R^2: 0.4403508871445678\n",
      "Epoch [757/1000], Loss: 94921.4921875, Train RMSE: 307.6446533203125, Train R^2: 0.44197965853599264\n",
      "Epoch [758/1000], Loss: 94645.2265625, Train RMSE: 307.19537353515625, Train R^2: 0.4436082630255551\n",
      "Epoch [759/1000], Loss: 94368.9921875, Train RMSE: 306.74554443359375, Train R^2: 0.4452365675204901\n",
      "Epoch [760/1000], Loss: 94092.828125, Train RMSE: 306.29547119140625, Train R^2: 0.44686330620967485\n",
      "Epoch [761/1000], Loss: 93816.90625, Train RMSE: 305.8447570800781, Train R^2: 0.44848995845570094\n",
      "Epoch [762/1000], Loss: 93541.03125, Train RMSE: 305.3934020996094, Train R^2: 0.45011665813377966\n",
      "Epoch [763/1000], Loss: 93265.1171875, Train RMSE: 304.9413757324219, Train R^2: 0.45174318770050137\n",
      "Epoch [764/1000], Loss: 92989.2421875, Train RMSE: 304.4888610839844, Train R^2: 0.45336920797589286\n",
      "Epoch [765/1000], Loss: 92713.453125, Train RMSE: 304.0356750488281, Train R^2: 0.4549951126847386\n",
      "Epoch [766/1000], Loss: 92437.6953125, Train RMSE: 303.5819091796875, Train R^2: 0.45662068003803025\n",
      "Epoch [767/1000], Loss: 92161.9765625, Train RMSE: 303.1275634765625, Train R^2: 0.45824594078205827\n",
      "Epoch [768/1000], Loss: 91886.3203125, Train RMSE: 302.67364501953125, Train R^2: 0.45986720721546137\n",
      "Epoch [769/1000], Loss: 91611.34375, Train RMSE: 302.2191162109375, Train R^2: 0.46148822422921365\n",
      "Epoch [770/1000], Loss: 91336.3984375, Train RMSE: 301.7639465332031, Train R^2: 0.4631091438137379\n",
      "Epoch [771/1000], Loss: 91061.4921875, Train RMSE: 301.30828857421875, Train R^2: 0.46472935818323835\n",
      "Epoch [772/1000], Loss: 90786.6796875, Train RMSE: 300.8519592285156, Train R^2: 0.4663493880831271\n",
      "Epoch [773/1000], Loss: 90511.90625, Train RMSE: 300.3950500488281, Train R^2: 0.4679691032055622\n",
      "Epoch [774/1000], Loss: 90237.1875, Train RMSE: 299.9375305175781, Train R^2: 0.4695884658761341\n",
      "Epoch [775/1000], Loss: 89962.5390625, Train RMSE: 299.4794616699219, Train R^2: 0.4712073241711028\n",
      "Epoch [776/1000], Loss: 89687.953125, Train RMSE: 299.0210266113281, Train R^2: 0.47282504437808714\n",
      "Epoch [777/1000], Loss: 89413.5703125, Train RMSE: 298.56195068359375, Train R^2: 0.474442471772592\n",
      "Epoch [778/1000], Loss: 89139.25, Train RMSE: 298.1022644042969, Train R^2: 0.47605964158292424\n",
      "Epoch [779/1000], Loss: 88864.9609375, Train RMSE: 297.64190673828125, Train R^2: 0.47767659117395245\n",
      "Epoch [780/1000], Loss: 88590.7109375, Train RMSE: 297.1811828613281, Train R^2: 0.4792924378878919\n",
      "Epoch [781/1000], Loss: 88316.6484375, Train RMSE: 296.7200927734375, Train R^2: 0.48090691502515825\n",
      "Epoch [782/1000], Loss: 88042.8203125, Train RMSE: 296.2584533691406, Train R^2: 0.48252086496331825\n",
      "Epoch [783/1000], Loss: 87769.0703125, Train RMSE: 295.7959899902344, Train R^2: 0.48413517787948845\n",
      "Epoch [784/1000], Loss: 87495.2734375, Train RMSE: 295.3327331542969, Train R^2: 0.48574971660913735\n",
      "Epoch [785/1000], Loss: 87221.4453125, Train RMSE: 294.8691101074219, Train R^2: 0.4873631317705074\n",
      "Epoch [786/1000], Loss: 86947.78125, Train RMSE: 294.4049072265625, Train R^2: 0.4889759417163225\n",
      "Epoch [787/1000], Loss: 86674.2421875, Train RMSE: 293.9399719238281, Train R^2: 0.49058863080898896\n",
      "Epoch [788/1000], Loss: 86400.71875, Train RMSE: 293.474365234375, Train R^2: 0.49220122168973646\n",
      "Epoch [789/1000], Loss: 86127.203125, Train RMSE: 293.0082092285156, Train R^2: 0.49381315951230353\n",
      "Epoch [790/1000], Loss: 85853.8046875, Train RMSE: 292.5415954589844, Train R^2: 0.49542404867650514\n",
      "Epoch [791/1000], Loss: 85580.5703125, Train RMSE: 292.0745849609375, Train R^2: 0.4970337169171165\n",
      "Epoch [792/1000], Loss: 85307.5703125, Train RMSE: 291.6068115234375, Train R^2: 0.49864345195734394\n",
      "Epoch [793/1000], Loss: 85034.5390625, Train RMSE: 291.13824462890625, Train R^2: 0.5002534489191999\n",
      "Epoch [794/1000], Loss: 84761.46875, Train RMSE: 290.6690368652344, Train R^2: 0.5018629715329554\n",
      "Epoch [795/1000], Loss: 84488.4765625, Train RMSE: 290.1993408203125, Train R^2: 0.5034715436409289\n",
      "Epoch [796/1000], Loss: 84215.65625, Train RMSE: 289.7293395996094, Train R^2: 0.5050785843901595\n",
      "Epoch [797/1000], Loss: 83943.0859375, Train RMSE: 289.2588806152344, Train R^2: 0.5066845589358231\n",
      "Epoch [798/1000], Loss: 83670.703125, Train RMSE: 288.7879333496094, Train R^2: 0.5082895933226292\n",
      "Epoch [799/1000], Loss: 83398.4765625, Train RMSE: 288.3165588378906, Train R^2: 0.5098934686950546\n",
      "Epoch [800/1000], Loss: 83126.4453125, Train RMSE: 287.8445129394531, Train R^2: 0.511496949807982\n",
      "Epoch [801/1000], Loss: 82854.4765625, Train RMSE: 287.37200927734375, Train R^2: 0.5130994487536261\n",
      "Epoch [802/1000], Loss: 82582.6796875, Train RMSE: 286.89910888671875, Train R^2: 0.514700656857698\n",
      "Epoch [803/1000], Loss: 82311.1015625, Train RMSE: 286.4257507324219, Train R^2: 0.5163007817933771\n",
      "Epoch [804/1000], Loss: 82039.703125, Train RMSE: 285.95184326171875, Train R^2: 0.517900044104284\n",
      "Epoch [805/1000], Loss: 81768.453125, Train RMSE: 285.4774169921875, Train R^2: 0.5194984685453318\n",
      "Epoch [806/1000], Loss: 81497.34375, Train RMSE: 285.0024108886719, Train R^2: 0.5210961342521546\n",
      "Epoch [807/1000], Loss: 81226.3671875, Train RMSE: 284.5269775390625, Train R^2: 0.5226925362206145\n",
      "Epoch [808/1000], Loss: 80955.6015625, Train RMSE: 284.05120849609375, Train R^2: 0.5242874405266882\n",
      "Epoch [809/1000], Loss: 80685.09375, Train RMSE: 283.5750427246094, Train R^2: 0.5258809856290894\n",
      "Epoch [810/1000], Loss: 80414.8203125, Train RMSE: 283.09844970703125, Train R^2: 0.5274733934416098\n",
      "Epoch [811/1000], Loss: 80144.71875, Train RMSE: 282.6214599609375, Train R^2: 0.5290644049656555\n",
      "Epoch [812/1000], Loss: 79874.8828125, Train RMSE: 282.14398193359375, Train R^2: 0.5306542829881196\n",
      "Epoch [813/1000], Loss: 79605.21875, Train RMSE: 281.666259765625, Train R^2: 0.5322422824754323\n",
      "Epoch [814/1000], Loss: 79335.8828125, Train RMSE: 281.1878967285156, Train R^2: 0.5338297581523637\n",
      "Epoch [815/1000], Loss: 79066.6328125, Train RMSE: 280.7093200683594, Train R^2: 0.5354151864901451\n",
      "Epoch [816/1000], Loss: 78797.7265625, Train RMSE: 280.2304382324219, Train R^2: 0.536999051740307\n",
      "Epoch [817/1000], Loss: 78529.09375, Train RMSE: 279.7509460449219, Train R^2: 0.5385821301733476\n",
      "Epoch [818/1000], Loss: 78260.59375, Train RMSE: 279.2712097167969, Train R^2: 0.5401632533486223\n",
      "Epoch [819/1000], Loss: 77992.4140625, Train RMSE: 278.79107666015625, Train R^2: 0.5417429914688439\n",
      "Epoch [820/1000], Loss: 77724.4765625, Train RMSE: 278.3103942871094, Train R^2: 0.5433218633325916\n",
      "Epoch [821/1000], Loss: 77456.6796875, Train RMSE: 277.8277587890625, Train R^2: 0.5449045122198005\n",
      "Epoch [822/1000], Loss: 77188.25, Train RMSE: 277.3437805175781, Train R^2: 0.5464886063616698\n",
      "Epoch [823/1000], Loss: 76919.578125, Train RMSE: 276.858642578125, Train R^2: 0.5480738474206\n",
      "Epoch [824/1000], Loss: 76650.703125, Train RMSE: 276.37261962890625, Train R^2: 0.549659122580277\n",
      "Epoch [825/1000], Loss: 76381.8359375, Train RMSE: 275.8856201171875, Train R^2: 0.5512447780235674\n",
      "Epoch [826/1000], Loss: 76112.890625, Train RMSE: 275.3978271484375, Train R^2: 0.5528302876953137\n",
      "Epoch [827/1000], Loss: 75843.96875, Train RMSE: 274.90936279296875, Train R^2: 0.554415193599965\n",
      "Epoch [828/1000], Loss: 75575.15625, Train RMSE: 274.4203186035156, Train R^2: 0.5559991521840014\n",
      "Epoch [829/1000], Loss: 75306.5078125, Train RMSE: 273.9305114746094, Train R^2: 0.5575826205997239\n",
      "Epoch [830/1000], Loss: 75037.9296875, Train RMSE: 273.4400634765625, Train R^2: 0.5591654616121645\n",
      "Epoch [831/1000], Loss: 74769.46875, Train RMSE: 272.948974609375, Train R^2: 0.5607474791259508\n",
      "Epoch [832/1000], Loss: 74501.140625, Train RMSE: 272.4574279785156, Train R^2: 0.562328139275016\n",
      "Epoch [833/1000], Loss: 74233.0546875, Train RMSE: 271.9655456542969, Train R^2: 0.5639070733330198\n",
      "Epoch [834/1000], Loss: 73965.2578125, Train RMSE: 271.4731750488281, Train R^2: 0.5654846683673127\n",
      "Epoch [835/1000], Loss: 73697.6796875, Train RMSE: 270.98040771484375, Train R^2: 0.5670606390829014\n",
      "Epoch [836/1000], Loss: 73430.375, Train RMSE: 270.4870300292969, Train R^2: 0.5686357208154564\n",
      "Epoch [837/1000], Loss: 73163.2265625, Train RMSE: 269.9931335449219, Train R^2: 0.5702095357788791\n",
      "Epoch [838/1000], Loss: 72896.3046875, Train RMSE: 269.4990539550781, Train R^2: 0.5717811725278167\n",
      "Epoch [839/1000], Loss: 72629.734375, Train RMSE: 269.0045166015625, Train R^2: 0.5733512765012856\n",
      "Epoch [840/1000], Loss: 72363.4296875, Train RMSE: 268.5095520019531, Train R^2: 0.5749198729839546\n",
      "Epoch [841/1000], Loss: 72097.3828125, Train RMSE: 268.014404296875, Train R^2: 0.5764861770186699\n",
      "Epoch [842/1000], Loss: 71831.71875, Train RMSE: 267.5186767578125, Train R^2: 0.5780514620719064\n",
      "Epoch [843/1000], Loss: 71566.234375, Train RMSE: 267.0223083496094, Train R^2: 0.5796157803967705\n",
      "Epoch [844/1000], Loss: 71300.90625, Train RMSE: 266.5257873535156, Train R^2: 0.5811777662827482\n",
      "Epoch [845/1000], Loss: 71035.9921875, Train RMSE: 266.0288391113281, Train R^2: 0.5827381481193912\n",
      "Epoch [846/1000], Loss: 70771.3203125, Train RMSE: 265.531494140625, Train R^2: 0.5842968325014598\n",
      "Epoch [847/1000], Loss: 70506.9609375, Train RMSE: 265.03363037109375, Train R^2: 0.5858541510072703\n",
      "Epoch [848/1000], Loss: 70242.8359375, Train RMSE: 264.53546142578125, Train R^2: 0.5874096219559892\n",
      "Epoch [849/1000], Loss: 69979.0078125, Train RMSE: 264.0367431640625, Train R^2: 0.5889637603431894\n",
      "Epoch [850/1000], Loss: 69715.4140625, Train RMSE: 263.5379333496094, Train R^2: 0.59051542702179\n",
      "Epoch [851/1000], Loss: 69452.2421875, Train RMSE: 263.03875732421875, Train R^2: 0.5920652025527946\n",
      "Epoch [852/1000], Loss: 69189.3828125, Train RMSE: 262.5391845703125, Train R^2: 0.5936132174601867\n",
      "Epoch [853/1000], Loss: 68926.8203125, Train RMSE: 262.039306640625, Train R^2: 0.5951592893457114\n",
      "Epoch [854/1000], Loss: 68664.59375, Train RMSE: 261.53912353515625, Train R^2: 0.596703336373902\n",
      "Epoch [855/1000], Loss: 68402.71875, Train RMSE: 261.0384521484375, Train R^2: 0.5982459091280166\n",
      "Epoch [856/1000], Loss: 68141.0703125, Train RMSE: 260.537353515625, Train R^2: 0.5997868296461132\n",
      "Epoch [857/1000], Loss: 67879.71875, Train RMSE: 260.03607177734375, Train R^2: 0.601325444317641\n",
      "Epoch [858/1000], Loss: 67618.7578125, Train RMSE: 259.534423828125, Train R^2: 0.6028622156464178\n",
      "Epoch [859/1000], Loss: 67358.109375, Train RMSE: 259.0323791503906, Train R^2: 0.6043971771719024\n",
      "Epoch [860/1000], Loss: 67097.765625, Train RMSE: 258.53118896484375, Train R^2: 0.6059265237773643\n",
      "Epoch [861/1000], Loss: 66838.3671875, Train RMSE: 258.0299377441406, Train R^2: 0.607453069077367\n",
      "Epoch [862/1000], Loss: 66579.4609375, Train RMSE: 257.52838134765625, Train R^2: 0.6089777392414176\n",
      "Epoch [863/1000], Loss: 66320.8671875, Train RMSE: 257.0263671875, Train R^2: 0.6105006878112602\n",
      "Epoch [864/1000], Loss: 66062.5546875, Train RMSE: 256.52410888671875, Train R^2: 0.6120215050255573\n",
      "Epoch [865/1000], Loss: 65804.6171875, Train RMSE: 256.0213928222656, Train R^2: 0.6135405838015109\n",
      "Epoch [866/1000], Loss: 65546.96875, Train RMSE: 255.51837158203125, Train R^2: 0.615057764304012\n",
      "Epoch [867/1000], Loss: 65289.63671875, Train RMSE: 255.01559448242188, Train R^2: 0.6165711430467122\n",
      "Epoch [868/1000], Loss: 65032.953125, Train RMSE: 254.5131072998047, Train R^2: 0.6180807133073181\n",
      "Epoch [869/1000], Loss: 64776.9140625, Train RMSE: 254.0103302001953, Train R^2: 0.6195881038177479\n",
      "Epoch [870/1000], Loss: 64521.25, Train RMSE: 253.5071258544922, Train R^2: 0.6210938532476032\n",
      "Epoch [871/1000], Loss: 64265.86328125, Train RMSE: 253.00352478027344, Train R^2: 0.6225977677556571\n",
      "Epoch [872/1000], Loss: 64010.77734375, Train RMSE: 252.4998016357422, Train R^2: 0.6240990699529587\n",
      "Epoch [873/1000], Loss: 63756.15234375, Train RMSE: 251.99562072753906, Train R^2: 0.625598782219736\n",
      "Epoch [874/1000], Loss: 63501.796875, Train RMSE: 251.49102783203125, Train R^2: 0.627096654498855\n",
      "Epoch [875/1000], Loss: 63247.73828125, Train RMSE: 250.98631286621094, Train R^2: 0.6285918984665997\n",
      "Epoch [876/1000], Loss: 62994.13671875, Train RMSE: 250.481201171875, Train R^2: 0.630085268982967\n",
      "Epoch [877/1000], Loss: 62740.8359375, Train RMSE: 249.97572326660156, Train R^2: 0.6315768219448907\n",
      "Epoch [878/1000], Loss: 62487.8515625, Train RMSE: 249.47003173828125, Train R^2: 0.6330659084730375\n",
      "Epoch [879/1000], Loss: 62235.30078125, Train RMSE: 248.964111328125, Train R^2: 0.6345526464846141\n",
      "Epoch [880/1000], Loss: 61983.12109375, Train RMSE: 248.45779418945312, Train R^2: 0.6360375947940595\n",
      "Epoch [881/1000], Loss: 61731.26171875, Train RMSE: 247.9513397216797, Train R^2: 0.6375198944483014\n",
      "Epoch [882/1000], Loss: 61479.86328125, Train RMSE: 247.44448852539062, Train R^2: 0.6390002236076979\n",
      "Epoch [883/1000], Loss: 61228.78515625, Train RMSE: 246.9374542236328, Train R^2: 0.6404781598170257\n",
      "Epoch [884/1000], Loss: 60978.11328125, Train RMSE: 246.43023681640625, Train R^2: 0.6419536477454888\n",
      "Epoch [885/1000], Loss: 60727.8515625, Train RMSE: 245.92294311523438, Train R^2: 0.6434262295731008\n",
      "Epoch [886/1000], Loss: 60478.09765625, Train RMSE: 245.41531372070312, Train R^2: 0.6448967937921966\n",
      "Epoch [887/1000], Loss: 60228.67578125, Train RMSE: 244.9071044921875, Train R^2: 0.6463659440755045\n",
      "Epoch [888/1000], Loss: 59979.484375, Train RMSE: 244.39842224121094, Train R^2: 0.6478334694634575\n",
      "Epoch [889/1000], Loss: 59730.5859375, Train RMSE: 243.8895721435547, Train R^2: 0.6492984054896436\n",
      "Epoch [890/1000], Loss: 59482.12109375, Train RMSE: 243.38021850585938, Train R^2: 0.6507616810171425\n",
      "Epoch [891/1000], Loss: 59233.9375, Train RMSE: 242.87054443359375, Train R^2: 0.6522229395122736\n",
      "Epoch [892/1000], Loss: 58986.08984375, Train RMSE: 242.36077880859375, Train R^2: 0.6536812968979383\n",
      "Epoch [893/1000], Loss: 58738.73828125, Train RMSE: 241.85055541992188, Train R^2: 0.6551378982955285\n",
      "Epoch [894/1000], Loss: 58491.6875, Train RMSE: 241.3400115966797, Train R^2: 0.6565923674638852\n",
      "Epoch [895/1000], Loss: 58244.99609375, Train RMSE: 240.829345703125, Train R^2: 0.658044066019316\n",
      "Epoch [896/1000], Loss: 57998.77734375, Train RMSE: 240.31832885742188, Train R^2: 0.6594937282344098\n",
      "Epoch [897/1000], Loss: 57752.91015625, Train RMSE: 239.80722045898438, Train R^2: 0.6609405660974433\n",
      "Epoch [898/1000], Loss: 57507.5, Train RMSE: 239.2960968017578, Train R^2: 0.662384350762572\n",
      "Epoch [899/1000], Loss: 57262.62890625, Train RMSE: 238.78465270996094, Train R^2: 0.6638260367554654\n",
      "Epoch [900/1000], Loss: 57018.109375, Train RMSE: 238.27276611328125, Train R^2: 0.66526579399284\n",
      "Epoch [901/1000], Loss: 56773.921875, Train RMSE: 237.76077270507812, Train R^2: 0.6667028101680974\n",
      "Epoch [902/1000], Loss: 56530.17578125, Train RMSE: 237.24871826171875, Train R^2: 0.6681368434057012\n",
      "Epoch [903/1000], Loss: 56286.94921875, Train RMSE: 236.73643493652344, Train R^2: 0.6695684280311055\n",
      "Epoch [904/1000], Loss: 56044.140625, Train RMSE: 236.2240447998047, Train R^2: 0.6709973134169189\n",
      "Epoch [905/1000], Loss: 55801.7890625, Train RMSE: 235.71168518066406, Train R^2: 0.6724228788615071\n",
      "Epoch [906/1000], Loss: 55560.0, Train RMSE: 235.19912719726562, Train R^2: 0.6738459843174563\n",
      "Epoch [907/1000], Loss: 55318.63671875, Train RMSE: 234.68634033203125, Train R^2: 0.6752666559439902\n",
      "Epoch [908/1000], Loss: 55077.67578125, Train RMSE: 234.17312622070312, Train R^2: 0.6766853315523107\n",
      "Epoch [909/1000], Loss: 54837.05078125, Train RMSE: 233.65982055664062, Train R^2: 0.6781011686664267\n",
      "Epoch [910/1000], Loss: 54596.9140625, Train RMSE: 233.14651489257812, Train R^2: 0.6795139047903094\n",
      "Epoch [911/1000], Loss: 54357.296875, Train RMSE: 232.6329803466797, Train R^2: 0.6809242278362326\n",
      "Epoch [912/1000], Loss: 54118.09765625, Train RMSE: 232.119384765625, Train R^2: 0.6823315338368549\n",
      "Epoch [913/1000], Loss: 53879.41015625, Train RMSE: 231.6056671142578, Train R^2: 0.6837360753507955\n",
      "Epoch [914/1000], Loss: 53641.18359375, Train RMSE: 231.09164428710938, Train R^2: 0.6851383153467738\n",
      "Epoch [915/1000], Loss: 53403.34765625, Train RMSE: 230.57769775390625, Train R^2: 0.6865372632463442\n",
      "Epoch [916/1000], Loss: 53166.0859375, Train RMSE: 230.06365966796875, Train R^2: 0.6879333760689048\n",
      "Epoch [917/1000], Loss: 52929.28515625, Train RMSE: 229.5495147705078, Train R^2: 0.689326648607405\n",
      "Epoch [918/1000], Loss: 52692.97265625, Train RMSE: 229.03512573242188, Train R^2: 0.6907174302269681\n",
      "Epoch [919/1000], Loss: 52457.078125, Train RMSE: 228.52072143554688, Train R^2: 0.6921051179687042\n",
      "Epoch [920/1000], Loss: 52221.72265625, Train RMSE: 228.00619506835938, Train R^2: 0.6934900278246835\n",
      "Epoch [921/1000], Loss: 51986.82421875, Train RMSE: 227.4916534423828, Train R^2: 0.6948718842268207\n",
      "Epoch [922/1000], Loss: 51752.44921875, Train RMSE: 226.9769287109375, Train R^2: 0.6962510516106791\n",
      "Epoch [923/1000], Loss: 51518.546875, Train RMSE: 226.46217346191406, Train R^2: 0.6976272585500586\n",
      "Epoch [924/1000], Loss: 51285.11328125, Train RMSE: 225.947265625, Train R^2: 0.6990007505584431\n",
      "Epoch [925/1000], Loss: 51052.16015625, Train RMSE: 225.43222045898438, Train R^2: 0.7003713755185303\n",
      "Epoch [926/1000], Loss: 50819.6875, Train RMSE: 224.91737365722656, Train R^2: 0.7017384371474537\n",
      "Epoch [927/1000], Loss: 50587.82421875, Train RMSE: 224.4025115966797, Train R^2: 0.7031023382144344\n",
      "Epoch [928/1000], Loss: 50356.49609375, Train RMSE: 223.88729858398438, Train R^2: 0.7044641081496388\n",
      "Epoch [929/1000], Loss: 50125.5234375, Train RMSE: 223.37210083007812, Train R^2: 0.7058226937793024\n",
      "Epoch [930/1000], Loss: 49895.09765625, Train RMSE: 222.85694885253906, Train R^2: 0.7071780144545363\n",
      "Epoch [931/1000], Loss: 49665.22265625, Train RMSE: 222.34193420410156, Train R^2: 0.708529878037214\n",
      "Epoch [932/1000], Loss: 49435.9375, Train RMSE: 221.8267364501953, Train R^2: 0.7098790590565973\n",
      "Epoch [933/1000], Loss: 49207.1015625, Train RMSE: 221.31146240234375, Train R^2: 0.7112252798883167\n",
      "Epoch [934/1000], Loss: 48978.76171875, Train RMSE: 220.79627990722656, Train R^2: 0.712568225036452\n",
      "Epoch [935/1000], Loss: 48750.98828125, Train RMSE: 220.2809600830078, Train R^2: 0.7139083255235268\n",
      "Epoch [936/1000], Loss: 48523.69921875, Train RMSE: 219.7656707763672, Train R^2: 0.7152452056278302\n",
      "Epoch [937/1000], Loss: 48296.953125, Train RMSE: 219.25033569335938, Train R^2: 0.7165791274258719\n",
      "Epoch [938/1000], Loss: 48070.7109375, Train RMSE: 218.7349395751953, Train R^2: 0.7179100501335155\n",
      "Epoch [939/1000], Loss: 47844.9765625, Train RMSE: 218.21949768066406, Train R^2: 0.7192379678512826\n",
      "Epoch [940/1000], Loss: 47619.74609375, Train RMSE: 217.70420837402344, Train R^2: 0.7205623270155557\n",
      "Epoch [941/1000], Loss: 47395.125, Train RMSE: 217.18875122070312, Train R^2: 0.7218839999335811\n",
      "Epoch [942/1000], Loss: 47170.95703125, Train RMSE: 216.6734619140625, Train R^2: 0.7232021040046961\n",
      "Epoch [943/1000], Loss: 46947.390625, Train RMSE: 216.15818786621094, Train R^2: 0.7245170713120435\n",
      "Epoch [944/1000], Loss: 46724.359375, Train RMSE: 215.6427764892578, Train R^2: 0.7258292293711983\n",
      "Epoch [945/1000], Loss: 46501.80859375, Train RMSE: 215.12745666503906, Train R^2: 0.7271380458658978\n",
      "Epoch [946/1000], Loss: 46279.8203125, Train RMSE: 214.61224365234375, Train R^2: 0.7284434182086328\n",
      "Epoch [947/1000], Loss: 46058.421875, Train RMSE: 214.09701538085938, Train R^2: 0.7297457108774126\n",
      "Epoch [948/1000], Loss: 45837.53515625, Train RMSE: 213.58184814453125, Train R^2: 0.731044784864423\n",
      "Epoch [949/1000], Loss: 45617.203125, Train RMSE: 213.06666564941406, Train R^2: 0.7323407212633609\n",
      "Epoch [950/1000], Loss: 45397.40234375, Train RMSE: 212.5514678955078, Train R^2: 0.7336335413988899\n",
      "Epoch [951/1000], Loss: 45178.12890625, Train RMSE: 212.03651428222656, Train R^2: 0.7349226615936696\n",
      "Epoch [952/1000], Loss: 44959.484375, Train RMSE: 211.52149963378906, Train R^2: 0.7362087704387743\n",
      "Epoch [953/1000], Loss: 44741.34765625, Train RMSE: 211.00653076171875, Train R^2: 0.7374916607248945\n",
      "Epoch [954/1000], Loss: 44523.75390625, Train RMSE: 210.49172973632812, Train R^2: 0.7387710218192671\n",
      "Epoch [955/1000], Loss: 44306.765625, Train RMSE: 209.9769287109375, Train R^2: 0.7400472474235127\n",
      "Epoch [956/1000], Loss: 44090.30859375, Train RMSE: 209.46221923828125, Train R^2: 0.7413200833634133\n",
      "Epoch [957/1000], Loss: 43874.421875, Train RMSE: 208.94761657714844, Train R^2: 0.7425895700604197\n",
      "Epoch [958/1000], Loss: 43659.10546875, Train RMSE: 208.43304443359375, Train R^2: 0.7438558418507155\n",
      "Epoch [959/1000], Loss: 43444.33984375, Train RMSE: 207.91876220703125, Train R^2: 0.7451183070607115\n",
      "Epoch [960/1000], Loss: 43230.2109375, Train RMSE: 207.4043426513672, Train R^2: 0.7463779796805317\n",
      "Epoch [961/1000], Loss: 43016.55078125, Train RMSE: 206.8899688720703, Train R^2: 0.7476343907010209\n",
      "Epoch [962/1000], Loss: 42803.45703125, Train RMSE: 206.375732421875, Train R^2: 0.7488873557889519\n",
      "Epoch [963/1000], Loss: 42590.94140625, Train RMSE: 205.86172485351562, Train R^2: 0.7501366843086599\n",
      "Epoch [964/1000], Loss: 42379.046875, Train RMSE: 205.3477020263672, Train R^2: 0.7513828896001904\n",
      "Epoch [965/1000], Loss: 42167.67578125, Train RMSE: 204.83384704589844, Train R^2: 0.7526255967913251\n",
      "Epoch [966/1000], Loss: 41956.90625, Train RMSE: 204.32017517089844, Train R^2: 0.7538647547509585\n",
      "Epoch [967/1000], Loss: 41746.734375, Train RMSE: 203.80648803710938, Train R^2: 0.7551008224293407\n",
      "Epoch [968/1000], Loss: 41537.08984375, Train RMSE: 203.29302978515625, Train R^2: 0.7563332548594164\n",
      "Epoch [969/1000], Loss: 41328.0546875, Train RMSE: 202.77967834472656, Train R^2: 0.7575623010972241\n",
      "Epoch [970/1000], Loss: 41119.59765625, Train RMSE: 202.26646423339844, Train R^2: 0.7587879043906319\n",
      "Epoch [971/1000], Loss: 40911.72265625, Train RMSE: 201.75344848632812, Train R^2: 0.760009951243203\n",
      "Epoch [972/1000], Loss: 40704.453125, Train RMSE: 201.24049377441406, Train R^2: 0.76122875871366\n",
      "Epoch [973/1000], Loss: 40497.734375, Train RMSE: 200.72781372070312, Train R^2: 0.7624437806789539\n",
      "Epoch [974/1000], Loss: 40291.65234375, Train RMSE: 200.21527099609375, Train R^2: 0.7636553932893687\n",
      "Epoch [975/1000], Loss: 40086.15234375, Train RMSE: 199.7028350830078, Train R^2: 0.7648636416126998\n",
      "Epoch [976/1000], Loss: 39881.22265625, Train RMSE: 199.19068908691406, Train R^2: 0.7660681190075838\n",
      "Epoch [977/1000], Loss: 39676.93359375, Train RMSE: 198.67864990234375, Train R^2: 0.7672693267103533\n",
      "Epoch [978/1000], Loss: 39473.1953125, Train RMSE: 198.16671752929688, Train R^2: 0.7684671109747995\n",
      "Epoch [979/1000], Loss: 39270.046875, Train RMSE: 197.65501403808594, Train R^2: 0.7696612869489612\n",
      "Epoch [980/1000], Loss: 39067.49609375, Train RMSE: 197.14341735839844, Train R^2: 0.7708520868064045\n",
      "Epoch [981/1000], Loss: 38865.52734375, Train RMSE: 196.63223266601562, Train R^2: 0.7720389213161131\n",
      "Epoch [982/1000], Loss: 38664.234375, Train RMSE: 196.12100219726562, Train R^2: 0.7732227424125316\n",
      "Epoch [983/1000], Loss: 38463.4453125, Train RMSE: 195.6101837158203, Train R^2: 0.7744025398476426\n",
      "Epoch [984/1000], Loss: 38263.34375, Train RMSE: 195.0994415283203, Train R^2: 0.7755790726064822\n",
      "Epoch [985/1000], Loss: 38063.79296875, Train RMSE: 194.5890350341797, Train R^2: 0.7767517810649301\n",
      "Epoch [986/1000], Loss: 37864.88671875, Train RMSE: 194.07872009277344, Train R^2: 0.7779211711718195\n",
      "Epoch [987/1000], Loss: 37666.5546875, Train RMSE: 193.56874084472656, Train R^2: 0.7790867765182451\n",
      "Epoch [988/1000], Loss: 37468.84765625, Train RMSE: 193.05897521972656, Train R^2: 0.7802487871114941\n",
      "Epoch [989/1000], Loss: 37271.765625, Train RMSE: 192.54940795898438, Train R^2: 0.7814072807161699\n",
      "Epoch [990/1000], Loss: 37075.27734375, Train RMSE: 192.04005432128906, Train R^2: 0.7825622492934168\n",
      "Epoch [991/1000], Loss: 36879.3828125, Train RMSE: 191.53097534179688, Train R^2: 0.7837135225866787\n",
      "Epoch [992/1000], Loss: 36684.11328125, Train RMSE: 191.0220947265625, Train R^2: 0.7848612965677708\n",
      "Epoch [993/1000], Loss: 36489.4375, Train RMSE: 190.51356506347656, Train R^2: 0.7860052503073548\n",
      "Epoch [994/1000], Loss: 36295.421875, Train RMSE: 190.0052490234375, Train R^2: 0.787145668156355\n",
      "Epoch [995/1000], Loss: 36101.99609375, Train RMSE: 189.4970703125, Train R^2: 0.7882826935729472\n",
      "Epoch [996/1000], Loss: 35909.140625, Train RMSE: 188.98928833007812, Train R^2: 0.7894158425766274\n",
      "Epoch [997/1000], Loss: 35716.953125, Train RMSE: 188.48175048828125, Train R^2: 0.7905453573174577\n",
      "Epoch [998/1000], Loss: 35525.37109375, Train RMSE: 187.97457885742188, Train R^2: 0.7916710895704713\n",
      "Epoch [999/1000], Loss: 35334.4453125, Train RMSE: 187.46749877929688, Train R^2: 0.7927935458383373\n",
      "Epoch [1000/1000], Loss: 35144.0625, Train RMSE: 186.96084594726562, Train R^2: 0.7939120278965935\n"
     ]
    }
   ],
   "source": [
    "# 存储最后一次训练的隐藏层特征\n",
    "last_hidden_features = None\n",
    "\n",
    "# 设置超参数\n",
    "\n",
    "learning_rate = 0.0001\n",
    "batch_size = 57\n",
    "num_epochs = 1000\n",
    "\n",
    "# 初始化模型和损失函数\n",
    "model = Net()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# 训练神经网络模型\n",
    "total_step = len(train_dataloader)\n",
    "for epoch in range(num_epochs):\n",
    "    for inputs, targets in train_dataloader:\n",
    "        outputs, intermediate_features = model(inputs)\n",
    "        loss = criterion(outputs, targets.unsqueeze(1))  # 将目标值展开为列向量进行比较\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "        # 计算训练集上的预测精度差\n",
    "        train_outputs,_ = model(data_train_tensor)\n",
    "        train_rmse = mean_squared_error(labels_train_tensor, train_outputs.detach().numpy(), squared=False)\n",
    "        train_r2 = r2_score(labels_train_tensor, train_outputs.detach().numpy())\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item()}, Train RMSE: {train_rmse}, Train R^2: {train_r2}\")\n",
    "        # 提取最后一次训练的隐藏层特征\n",
    "    last_hidden_features = intermediate_features.detach().numpy().astype(np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-24T09:51:06.485738900Z",
     "start_time": "2023-07-24T09:51:06.473272600Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 5.5541353  44.06235     0.5568623  74.26954     0.40456483 43.76542\n",
      "  0.18217534 16.04346     0.5568623  62.292477   85.68736    52.884235  ]\n",
      "[35.7 31.  21.  24.7 17.  21.  28.  22.  31.  79.  58.  22. ]\n",
      "Test RMSE: 26.589151540746386\n",
      "Test R2: -1.3348397137471841\n"
     ]
    }
   ],
   "source": [
    "# 直接通过神经网路进行预测\n",
    "model_path = \"model_full_spectrum.pth\"\n",
    "# 将模型的参数保存到文件中\n",
    "torch.save(model.state_dict(), model_path)\n",
    "model1 = Net()\n",
    "model1.load_state_dict(torch.load(model_path))\n",
    "\n",
    "test_inputs = data_test_tensor\n",
    "test_outputs,_ = model1(test_inputs)\n",
    "test_outputs = test_outputs.detach().numpy()\n",
    "test_outputs = np.squeeze(test_outputs)\n",
    "labels_test = np.squeeze(labels_test)\n",
    "\n",
    "print(test_outputs)\n",
    "print(labels_test)\n",
    "test_rmse = mean_squared_error(labels_test, test_outputs, squared=False)\n",
    "test_r2 = r2_score(labels_test, test_outputs)\n",
    "print(\"Test RMSE:\", test_rmse)\n",
    "print(\"Test R2:\",test_r2)\n",
    "\n",
    "if (test_r2>=0.88):\n",
    "    good_model_full_spectrum_path = \"good_model_full_spectrum.pth\"+str(test_r2)\n",
    "    torch.save(model.state_dict(),good_model_full_spectrum_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-24T09:50:42.960286100Z",
     "start_time": "2023-07-24T09:50:42.925430600Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(56, 4)\n",
      "Epoch [1/500], Loss: 192589.359375, Train RMSE: 438.65338134765625, Train R^2: -0.13447333777314663\n",
      "Epoch [2/500], Loss: 192416.796875, Train RMSE: 438.45751953125, Train R^2: -0.13346041940218467\n",
      "Epoch [3/500], Loss: 192245.0, Train RMSE: 438.2488098144531, Train R^2: -0.13238164672152397\n",
      "Epoch [4/500], Loss: 192062.015625, Train RMSE: 438.0294189453125, Train R^2: -0.13124793569067061\n",
      "Epoch [5/500], Loss: 191869.75, Train RMSE: 437.8118896484375, Train R^2: -0.13012478575520592\n",
      "Epoch [6/500], Loss: 191679.25, Train RMSE: 437.59649658203125, Train R^2: -0.12901306390038703\n",
      "Epoch [7/500], Loss: 191490.6875, Train RMSE: 437.3786926269531, Train R^2: -0.12788954484856507\n",
      "Epoch [8/500], Loss: 191300.140625, Train RMSE: 437.15740966796875, Train R^2: -0.12674847643689469\n",
      "Epoch [9/500], Loss: 191106.609375, Train RMSE: 436.931640625, Train R^2: -0.12558510586720129\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/500], Loss: 190909.265625, Train RMSE: 436.7046813964844, Train R^2: -0.12441606433770946\n",
      "Epoch [11/500], Loss: 190711.0, Train RMSE: 436.4761047363281, Train R^2: -0.12323923312175444\n",
      "Epoch [12/500], Loss: 190511.390625, Train RMSE: 436.2442626953125, Train R^2: -0.12204616007346569\n",
      "Epoch [13/500], Loss: 190309.03125, Train RMSE: 436.0046081542969, Train R^2: -0.1208138181079137\n",
      "Epoch [14/500], Loss: 190100.015625, Train RMSE: 435.75860595703125, Train R^2: -0.11954939843617685\n",
      "Epoch [15/500], Loss: 189885.5625, Train RMSE: 435.50537109375, Train R^2: -0.11824840308266471\n",
      "Epoch [16/500], Loss: 189664.90625, Train RMSE: 435.2492370605469, Train R^2: -0.11693346676955585\n",
      "Epoch [17/500], Loss: 189441.890625, Train RMSE: 434.99346923828125, Train R^2: -0.1156212803759269\n",
      "Epoch [18/500], Loss: 189219.328125, Train RMSE: 434.74530029296875, Train R^2: -0.11434857737992754\n",
      "Epoch [19/500], Loss: 189003.453125, Train RMSE: 434.4920654296875, Train R^2: -0.1130507055792922\n",
      "Epoch [20/500], Loss: 188783.34375, Train RMSE: 434.2333068847656, Train R^2: -0.11172548090779544\n",
      "Epoch [21/500], Loss: 188558.5625, Train RMSE: 433.9685974121094, Train R^2: -0.11037044753057956\n",
      "Epoch [22/500], Loss: 188328.734375, Train RMSE: 433.6981506347656, Train R^2: -0.10898700882282397\n",
      "Epoch [23/500], Loss: 188094.09375, Train RMSE: 433.4216613769531, Train R^2: -0.10757347516115079\n",
      "Epoch [24/500], Loss: 187854.359375, Train RMSE: 433.1392822265625, Train R^2: -0.10613076486728978\n",
      "Epoch [25/500], Loss: 187609.640625, Train RMSE: 432.85089111328125, Train R^2: -0.10465834231655258\n",
      "Epoch [26/500], Loss: 187359.890625, Train RMSE: 432.5561828613281, Train R^2: -0.1031544686600494\n",
      "Epoch [27/500], Loss: 187104.859375, Train RMSE: 432.257568359375, Train R^2: -0.10163185881122283\n",
      "Epoch [28/500], Loss: 186846.59375, Train RMSE: 431.9526672363281, Train R^2: -0.10007837337114878\n",
      "Epoch [29/500], Loss: 186583.109375, Train RMSE: 431.64117431640625, Train R^2: -0.09849229075053256\n",
      "Epoch [30/500], Loss: 186314.09375, Train RMSE: 431.32489013671875, Train R^2: -0.09688323405423382\n",
      "Epoch [31/500], Loss: 186041.1875, Train RMSE: 431.0090026855469, Train R^2: -0.09527715814213922\n",
      "Epoch [32/500], Loss: 185768.765625, Train RMSE: 430.6859130859375, Train R^2: -0.09363558488633528\n",
      "Epoch [33/500], Loss: 185490.359375, Train RMSE: 430.35595703125, Train R^2: -0.09196061029790692\n",
      "Epoch [34/500], Loss: 185206.25, Train RMSE: 430.0184326171875, Train R^2: -0.09024835841475887\n",
      "Epoch [35/500], Loss: 184915.84375, Train RMSE: 429.6734313964844, Train R^2: -0.08849972300425923\n",
      "Epoch [36/500], Loss: 184619.265625, Train RMSE: 429.32086181640625, Train R^2: -0.0867140949246974\n",
      "Epoch [37/500], Loss: 184316.390625, Train RMSE: 428.9606018066406, Train R^2: -0.08489105538425346\n",
      "Epoch [38/500], Loss: 184007.203125, Train RMSE: 428.5925598144531, Train R^2: -0.08303027793825724\n",
      "Epoch [39/500], Loss: 183691.609375, Train RMSE: 428.2166442871094, Train R^2: -0.08113113397748162\n",
      "Epoch [40/500], Loss: 183369.484375, Train RMSE: 427.8327331542969, Train R^2: -0.07919351148607334\n",
      "Epoch [41/500], Loss: 183040.84375, Train RMSE: 427.4408264160156, Train R^2: -0.07721727251617305\n",
      "Epoch [42/500], Loss: 182705.65625, Train RMSE: 427.0408020019531, Train R^2: -0.07520199304483133\n",
      "Epoch [43/500], Loss: 182363.84375, Train RMSE: 426.6325378417969, Train R^2: -0.0731471206924117\n",
      "Epoch [44/500], Loss: 182015.328125, Train RMSE: 426.216064453125, Train R^2: -0.07105294277616725\n",
      "Epoch [45/500], Loss: 181660.140625, Train RMSE: 425.791259765625, Train R^2: -0.0689190311095993\n",
      "Epoch [46/500], Loss: 181298.1875, Train RMSE: 425.35809326171875, Train R^2: -0.06674531285915863\n",
      "Epoch [47/500], Loss: 180929.515625, Train RMSE: 424.91650390625, Train R^2: -0.06453156300450624\n",
      "Epoch [48/500], Loss: 180554.0625, Train RMSE: 424.4664001464844, Train R^2: -0.062277434213868776\n",
      "Epoch [49/500], Loss: 180171.734375, Train RMSE: 424.00689697265625, Train R^2: -0.05997866426516851\n",
      "Epoch [50/500], Loss: 179781.8125, Train RMSE: 423.5376892089844, Train R^2: -0.05763406122914638\n",
      "Epoch [51/500], Loss: 179384.171875, Train RMSE: 423.05914306640625, Train R^2: -0.05524537150991904\n",
      "Epoch [52/500], Loss: 178979.03125, Train RMSE: 422.5697937011719, Train R^2: -0.052805560638422344\n",
      "Epoch [53/500], Loss: 178565.203125, Train RMSE: 422.0714416503906, Train R^2: -0.050323828484434285\n",
      "Epoch [54/500], Loss: 178144.296875, Train RMSE: 421.563232421875, Train R^2: -0.04779603701634483\n",
      "Epoch [55/500], Loss: 177715.5625, Train RMSE: 421.0450744628906, Train R^2: -0.04522180293470712\n",
      "Epoch [56/500], Loss: 177278.9375, Train RMSE: 420.5169372558594, Train R^2: -0.04260128431902577\n",
      "Epoch [57/500], Loss: 176834.453125, Train RMSE: 419.9787292480469, Train R^2: -0.03993435804821743\n",
      "Epoch [58/500], Loss: 176382.15625, Train RMSE: 419.4304504394531, Train R^2: -0.03722074129033359\n",
      "Epoch [59/500], Loss: 175921.890625, Train RMSE: 418.8718566894531, Train R^2: -0.03445997014515001\n",
      "Epoch [60/500], Loss: 175453.640625, Train RMSE: 418.3029479980469, Train R^2: -0.03165190776273907\n",
      "Epoch [61/500], Loss: 174977.375, Train RMSE: 417.7235107421875, Train R^2: -0.028795676403039483\n",
      "Epoch [62/500], Loss: 174492.921875, Train RMSE: 417.1333312988281, Train R^2: -0.025890695495391913\n",
      "Epoch [63/500], Loss: 174000.21875, Train RMSE: 416.53253173828125, Train R^2: -0.02293763927906367\n",
      "Epoch [64/500], Loss: 173499.359375, Train RMSE: 415.9209899902344, Train R^2: -0.019936095557428368\n",
      "Epoch [65/500], Loss: 172990.265625, Train RMSE: 415.29827880859375, Train R^2: -0.016884296830618206\n",
      "Epoch [66/500], Loss: 172472.640625, Train RMSE: 414.6642150878906, Train R^2: -0.013781641211353612\n",
      "Epoch [67/500], Loss: 171946.390625, Train RMSE: 414.01812744140625, Train R^2: -0.010624949025230812\n",
      "Epoch [68/500], Loss: 171411.015625, Train RMSE: 413.3599853515625, Train R^2: -0.0074144078996505325\n",
      "Epoch [69/500], Loss: 170866.453125, Train RMSE: 412.68963623046875, Train R^2: -0.004149555501936852\n",
      "Epoch [70/500], Loss: 170312.734375, Train RMSE: 412.00677490234375, Train R^2: -0.0008293603202449429\n",
      "Epoch [71/500], Loss: 169749.609375, Train RMSE: 411.3113098144531, Train R^2: 0.0025465625974187\n",
      "Epoch [72/500], Loss: 169177.015625, Train RMSE: 410.6032409667969, Train R^2: 0.005977914168768916\n",
      "Epoch [73/500], Loss: 168595.0, Train RMSE: 409.88214111328125, Train R^2: 0.009466336819436982\n",
      "Epoch [74/500], Loss: 168003.34375, Train RMSE: 409.1476745605469, Train R^2: 0.013012946252063062\n",
      "Epoch [75/500], Loss: 167401.796875, Train RMSE: 408.3997802734375, Train R^2: 0.016617912597065465\n",
      "Epoch [76/500], Loss: 166790.375, Train RMSE: 407.6379699707031, Train R^2: 0.020283233647714116\n",
      "Epoch [77/500], Loss: 166168.703125, Train RMSE: 406.862060546875, Train R^2: 0.024009339044556932\n",
      "Epoch [78/500], Loss: 165536.734375, Train RMSE: 406.0718688964844, Train R^2: 0.02779671608485712\n",
      "Epoch [79/500], Loss: 164894.359375, Train RMSE: 405.26715087890625, Train R^2: 0.031646075391131956\n",
      "Epoch [80/500], Loss: 164241.46875, Train RMSE: 404.44775390625, Train R^2: 0.0355578958069277\n",
      "Epoch [81/500], Loss: 163577.984375, Train RMSE: 403.61334228515625, Train R^2: 0.0395332895462287\n",
      "Epoch [82/500], Loss: 162903.71875, Train RMSE: 402.7635498046875, Train R^2: 0.04357339588458364\n",
      "Epoch [83/500], Loss: 162218.484375, Train RMSE: 401.89837646484375, Train R^2: 0.04767811847235626\n",
      "Epoch [84/500], Loss: 161522.28125, Train RMSE: 401.0177307128906, Train R^2: 0.05184693652750716\n",
      "Epoch [85/500], Loss: 160815.234375, Train RMSE: 400.1216735839844, Train R^2: 0.05607939247250837\n",
      "Epoch [86/500], Loss: 160097.359375, Train RMSE: 399.20916748046875, Train R^2: 0.06037980564776013\n",
      "Epoch [87/500], Loss: 159367.953125, Train RMSE: 398.2789306640625, Train R^2: 0.06475384809969997\n",
      "Epoch [88/500], Loss: 158626.09375, Train RMSE: 397.3310852050781, Train R^2: 0.06920002481549781\n",
      "Epoch [89/500], Loss: 157871.984375, Train RMSE: 396.36663818359375, Train R^2: 0.07371303755699277\n",
      "Epoch [90/500], Loss: 157106.53125, Train RMSE: 395.3854064941406, Train R^2: 0.07829371355066794\n",
      "Epoch [91/500], Loss: 156329.609375, Train RMSE: 394.3861083984375, Train R^2: 0.08294684849423972\n",
      "Epoch [92/500], Loss: 155540.40625, Train RMSE: 393.3683166503906, Train R^2: 0.08767388153585787\n",
      "Epoch [93/500], Loss: 154738.640625, Train RMSE: 392.33245849609375, Train R^2: 0.09247241220177282\n",
      "Epoch [94/500], Loss: 153924.78125, Train RMSE: 391.2781982421875, Train R^2: 0.09734330102775313\n",
      "Epoch [95/500], Loss: 153098.625, Train RMSE: 390.2052307128906, Train R^2: 0.10228703963008323\n",
      "Epoch [96/500], Loss: 152260.125, Train RMSE: 389.1130676269531, Train R^2: 0.10730541667848659\n",
      "Epoch [97/500], Loss: 151408.953125, Train RMSE: 388.00140380859375, Train R^2: 0.1123986996157611\n",
      "Epoch [98/500], Loss: 150545.09375, Train RMSE: 386.8697814941406, Train R^2: 0.11756858478930343\n",
      "Epoch [99/500], Loss: 149668.234375, Train RMSE: 385.7184753417969, Train R^2: 0.12281310932217415\n",
      "Epoch [100/500], Loss: 148778.71875, Train RMSE: 384.5463562011719, Train R^2: 0.12813603662327333\n",
      "Epoch [101/500], Loss: 147875.90625, Train RMSE: 383.3535461425781, Train R^2: 0.13353647364723376\n",
      "Epoch [102/500], Loss: 146959.9375, Train RMSE: 382.1396484375, Train R^2: 0.1390151390778912\n",
      "Epoch [103/500], Loss: 146030.703125, Train RMSE: 380.90509033203125, Train R^2: 0.1445691843117728\n",
      "Epoch [104/500], Loss: 145088.703125, Train RMSE: 379.6493835449219, Train R^2: 0.15020002065966487\n",
      "Epoch [105/500], Loss: 144133.65625, Train RMSE: 378.3725280761719, Train R^2: 0.15590666413042753\n",
      "Epoch [106/500], Loss: 143165.765625, Train RMSE: 377.07440185546875, Train R^2: 0.16168863193997374\n",
      "Epoch [107/500], Loss: 142185.09375, Train RMSE: 375.7537536621094, Train R^2: 0.16755038086943075\n",
      "Epoch [108/500], Loss: 141190.890625, Train RMSE: 374.410400390625, Train R^2: 0.17349193853018163\n",
      "Epoch [109/500], Loss: 140183.140625, Train RMSE: 373.04339599609375, Train R^2: 0.17951615519350783\n",
      "Epoch [110/500], Loss: 139161.375, Train RMSE: 371.6512451171875, Train R^2: 0.1856286630237437\n",
      "Epoch [111/500], Loss: 138124.640625, Train RMSE: 370.23284912109375, Train R^2: 0.19183289096943712\n",
      "Epoch [112/500], Loss: 137072.359375, Train RMSE: 368.7901611328125, Train R^2: 0.19811883325376833\n",
      "Epoch [113/500], Loss: 136006.1875, Train RMSE: 367.3223571777344, Train R^2: 0.20448926187529815\n",
      "Epoch [114/500], Loss: 134925.71875, Train RMSE: 365.8307189941406, Train R^2: 0.21093698602115818\n",
      "Epoch [115/500], Loss: 133832.140625, Train RMSE: 364.315185546875, Train R^2: 0.21746121733289325\n",
      "Epoch [116/500], Loss: 132725.5625, Train RMSE: 362.77264404296875, Train R^2: 0.22407389842851644\n",
      "Epoch [117/500], Loss: 131604.0, Train RMSE: 361.2074890136719, Train R^2: 0.23075477064770133\n",
      "Epoch [118/500], Loss: 130470.8515625, Train RMSE: 359.61688232421875, Train R^2: 0.23751474720350474\n",
      "Epoch [119/500], Loss: 129324.3046875, Train RMSE: 357.99383544921875, Train R^2: 0.2443817830423627\n",
      "Epoch [120/500], Loss: 128159.6015625, Train RMSE: 356.3440856933594, Train R^2: 0.25132999935908507\n",
      "Epoch [121/500], Loss: 126981.1171875, Train RMSE: 354.6690368652344, Train R^2: 0.2583519661126189\n",
      "Epoch [122/500], Loss: 125790.1328125, Train RMSE: 352.9702453613281, Train R^2: 0.26543965750743337\n",
      "Epoch [123/500], Loss: 124587.9921875, Train RMSE: 351.24835205078125, Train R^2: 0.27258896706381297\n",
      "Epoch [124/500], Loss: 123375.40625, Train RMSE: 349.5023498535156, Train R^2: 0.27980271337018503\n",
      "Epoch [125/500], Loss: 122151.8828125, Train RMSE: 347.7299499511719, Train R^2: 0.28708862462394114\n",
      "Epoch [126/500], Loss: 120916.125, Train RMSE: 345.9334411621094, Train R^2: 0.2944359703262627\n",
      "Epoch [127/500], Loss: 119669.953125, Train RMSE: 344.11456298828125, Train R^2: 0.3018361502527206\n",
      "Epoch [128/500], Loss: 118414.828125, Train RMSE: 342.2695007324219, Train R^2: 0.30930281326638254\n",
      "Epoch [129/500], Loss: 117148.3984375, Train RMSE: 340.40087890625, Train R^2: 0.31682388301766473\n",
      "Epoch [130/500], Loss: 115872.7578125, Train RMSE: 338.51031494140625, Train R^2: 0.32439155948194376\n",
      "Epoch [131/500], Loss: 114589.21875, Train RMSE: 336.5988464355469, Train R^2: 0.331999911780934\n",
      "Epoch [132/500], Loss: 113298.7578125, Train RMSE: 334.663330078125, Train R^2: 0.33965999095805455\n",
      "Epoch [133/500], Loss: 111999.5546875, Train RMSE: 332.70306396484375, Train R^2: 0.3473732992624552\n",
      "Epoch [134/500], Loss: 110691.3046875, Train RMSE: 330.718994140625, Train R^2: 0.3551338212452103\n",
      "Epoch [135/500], Loss: 109375.0703125, Train RMSE: 328.7131652832031, Train R^2: 0.3629323063640235\n",
      "Epoch [136/500], Loss: 108052.375, Train RMSE: 326.6794128417969, Train R^2: 0.3707911096762766\n",
      "Epoch [137/500], Loss: 106719.421875, Train RMSE: 324.6199645996094, Train R^2: 0.37869947190285547\n",
      "Epoch [138/500], Loss: 105378.1015625, Train RMSE: 322.5360107421875, Train R^2: 0.38665104631361635\n",
      "Epoch [139/500], Loss: 104029.453125, Train RMSE: 320.4278564453125, Train R^2: 0.3946426313385847\n",
      "Epoch [140/500], Loss: 102674.0078125, Train RMSE: 318.296875, Train R^2: 0.40266765759904455\n",
      "Epoch [141/500], Loss: 101312.8828125, Train RMSE: 316.1446533203125, Train R^2: 0.4107182275053033\n",
      "Epoch [142/500], Loss: 99947.4453125, Train RMSE: 313.9674377441406, Train R^2: 0.41880673877877916\n",
      "Epoch [143/500], Loss: 98575.546875, Train RMSE: 311.7656555175781, Train R^2: 0.42692976604861477\n",
      "Epoch [144/500], Loss: 97197.8203125, Train RMSE: 309.5403137207031, Train R^2: 0.43508159290619075\n",
      "Epoch [145/500], Loss: 95815.1953125, Train RMSE: 307.2970275878906, Train R^2: 0.44323992898992026\n",
      "Epoch [146/500], Loss: 94431.46875, Train RMSE: 305.0218811035156, Train R^2: 0.45145371528044975\n",
      "Epoch [147/500], Loss: 93038.34375, Train RMSE: 302.72882080078125, Train R^2: 0.4596702588296221\n",
      "Epoch [148/500], Loss: 91644.7421875, Train RMSE: 300.41290283203125, Train R^2: 0.46790592325258296\n",
      "Epoch [149/500], Loss: 90247.8984375, Train RMSE: 298.0749206542969, Train R^2: 0.47615578812344816\n",
      "Epoch [150/500], Loss: 88848.6484375, Train RMSE: 295.7167053222656, Train R^2: 0.4844118109122991\n",
      "Epoch [151/500], Loss: 87448.359375, Train RMSE: 293.3379821777344, Train R^2: 0.4926731390114092\n",
      "Epoch [152/500], Loss: 86047.15625, Train RMSE: 290.9381408691406, Train R^2: 0.5009401512268372\n",
      "Epoch [153/500], Loss: 84644.9921875, Train RMSE: 288.5173645019531, Train R^2: 0.5092105326513057\n",
      "Epoch [154/500], Loss: 83242.2734375, Train RMSE: 286.0772399902344, Train R^2: 0.5174770753329168\n",
      "Epoch [155/500], Loss: 81840.1953125, Train RMSE: 283.61688232421875, Train R^2: 0.5257410921673622\n",
      "Epoch [156/500], Loss: 80438.5390625, Train RMSE: 281.14166259765625, Train R^2: 0.5339830286144913\n",
      "Epoch [157/500], Loss: 79040.6328125, Train RMSE: 278.6494445800781, Train R^2: 0.5422085912174486\n",
      "Epoch [158/500], Loss: 77645.5, Train RMSE: 276.1402282714844, Train R^2: 0.5504162317062293\n",
      "Epoch [159/500], Loss: 76253.4140625, Train RMSE: 273.6116027832031, Train R^2: 0.5586122011604057\n",
      "Epoch [160/500], Loss: 74863.3046875, Train RMSE: 271.0625305175781, Train R^2: 0.566798200728204\n",
      "Epoch [161/500], Loss: 73474.8828125, Train RMSE: 268.4895324707031, Train R^2: 0.5749832225771238\n",
      "Epoch [162/500], Loss: 72086.640625, Train RMSE: 265.8982849121094, Train R^2: 0.583147545885069\n",
      "Epoch [163/500], Loss: 70701.890625, Train RMSE: 263.29022216796875, Train R^2: 0.5912847683841305\n",
      "Epoch [164/500], Loss: 69321.7421875, Train RMSE: 260.6649169921875, Train R^2: 0.5993949208724234\n",
      "Epoch [165/500], Loss: 67946.1953125, Train RMSE: 258.02288818359375, Train R^2: 0.6074746304082163\n",
      "Epoch [166/500], Loss: 66575.8046875, Train RMSE: 255.36489868164062, Train R^2: 0.6155200334605262\n",
      "Epoch [167/500], Loss: 65211.2265625, Train RMSE: 252.69161987304688, Train R^2: 0.6235277013286473\n",
      "Epoch [168/500], Loss: 63853.05859375, Train RMSE: 250.00372314453125, Train R^2: 0.6314943055042562\n",
      "Epoch [169/500], Loss: 62501.8515625, Train RMSE: 247.3022918701172, Train R^2: 0.6394150826731897\n",
      "Epoch [170/500], Loss: 61158.421875, Train RMSE: 244.58920288085938, Train R^2: 0.6472834034759039\n",
      "Epoch [171/500], Loss: 59823.875, Train RMSE: 241.86338806152344, Train R^2: 0.6551012865558938\n",
      "Epoch [172/500], Loss: 58497.8984375, Train RMSE: 239.1241455078125, Train R^2: 0.6628694196151284\n",
      "Epoch [173/500], Loss: 57180.3515625, Train RMSE: 236.37136840820312, Train R^2: 0.6705867729014946\n",
      "Epoch [174/500], Loss: 55871.42578125, Train RMSE: 233.608642578125, Train R^2: 0.6782421756937917\n",
      "Epoch [175/500], Loss: 54572.99609375, Train RMSE: 230.83912658691406, Train R^2: 0.6858260543779005\n",
      "Epoch [176/500], Loss: 53286.703125, Train RMSE: 228.06219482421875, Train R^2: 0.6933394621610816\n",
      "Epoch [177/500], Loss: 52012.36328125, Train RMSE: 225.2785186767578, Train R^2: 0.7007798481095638\n",
      "Epoch [178/500], Loss: 50750.41015625, Train RMSE: 222.49169921875, Train R^2: 0.7081370755477617\n",
      "Epoch [179/500], Loss: 49502.55859375, Train RMSE: 219.70399475097656, Train R^2: 0.7154050274784705\n",
      "Epoch [180/500], Loss: 48269.84765625, Train RMSE: 216.92013549804688, Train R^2: 0.7225714855784716\n",
      "Epoch [181/500], Loss: 47054.3515625, Train RMSE: 214.13568115234375, Train R^2: 0.7296481210351812\n",
      "Epoch [182/500], Loss: 45854.08984375, Train RMSE: 211.35108947753906, Train R^2: 0.7366336355970522\n",
      "Epoch [183/500], Loss: 44669.28515625, Train RMSE: 208.56683349609375, Train R^2: 0.7435269391984128\n",
      "Epoch [184/500], Loss: 43500.12109375, Train RMSE: 205.7843780517578, Train R^2: 0.7503243984948121\n",
      "Epoch [185/500], Loss: 42347.21484375, Train RMSE: 203.0052490234375, Train R^2: 0.757022611259176\n",
      "Epoch [186/500], Loss: 41211.1328125, Train RMSE: 200.2333526611328, Train R^2: 0.7636127186424607\n",
      "Epoch [187/500], Loss: 40093.390625, Train RMSE: 197.46554565429688, Train R^2: 0.7701026191265421\n",
      "Epoch [188/500], Loss: 38992.64453125, Train RMSE: 194.70436096191406, Train R^2: 0.7764870502459537\n",
      "Epoch [189/500], Loss: 37909.79296875, Train RMSE: 191.94749450683594, Train R^2: 0.782771815469699\n",
      "Epoch [190/500], Loss: 36843.83984375, Train RMSE: 189.2030029296875, Train R^2: 0.7889393417851336\n",
      "Epoch [191/500], Loss: 35797.7734375, Train RMSE: 186.46780395507812, Train R^2: 0.7949975510166792\n",
      "Epoch [192/500], Loss: 34770.24609375, Train RMSE: 183.74465942382812, Train R^2: 0.8009414878255994\n",
      "Epoch [193/500], Loss: 33762.1015625, Train RMSE: 181.0360107421875, Train R^2: 0.8067670465581874\n",
      "Epoch [194/500], Loss: 32774.03515625, Train RMSE: 178.34242248535156, Train R^2: 0.8124743605957359\n",
      "Epoch [195/500], Loss: 31806.01953125, Train RMSE: 175.66732788085938, Train R^2: 0.8180578696284886\n",
      "Epoch [196/500], Loss: 30859.0078125, Train RMSE: 173.012451171875, Train R^2: 0.8235157328103735\n",
      "Epoch [197/500], Loss: 29933.306640625, Train RMSE: 170.38870239257812, Train R^2: 0.8288279215805113\n",
      "Epoch [198/500], Loss: 29032.3125, Train RMSE: 167.79107666015625, Train R^2: 0.834007277715834\n",
      "Epoch [199/500], Loss: 28153.849609375, Train RMSE: 165.21861267089844, Train R^2: 0.8390580438205172\n",
      "Epoch [200/500], Loss: 27297.193359375, Train RMSE: 162.6726837158203, Train R^2: 0.8439798823996543\n",
      "Epoch [201/500], Loss: 26462.404296875, Train RMSE: 160.158203125, Train R^2: 0.8487659305292371\n",
      "Epoch [202/500], Loss: 25650.6484375, Train RMSE: 157.67330932617188, Train R^2: 0.8534223840970807\n",
      "Epoch [203/500], Loss: 24860.873046875, Train RMSE: 155.2196502685547, Train R^2: 0.8579488732945226\n",
      "Epoch [204/500], Loss: 24093.142578125, Train RMSE: 152.79989624023438, Train R^2: 0.8623432825083724\n",
      "Epoch [205/500], Loss: 23347.806640625, Train RMSE: 150.41502380371094, Train R^2: 0.8666067672406632\n",
      "Epoch [206/500], Loss: 22624.68359375, Train RMSE: 148.06484985351562, Train R^2: 0.8707426526409476\n",
      "Epoch [207/500], Loss: 21923.19921875, Train RMSE: 145.7530059814453, Train R^2: 0.8747475058369838\n",
      "Epoch [208/500], Loss: 21243.939453125, Train RMSE: 143.4823760986328, Train R^2: 0.8786196382682674\n",
      "Epoch [209/500], Loss: 20587.193359375, Train RMSE: 141.25323486328125, Train R^2: 0.8823618741286302\n",
      "Epoch [210/500], Loss: 19952.4765625, Train RMSE: 139.06663513183594, Train R^2: 0.8859757552190037\n",
      "Epoch [211/500], Loss: 19339.529296875, Train RMSE: 136.9304962158203, Train R^2: 0.8894518092179322\n",
      "Epoch [212/500], Loss: 18749.9609375, Train RMSE: 134.8496856689453, Train R^2: 0.8927860580542313\n",
      "Epoch [213/500], Loss: 18184.439453125, Train RMSE: 132.8015899658203, Train R^2: 0.8960180658998812\n",
      "Epoch [214/500], Loss: 17636.263671875, Train RMSE: 130.8011474609375, Train R^2: 0.8991271355471131\n",
      "Epoch [215/500], Loss: 17108.9375, Train RMSE: 128.84605407714844, Train R^2: 0.9021201071708936\n",
      "Epoch [216/500], Loss: 16601.3046875, Train RMSE: 126.93614196777344, Train R^2: 0.9050003689691094\n",
      "Epoch [217/500], Loss: 16112.78515625, Train RMSE: 125.07135009765625, Train R^2: 0.9077711079847356\n",
      "Epoch [218/500], Loss: 15642.8427734375, Train RMSE: 123.2515869140625, Train R^2: 0.9104354131724302\n",
      "Epoch [219/500], Loss: 15190.953125, Train RMSE: 121.47813415527344, Train R^2: 0.91299435138863\n",
      "Epoch [220/500], Loss: 14756.9345703125, Train RMSE: 119.75174713134766, Train R^2: 0.9154497124253911\n",
      "Epoch [221/500], Loss: 14340.484375, Train RMSE: 118.07710266113281, Train R^2: 0.9177979389442357\n",
      "Epoch [222/500], Loss: 13942.203125, Train RMSE: 116.4521255493164, Train R^2: 0.9200449076039879\n",
      "Epoch [223/500], Loss: 13561.0966796875, Train RMSE: 114.86619567871094, Train R^2: 0.9222078485703138\n",
      "Epoch [224/500], Loss: 13194.2431640625, Train RMSE: 113.32093811035156, Train R^2: 0.9242867828733133\n",
      "Epoch [225/500], Loss: 12841.63671875, Train RMSE: 111.8203353881836, Train R^2: 0.9262787264861105\n",
      "Epoch [226/500], Loss: 12503.78515625, Train RMSE: 110.3706283569336, Train R^2: 0.9281778496383165\n",
      "Epoch [227/500], Loss: 12181.67578125, Train RMSE: 108.95702362060547, Train R^2: 0.9300058388190656\n",
      "Epoch [228/500], Loss: 11871.6337890625, Train RMSE: 107.58633422851562, Train R^2: 0.931755829063139\n",
      "Epoch [229/500], Loss: 11574.8203125, Train RMSE: 106.25983428955078, Train R^2: 0.9334283166726691\n",
      "Epoch [230/500], Loss: 11291.1513671875, Train RMSE: 104.97570037841797, Train R^2: 0.9350275989442672\n",
      "Epoch [231/500], Loss: 11019.8984375, Train RMSE: 103.73905944824219, Train R^2: 0.9365493747982427\n",
      "Epoch [232/500], Loss: 10761.79296875, Train RMSE: 102.54277038574219, Train R^2: 0.938004327273855\n",
      "Epoch [233/500], Loss: 10515.01953125, Train RMSE: 101.38378143310547, Train R^2: 0.9393978121010423\n",
      "Epoch [234/500], Loss: 10278.671875, Train RMSE: 100.28265380859375, Train R^2: 0.9407070684462175\n",
      "Epoch [235/500], Loss: 10056.609375, Train RMSE: 99.2304458618164, Train R^2: 0.9419447883500432\n",
      "Epoch [236/500], Loss: 9846.681640625, Train RMSE: 98.2220687866211, Train R^2: 0.9431187168658989\n",
      "Epoch [237/500], Loss: 9647.5732421875, Train RMSE: 97.27098846435547, Train R^2: 0.9442149276432988\n",
      "Epoch [238/500], Loss: 9461.646484375, Train RMSE: 96.36790466308594, Train R^2: 0.945245959925158\n",
      "Epoch [239/500], Loss: 9286.7744140625, Train RMSE: 95.51412963867188, Train R^2: 0.9462118579847659\n",
      "Epoch [240/500], Loss: 9122.94921875, Train RMSE: 94.69874572753906, Train R^2: 0.9471262960584778\n",
      "Epoch [241/500], Loss: 8967.8525390625, Train RMSE: 93.91778564453125, Train R^2: 0.9479947669954375\n",
      "Epoch [242/500], Loss: 8820.55078125, Train RMSE: 93.16951751708984, Train R^2: 0.9488201570074278\n",
      "Epoch [243/500], Loss: 8680.55859375, Train RMSE: 92.45692443847656, Train R^2: 0.9496000340714196\n",
      "Epoch [244/500], Loss: 8548.283203125, Train RMSE: 91.77226257324219, Train R^2: 0.9503437185202516\n",
      "Epoch [245/500], Loss: 8422.1474609375, Train RMSE: 91.1218490600586, Train R^2: 0.9510450806560945\n",
      "Epoch [246/500], Loss: 8303.1904296875, Train RMSE: 90.49939727783203, Train R^2: 0.9517116132320538\n",
      "Epoch [247/500], Loss: 8190.14111328125, Train RMSE: 89.89995574951172, Train R^2: 0.9523491980311217\n",
      "Epoch [248/500], Loss: 8082.00146484375, Train RMSE: 89.32400512695312, Train R^2: 0.9529577923631686\n",
      "Epoch [249/500], Loss: 7978.7783203125, Train RMSE: 88.77316284179688, Train R^2: 0.9535362011243682\n",
      "Epoch [250/500], Loss: 7880.67529296875, Train RMSE: 88.24783325195312, Train R^2: 0.9540844917253986\n",
      "Epoch [251/500], Loss: 7787.6796875, Train RMSE: 87.74473571777344, Train R^2: 0.9546065202588297\n",
      "Epoch [252/500], Loss: 7699.13916015625, Train RMSE: 87.26597595214844, Train R^2: 0.9551005287622357\n",
      "Epoch [253/500], Loss: 7615.35107421875, Train RMSE: 86.8060073852539, Train R^2: 0.9555725999010054\n",
      "Epoch [254/500], Loss: 7535.2841796875, Train RMSE: 86.36177062988281, Train R^2: 0.9560261643084733\n",
      "Epoch [255/500], Loss: 7458.35498046875, Train RMSE: 85.93486022949219, Train R^2: 0.9564598331840715\n",
      "Epoch [256/500], Loss: 7384.80126953125, Train RMSE: 85.52701568603516, Train R^2: 0.9568721343444573\n",
      "Epoch [257/500], Loss: 7314.87060546875, Train RMSE: 85.13604736328125, Train R^2: 0.9572655359307174\n",
      "Epoch [258/500], Loss: 7248.146484375, Train RMSE: 84.7613754272461, Train R^2: 0.9576408425815705\n",
      "Epoch [259/500], Loss: 7184.49169921875, Train RMSE: 84.40093231201172, Train R^2: 0.9580003390684966\n",
      "Epoch [260/500], Loss: 7123.517578125, Train RMSE: 84.05390167236328, Train R^2: 0.9583450085635081\n",
      "Epoch [261/500], Loss: 7065.0576171875, Train RMSE: 83.7196044921875, Train R^2: 0.9586756852411052\n",
      "Epoch [262/500], Loss: 7008.97216796875, Train RMSE: 83.39849853515625, Train R^2: 0.9589920787363805\n",
      "Epoch [263/500], Loss: 6955.30908203125, Train RMSE: 83.08706665039062, Train R^2: 0.9592977753692932\n",
      "Epoch [264/500], Loss: 6903.46044921875, Train RMSE: 82.78785705566406, Train R^2: 0.9595903946233427\n",
      "Epoch [265/500], Loss: 6853.82958984375, Train RMSE: 82.5019760131836, Train R^2: 0.9598689932734574\n",
      "Epoch [266/500], Loss: 6806.57666015625, Train RMSE: 82.22404479980469, Train R^2: 0.9601389320092544\n",
      "Epoch [267/500], Loss: 6760.79248046875, Train RMSE: 81.95179748535156, Train R^2: 0.9604024573900295\n",
      "Epoch [268/500], Loss: 6716.0966796875, Train RMSE: 81.68914031982422, Train R^2: 0.9606558709033935\n",
      "Epoch [269/500], Loss: 6673.115234375, Train RMSE: 81.43538665771484, Train R^2: 0.9608999238314878\n",
      "Epoch [270/500], Loss: 6631.7216796875, Train RMSE: 81.19044494628906, Train R^2: 0.9611347782551485\n",
      "Epoch [271/500], Loss: 6591.888671875, Train RMSE: 80.95327758789062, Train R^2: 0.9613615048544134\n",
      "Epoch [272/500], Loss: 6553.43408203125, Train RMSE: 80.7225341796875, Train R^2: 0.9615814571347131\n",
      "Epoch [273/500], Loss: 6516.126953125, Train RMSE: 80.49943542480469, Train R^2: 0.9617935222904321\n",
      "Epoch [274/500], Loss: 6480.1591796875, Train RMSE: 80.28135681152344, Train R^2: 0.9620002537125757\n",
      "Epoch [275/500], Loss: 6445.09521484375, Train RMSE: 80.06790924072266, Train R^2: 0.962202048708553\n",
      "Epoch [276/500], Loss: 6410.869140625, Train RMSE: 79.86181640625, Train R^2: 0.9623963883745792\n",
      "Epoch [277/500], Loss: 6377.90771484375, Train RMSE: 79.66205596923828, Train R^2: 0.9625842569776932\n",
      "Epoch [278/500], Loss: 6346.0439453125, Train RMSE: 79.466064453125, Train R^2: 0.9627681410648623\n",
      "Epoch [279/500], Loss: 6314.85498046875, Train RMSE: 79.27543640136719, Train R^2: 0.9629465521347818\n",
      "Epoch [280/500], Loss: 6284.59521484375, Train RMSE: 79.09101867675781, Train R^2: 0.9631187504468411\n",
      "Epoch [281/500], Loss: 6255.38916015625, Train RMSE: 78.9096450805664, Train R^2: 0.9632877099024527\n",
      "Epoch [282/500], Loss: 6226.7314453125, Train RMSE: 78.73223114013672, Train R^2: 0.9634526067461352\n",
      "Epoch [283/500], Loss: 6198.763671875, Train RMSE: 78.55862426757812, Train R^2: 0.9636135971274543\n",
      "Epoch [284/500], Loss: 6171.4580078125, Train RMSE: 78.38912963867188, Train R^2: 0.9637704416428385\n",
      "Epoch [285/500], Loss: 6144.8564453125, Train RMSE: 78.21997833251953, Train R^2: 0.9639266284754986\n",
      "Epoch [286/500], Loss: 6118.365234375, Train RMSE: 78.05779266357422, Train R^2: 0.9640760750107421\n",
      "Epoch [287/500], Loss: 6093.017578125, Train RMSE: 77.9059829711914, Train R^2: 0.964215669238549\n",
      "Epoch [288/500], Loss: 6069.341796875, Train RMSE: 77.75602722167969, Train R^2: 0.9643532926524732\n",
      "Epoch [289/500], Loss: 6045.99951171875, Train RMSE: 77.6112289428711, Train R^2: 0.9644859342016245\n",
      "Epoch [290/500], Loss: 6023.5029296875, Train RMSE: 77.46682739257812, Train R^2: 0.964617957978061\n",
      "Epoch [291/500], Loss: 6001.10986328125, Train RMSE: 77.32738494873047, Train R^2: 0.964745224186154\n",
      "Epoch [292/500], Loss: 5979.52490234375, Train RMSE: 77.18973541259766, Train R^2: 0.9648706287794289\n",
      "Epoch [293/500], Loss: 5958.25439453125, Train RMSE: 77.05363464355469, Train R^2: 0.9649943970237338\n",
      "Epoch [294/500], Loss: 5937.2626953125, Train RMSE: 76.91887664794922, Train R^2: 0.9651167332937192\n",
      "Epoch [295/500], Loss: 5916.513671875, Train RMSE: 76.78523254394531, Train R^2: 0.9652378421967417\n",
      "Epoch [296/500], Loss: 5895.9716796875, Train RMSE: 76.65628814697266, Train R^2: 0.9653545000301396\n",
      "Epoch [297/500], Loss: 5876.185546875, Train RMSE: 76.52850341796875, Train R^2: 0.9654699056061714\n",
      "Epoch [298/500], Loss: 5856.6123046875, Train RMSE: 76.40111541748047, Train R^2: 0.9655847686995875\n",
      "Epoch [299/500], Loss: 5837.1298828125, Train RMSE: 76.27689361572266, Train R^2: 0.9656965942184831\n",
      "Epoch [300/500], Loss: 5818.16357421875, Train RMSE: 76.15401458740234, Train R^2: 0.9658070242086347\n",
      "Epoch [301/500], Loss: 5799.43408203125, Train RMSE: 76.03018951416016, Train R^2: 0.965918126382798\n",
      "Epoch [302/500], Loss: 5780.58984375, Train RMSE: 75.90794372558594, Train R^2: 0.9660276360745715\n",
      "Epoch [303/500], Loss: 5762.01611328125, Train RMSE: 75.7871322631836, Train R^2: 0.9661356909750279\n",
      "Epoch [304/500], Loss: 5743.68896484375, Train RMSE: 75.66937255859375, Train R^2: 0.966240838960601\n",
      "Epoch [305/500], Loss: 5725.85498046875, Train RMSE: 75.55482482910156, Train R^2: 0.9663429783075433\n",
      "Epoch [306/500], Loss: 5708.53076171875, Train RMSE: 75.44110107421875, Train R^2: 0.9664442221248901\n",
      "Epoch [307/500], Loss: 5691.35986328125, Train RMSE: 75.3284683227539, Train R^2: 0.9665443375590309\n",
      "Epoch [308/500], Loss: 5674.37890625, Train RMSE: 75.21713256835938, Train R^2: 0.9666431667011524\n",
      "Epoch [309/500], Loss: 5657.6162109375, Train RMSE: 75.1066665649414, Train R^2: 0.966741069884837\n",
      "Epoch [310/500], Loss: 5641.01123046875, Train RMSE: 74.99553680419922, Train R^2: 0.9668394190239751\n",
      "Epoch [311/500], Loss: 5624.330078125, Train RMSE: 74.88541412353516, Train R^2: 0.9669367286252155\n",
      "Epoch [312/500], Loss: 5607.82568359375, Train RMSE: 74.77466583251953, Train R^2: 0.9670344524797589\n",
      "Epoch [313/500], Loss: 5591.25048828125, Train RMSE: 74.66304779052734, Train R^2: 0.9671327938158466\n",
      "Epoch [314/500], Loss: 5574.5703125, Train RMSE: 74.55389404296875, Train R^2: 0.9672288340315072\n",
      "Epoch [315/500], Loss: 5558.28271484375, Train RMSE: 74.44589233398438, Train R^2: 0.9673237108401388\n",
      "Epoch [316/500], Loss: 5542.18994140625, Train RMSE: 74.3316650390625, Train R^2: 0.9674239055202489\n",
      "Epoch [317/500], Loss: 5525.19580078125, Train RMSE: 74.21452331542969, Train R^2: 0.9675265045339497\n",
      "Epoch [318/500], Loss: 5507.79443359375, Train RMSE: 74.09349822998047, Train R^2: 0.9676323272336357\n",
      "Epoch [319/500], Loss: 5489.845703125, Train RMSE: 73.98030853271484, Train R^2: 0.9677311360309835\n",
      "Epoch [320/500], Loss: 5473.08740234375, Train RMSE: 73.87522888183594, Train R^2: 0.9678227462186909\n",
      "Epoch [321/500], Loss: 5457.54931640625, Train RMSE: 73.77786254882812, Train R^2: 0.9679075061282386\n",
      "Epoch [322/500], Loss: 5443.17333984375, Train RMSE: 73.6822509765625, Train R^2: 0.9679906362474426\n",
      "Epoch [323/500], Loss: 5429.07373046875, Train RMSE: 73.58849334716797, Train R^2: 0.9680720457942757\n",
      "Epoch [324/500], Loss: 5415.265625, Train RMSE: 73.4999008178711, Train R^2: 0.9681488746616185\n",
      "Epoch [325/500], Loss: 5402.23486328125, Train RMSE: 73.41228485107422, Train R^2: 0.9682247660271106\n",
      "Epoch [326/500], Loss: 5389.36328125, Train RMSE: 73.32661437988281, Train R^2: 0.9682988844564442\n",
      "Epoch [327/500], Loss: 5376.7919921875, Train RMSE: 73.2468490600586, Train R^2: 0.9683678102926316\n",
      "Epoch [328/500], Loss: 5365.10205078125, Train RMSE: 73.1651840209961, Train R^2: 0.9684383135834491\n",
      "Epoch [329/500], Loss: 5353.1435546875, Train RMSE: 73.08522033691406, Train R^2: 0.9685072592703818\n",
      "Epoch [330/500], Loss: 5341.4501953125, Train RMSE: 73.00829315185547, Train R^2: 0.968573515761994\n",
      "Epoch [331/500], Loss: 5330.2119140625, Train RMSE: 72.9302749633789, Train R^2: 0.9686406527257507\n",
      "Epoch [332/500], Loss: 5318.8251953125, Train RMSE: 72.85430908203125, Train R^2: 0.9687059494565052\n",
      "Epoch [333/500], Loss: 5307.75048828125, Train RMSE: 72.77849578857422, Train R^2: 0.9687710457439977\n",
      "Epoch [334/500], Loss: 5296.708984375, Train RMSE: 72.70467376708984, Train R^2: 0.9688343667490724\n",
      "Epoch [335/500], Loss: 5285.9697265625, Train RMSE: 72.63009643554688, Train R^2: 0.9688982673601715\n",
      "Epoch [336/500], Loss: 5275.13134765625, Train RMSE: 72.5577621459961, Train R^2: 0.9689601918337856\n",
      "Epoch [337/500], Loss: 5264.62841796875, Train RMSE: 72.48466491699219, Train R^2: 0.9690227000997624\n",
      "Epoch [338/500], Loss: 5254.02685546875, Train RMSE: 72.41744232177734, Train R^2: 0.9690801328275745\n",
      "Epoch [339/500], Loss: 5244.28515625, Train RMSE: 72.3469467163086, Train R^2: 0.9691402976133703\n",
      "Epoch [340/500], Loss: 5234.0810546875, Train RMSE: 72.27940368652344, Train R^2: 0.969197895098336\n",
      "Epoch [341/500], Loss: 5224.31201171875, Train RMSE: 72.21221160888672, Train R^2: 0.9692551326781988\n",
      "Epoch [342/500], Loss: 5214.603515625, Train RMSE: 72.14627838134766, Train R^2: 0.9693112522548654\n",
      "Epoch [343/500], Loss: 5205.08544921875, Train RMSE: 72.07937622070312, Train R^2: 0.9693681372214891\n",
      "Epoch [344/500], Loss: 5195.4375, Train RMSE: 72.0158462524414, Train R^2: 0.9694221207754707\n",
      "Epoch [345/500], Loss: 5186.28173828125, Train RMSE: 71.95496368408203, Train R^2: 0.9694737946918859\n",
      "Epoch [346/500], Loss: 5177.51708984375, Train RMSE: 71.89452362060547, Train R^2: 0.9695250612149766\n",
      "Epoch [347/500], Loss: 5168.82080078125, Train RMSE: 71.83232879638672, Train R^2: 0.9695777626645302\n",
      "Epoch [348/500], Loss: 5159.88232421875, Train RMSE: 71.77184295654297, Train R^2: 0.9696289713892977\n",
      "Epoch [349/500], Loss: 5151.197265625, Train RMSE: 71.71143341064453, Train R^2: 0.9696800763101259\n",
      "Epoch [350/500], Loss: 5142.529296875, Train RMSE: 71.65093994140625, Train R^2: 0.9697312064126238\n",
      "Epoch [351/500], Loss: 5133.857421875, Train RMSE: 71.59224700927734, Train R^2: 0.9697807804767262\n",
      "Epoch [352/500], Loss: 5125.44921875, Train RMSE: 71.53712463378906, Train R^2: 0.9698272944670578\n",
      "Epoch [353/500], Loss: 5117.56005859375, Train RMSE: 71.4816665649414, Train R^2: 0.9698740580029299\n",
      "Epoch [354/500], Loss: 5109.62841796875, Train RMSE: 71.42887878417969, Train R^2: 0.9699185357586073\n",
      "Epoch [355/500], Loss: 5102.08447265625, Train RMSE: 71.37689208984375, Train R^2: 0.969962304877637\n",
      "Epoch [356/500], Loss: 5094.6611328125, Train RMSE: 71.32515716552734, Train R^2: 0.9700058306811707\n",
      "Epoch [357/500], Loss: 5087.27880859375, Train RMSE: 71.2723617553711, Train R^2: 0.9700502253625158\n",
      "Epoch [358/500], Loss: 5079.74951171875, Train RMSE: 71.21914672851562, Train R^2: 0.9700949299949692\n",
      "Epoch [359/500], Loss: 5072.16650390625, Train RMSE: 71.16767120361328, Train R^2: 0.9701381415081389\n",
      "Epoch [360/500], Loss: 5064.837890625, Train RMSE: 71.11605072021484, Train R^2: 0.9701814441708534\n",
      "Epoch [361/500], Loss: 5057.49365234375, Train RMSE: 71.06324768066406, Train R^2: 0.9702257129841354\n",
      "Epoch [362/500], Loss: 5049.984375, Train RMSE: 71.01113891601562, Train R^2: 0.9702693622958242\n",
      "Epoch [363/500], Loss: 5042.58154296875, Train RMSE: 70.96333312988281, Train R^2: 0.9703093796819202\n",
      "Epoch [364/500], Loss: 5035.7939453125, Train RMSE: 70.9144287109375, Train R^2: 0.9703502816239381\n",
      "Epoch [365/500], Loss: 5028.85693359375, Train RMSE: 70.8655776977539, Train R^2: 0.9703911232745052\n",
      "Epoch [366/500], Loss: 5021.9296875, Train RMSE: 70.82000732421875, Train R^2: 0.9704291911827556\n",
      "Epoch [367/500], Loss: 5015.47265625, Train RMSE: 70.77481842041016, Train R^2: 0.9704669144250853\n",
      "Epoch [368/500], Loss: 5009.07470703125, Train RMSE: 70.72820281982422, Train R^2: 0.9705058067939014\n",
      "Epoch [369/500], Loss: 5002.478515625, Train RMSE: 70.68195343017578, Train R^2: 0.9705443629151905\n",
      "Epoch [370/500], Loss: 4995.9384765625, Train RMSE: 70.63536834716797, Train R^2: 0.9705831823849963\n",
      "Epoch [371/500], Loss: 4989.35498046875, Train RMSE: 70.58930969238281, Train R^2: 0.9706215266336529\n",
      "Epoch [372/500], Loss: 4982.85107421875, Train RMSE: 70.5433578491211, Train R^2: 0.9706597682894411\n",
      "Epoch [373/500], Loss: 4976.365234375, Train RMSE: 70.4957504272461, Train R^2: 0.9706993551481722\n",
      "Epoch [374/500], Loss: 4969.65087890625, Train RMSE: 70.44905853271484, Train R^2: 0.9707381582948446\n",
      "Epoch [375/500], Loss: 4963.0693359375, Train RMSE: 70.40279388427734, Train R^2: 0.970776577509204\n",
      "Epoch [376/500], Loss: 4956.552734375, Train RMSE: 70.3552017211914, Train R^2: 0.9708160708732423\n",
      "Epoch [377/500], Loss: 4949.85498046875, Train RMSE: 70.30699157714844, Train R^2: 0.9708560590013099\n",
      "Epoch [378/500], Loss: 4943.072265625, Train RMSE: 70.26354217529297, Train R^2: 0.9708920615367772\n",
      "Epoch [379/500], Loss: 4936.9658203125, Train RMSE: 70.21487426757812, Train R^2: 0.9709323738538005\n",
      "Epoch [380/500], Loss: 4930.12890625, Train RMSE: 70.16841888427734, Train R^2: 0.9709708243956923\n",
      "Epoch [381/500], Loss: 4923.60693359375, Train RMSE: 70.12139892578125, Train R^2: 0.9710097171739653\n",
      "Epoch [382/500], Loss: 4917.0107421875, Train RMSE: 70.07749938964844, Train R^2: 0.9710460044336972\n",
      "Epoch [383/500], Loss: 4910.85595703125, Train RMSE: 70.02972412109375, Train R^2: 0.9710854660693242\n",
      "Epoch [384/500], Loss: 4904.1630859375, Train RMSE: 69.98301696777344, Train R^2: 0.9711240223065193\n",
      "Epoch [385/500], Loss: 4897.623046875, Train RMSE: 69.93610382080078, Train R^2: 0.9711627324797885\n",
      "Epoch [386/500], Loss: 4891.05810546875, Train RMSE: 69.89007568359375, Train R^2: 0.9712006722122584\n",
      "Epoch [387/500], Loss: 4884.623046875, Train RMSE: 69.84242248535156, Train R^2: 0.9712399339351139\n",
      "Epoch [388/500], Loss: 4877.96435546875, Train RMSE: 69.7953109741211, Train R^2: 0.9712787209903934\n",
      "Epoch [389/500], Loss: 4871.38525390625, Train RMSE: 69.75019836425781, Train R^2: 0.9713158351530888\n",
      "Epoch [390/500], Loss: 4865.08984375, Train RMSE: 69.7025375366211, Train R^2: 0.971355020593177\n",
      "Epoch [391/500], Loss: 4858.44384765625, Train RMSE: 69.65694427490234, Train R^2: 0.9713924839200742\n",
      "Epoch [392/500], Loss: 4852.08984375, Train RMSE: 69.61128997802734, Train R^2: 0.9714299716758384\n",
      "Epoch [393/500], Loss: 4845.73095703125, Train RMSE: 69.56659698486328, Train R^2: 0.9714666453026943\n",
      "Epoch [394/500], Loss: 4839.51123046875, Train RMSE: 69.5221939086914, Train R^2: 0.9715030552841962\n",
      "Epoch [395/500], Loss: 4833.33544921875, Train RMSE: 69.48114776611328, Train R^2: 0.9715367021353019\n",
      "Epoch [396/500], Loss: 4827.62939453125, Train RMSE: 69.44066619873047, Train R^2: 0.9715698599613237\n",
      "Epoch [397/500], Loss: 4822.005859375, Train RMSE: 69.40345001220703, Train R^2: 0.9716003221233263\n",
      "Epoch [398/500], Loss: 4816.83837890625, Train RMSE: 69.36299896240234, Train R^2: 0.971633421072271\n",
      "Epoch [399/500], Loss: 4811.224609375, Train RMSE: 69.33049774169922, Train R^2: 0.9716599881692254\n",
      "Epoch [400/500], Loss: 4806.71875, Train RMSE: 69.29801177978516, Train R^2: 0.9716865467952981\n",
      "Epoch [401/500], Loss: 4802.21435546875, Train RMSE: 69.26042175292969, Train R^2: 0.9717172525218931\n",
      "Epoch [402/500], Loss: 4797.005859375, Train RMSE: 69.21664428710938, Train R^2: 0.9717530011093973\n",
      "Epoch [403/500], Loss: 4790.94287109375, Train RMSE: 69.16964721679688, Train R^2: 0.9717913409801272\n",
      "Epoch [404/500], Loss: 4784.4404296875, Train RMSE: 69.13150024414062, Train R^2: 0.971822450160259\n",
      "Epoch [405/500], Loss: 4779.1640625, Train RMSE: 69.09580993652344, Train R^2: 0.971851536856581\n",
      "Epoch [406/500], Loss: 4774.23046875, Train RMSE: 69.05776977539062, Train R^2: 0.971882517056454\n",
      "Epoch [407/500], Loss: 4768.97607421875, Train RMSE: 69.02532196044922, Train R^2: 0.9719089365299767\n",
      "Epoch [408/500], Loss: 4764.4951171875, Train RMSE: 68.99447631835938, Train R^2: 0.9719340374193767\n",
      "Epoch [409/500], Loss: 4760.23779296875, Train RMSE: 68.9622573852539, Train R^2: 0.971960236927654\n",
      "Epoch [410/500], Loss: 4755.7939453125, Train RMSE: 68.92948150634766, Train R^2: 0.9719868893429707\n",
      "Epoch [411/500], Loss: 4751.27392578125, Train RMSE: 68.89285278320312, Train R^2: 0.9720166541232699\n",
      "Epoch [412/500], Loss: 4746.22509765625, Train RMSE: 68.85696411132812, Train R^2: 0.9720458019469566\n",
      "Epoch [413/500], Loss: 4741.28173828125, Train RMSE: 68.81753540039062, Train R^2: 0.9720778052989718\n",
      "Epoch [414/500], Loss: 4735.853515625, Train RMSE: 68.77976989746094, Train R^2: 0.9721084435331578\n",
      "Epoch [415/500], Loss: 4730.65673828125, Train RMSE: 68.73792266845703, Train R^2: 0.9721423762227738\n",
      "Epoch [416/500], Loss: 4724.90087890625, Train RMSE: 68.7035140991211, Train R^2: 0.9721702530604397\n",
      "Epoch [417/500], Loss: 4720.17333984375, Train RMSE: 68.67005157470703, Train R^2: 0.9721973615788331\n",
      "Epoch [418/500], Loss: 4715.5751953125, Train RMSE: 68.63876342773438, Train R^2: 0.9722226864567001\n",
      "Epoch [419/500], Loss: 4711.2802734375, Train RMSE: 68.60247802734375, Train R^2: 0.9722520467942323\n",
      "Epoch [420/500], Loss: 4706.30029296875, Train RMSE: 68.56782531738281, Train R^2: 0.9722800773664784\n",
      "Epoch [421/500], Loss: 4701.54638671875, Train RMSE: 68.53176879882812, Train R^2: 0.9723092173053687\n",
      "Epoch [422/500], Loss: 4696.60400390625, Train RMSE: 68.4959945678711, Train R^2: 0.9723381201176751\n",
      "Epoch [423/500], Loss: 4691.70166015625, Train RMSE: 68.45955657958984, Train R^2: 0.972367544558698\n",
      "Epoch [424/500], Loss: 4686.7109375, Train RMSE: 68.42251586914062, Train R^2: 0.9723974403103836\n",
      "Epoch [425/500], Loss: 4681.64013671875, Train RMSE: 68.38525390625, Train R^2: 0.972427491389199\n",
      "Epoch [426/500], Loss: 4676.54345703125, Train RMSE: 68.3564682006836, Train R^2: 0.9724507018013171\n",
      "Epoch [427/500], Loss: 4672.607421875, Train RMSE: 68.3219223022461, Train R^2: 0.9724785402709265\n",
      "Epoch [428/500], Loss: 4667.884765625, Train RMSE: 68.2874984741211, Train R^2: 0.9725062692288897\n",
      "Epoch [429/500], Loss: 4663.18212890625, Train RMSE: 68.24919128417969, Train R^2: 0.9725371084614628\n",
      "Epoch [430/500], Loss: 4657.951171875, Train RMSE: 68.2143783569336, Train R^2: 0.9725651121833063\n",
      "Epoch [431/500], Loss: 4653.20166015625, Train RMSE: 68.1826171875, Train R^2: 0.9725906531137376\n",
      "Epoch [432/500], Loss: 4648.8701171875, Train RMSE: 68.14677429199219, Train R^2: 0.9726194700921973\n",
      "Epoch [433/500], Loss: 4643.982421875, Train RMSE: 68.11082458496094, Train R^2: 0.9726483528717367\n",
      "Epoch [434/500], Loss: 4639.0830078125, Train RMSE: 68.07618713378906, Train R^2: 0.9726761567049654\n",
      "Epoch [435/500], Loss: 4634.3671875, Train RMSE: 68.03944396972656, Train R^2: 0.9727056485205076\n",
      "Epoch [436/500], Loss: 4629.365234375, Train RMSE: 68.00576782226562, Train R^2: 0.9727326546742115\n",
      "Epoch [437/500], Loss: 4624.78466796875, Train RMSE: 67.97293853759766, Train R^2: 0.972758976708378\n",
      "Epoch [438/500], Loss: 4620.32080078125, Train RMSE: 67.93893432617188, Train R^2: 0.9727862304463442\n",
      "Epoch [439/500], Loss: 4615.69775390625, Train RMSE: 67.90607452392578, Train R^2: 0.9728125409882231\n",
      "Epoch [440/500], Loss: 4611.2353515625, Train RMSE: 67.87384796142578, Train R^2: 0.9728383469307214\n",
      "Epoch [441/500], Loss: 4606.85888671875, Train RMSE: 67.83882904052734, Train R^2: 0.9728663610893145\n",
      "Epoch [442/500], Loss: 4602.10693359375, Train RMSE: 67.80616760253906, Train R^2: 0.9728924854688721\n",
      "Epoch [443/500], Loss: 4597.67626953125, Train RMSE: 67.77323150634766, Train R^2: 0.9729188175642676\n",
      "Epoch [444/500], Loss: 4593.2099609375, Train RMSE: 67.73808288574219, Train R^2: 0.9729468924287401\n",
      "Epoch [445/500], Loss: 4588.44873046875, Train RMSE: 67.70065307617188, Train R^2: 0.9729767812743052\n",
      "Epoch [446/500], Loss: 4583.37890625, Train RMSE: 67.66639709472656, Train R^2: 0.9730041212308302\n",
      "Epoch [447/500], Loss: 4578.7421875, Train RMSE: 67.63200378417969, Train R^2: 0.9730315589142033\n",
      "Epoch [448/500], Loss: 4574.08837890625, Train RMSE: 67.59661865234375, Train R^2: 0.973059771313157\n",
      "Epoch [449/500], Loss: 4569.30322265625, Train RMSE: 67.56450653076172, Train R^2: 0.9730853638196647\n",
      "Epoch [450/500], Loss: 4564.96240234375, Train RMSE: 67.53067016601562, Train R^2: 0.9731123147898072\n",
      "Epoch [451/500], Loss: 4560.39111328125, Train RMSE: 67.49705505371094, Train R^2: 0.9731390737707263\n",
      "Epoch [452/500], Loss: 4555.85302734375, Train RMSE: 67.46368408203125, Train R^2: 0.9731656311345722\n",
      "Epoch [453/500], Loss: 4551.34814453125, Train RMSE: 67.42972564697266, Train R^2: 0.9731926405801458\n",
      "Epoch [454/500], Loss: 4546.767578125, Train RMSE: 67.39664459228516, Train R^2: 0.9732189286591746\n",
      "Epoch [455/500], Loss: 4542.30908203125, Train RMSE: 67.3611068725586, Train R^2: 0.9732471672884592\n",
      "Epoch [456/500], Loss: 4537.51904296875, Train RMSE: 67.32894897460938, Train R^2: 0.9732727086953027\n",
      "Epoch [457/500], Loss: 4533.18701171875, Train RMSE: 67.29988098144531, Train R^2: 0.9732957800144661\n",
      "Epoch [458/500], Loss: 4529.2734375, Train RMSE: 67.26068878173828, Train R^2: 0.9733268746903978\n",
      "Epoch [459/500], Loss: 4524.0, Train RMSE: 67.22731018066406, Train R^2: 0.9733533355681676\n",
      "Epoch [460/500], Loss: 4519.51220703125, Train RMSE: 67.19473266601562, Train R^2: 0.9733791572675714\n",
      "Epoch [461/500], Loss: 4515.13232421875, Train RMSE: 67.16051483154297, Train R^2: 0.9734062632286524\n",
      "Epoch [462/500], Loss: 4510.53515625, Train RMSE: 67.12835693359375, Train R^2: 0.9734317238758736\n",
      "Epoch [463/500], Loss: 4506.216796875, Train RMSE: 67.09404754638672, Train R^2: 0.9734588764062568\n",
      "Epoch [464/500], Loss: 4501.611328125, Train RMSE: 67.05941009521484, Train R^2: 0.9734862761632296\n",
      "Epoch [465/500], Loss: 4496.9638671875, Train RMSE: 67.02986145019531, Train R^2: 0.973509632744448\n",
      "Epoch [466/500], Loss: 4493.00244140625, Train RMSE: 66.9959945678711, Train R^2: 0.9735363955616576\n",
      "Epoch [467/500], Loss: 4488.46337890625, Train RMSE: 66.96345520019531, Train R^2: 0.9735620973755768\n",
      "Epoch [468/500], Loss: 4484.1044921875, Train RMSE: 66.93206787109375, Train R^2: 0.9735868746319388\n",
      "Epoch [469/500], Loss: 4479.90185546875, Train RMSE: 66.90089416503906, Train R^2: 0.9736114742950666\n",
      "Epoch [470/500], Loss: 4475.7294921875, Train RMSE: 66.86561584472656, Train R^2: 0.9736392987114859\n",
      "Epoch [471/500], Loss: 4471.009765625, Train RMSE: 66.83418273925781, Train R^2: 0.9736640770511285\n",
      "Epoch [472/500], Loss: 4466.80712890625, Train RMSE: 66.79916381835938, Train R^2: 0.9736916672599076\n",
      "Epoch [473/500], Loss: 4462.1279296875, Train RMSE: 66.76878356933594, Train R^2: 0.9737155870197081\n",
      "Epoch [474/500], Loss: 4458.07080078125, Train RMSE: 66.73584747314453, Train R^2: 0.9737415180463822\n",
      "Epoch [475/500], Loss: 4453.6728515625, Train RMSE: 66.70207977294922, Train R^2: 0.9737680855268959\n",
      "Epoch [476/500], Loss: 4449.16650390625, Train RMSE: 66.66783142089844, Train R^2: 0.9737950092076728\n",
      "Epoch [477/500], Loss: 4444.60009765625, Train RMSE: 66.63445281982422, Train R^2: 0.9738212452825474\n",
      "Epoch [478/500], Loss: 4440.150390625, Train RMSE: 66.60066223144531, Train R^2: 0.9738477870114135\n",
      "Epoch [479/500], Loss: 4435.6484375, Train RMSE: 66.56478881835938, Train R^2: 0.9738759603297865\n",
      "Epoch [480/500], Loss: 4430.8701171875, Train RMSE: 66.53400421142578, Train R^2: 0.9739001136519807\n",
      "Epoch [481/500], Loss: 4426.7734375, Train RMSE: 66.50349426269531, Train R^2: 0.9739240441561864\n",
      "Epoch [482/500], Loss: 4422.71435546875, Train RMSE: 66.46788787841797, Train R^2: 0.9739519613050279\n",
      "Epoch [483/500], Loss: 4417.97998046875, Train RMSE: 66.43151092529297, Train R^2: 0.9739804612062568\n",
      "Epoch [484/500], Loss: 4413.14599609375, Train RMSE: 66.39810180664062, Train R^2: 0.9740066303043502\n",
      "Epoch [485/500], Loss: 4408.70751953125, Train RMSE: 66.36290740966797, Train R^2: 0.9740341764511096\n",
      "Epoch [486/500], Loss: 4404.03564453125, Train RMSE: 66.33352661132812, Train R^2: 0.9740571557892113\n",
      "Epoch [487/500], Loss: 4400.1376953125, Train RMSE: 66.29862976074219, Train R^2: 0.9740844470543762\n",
      "Epoch [488/500], Loss: 4395.50927734375, Train RMSE: 66.26434326171875, Train R^2: 0.9741112483587752\n",
      "Epoch [489/500], Loss: 4390.962890625, Train RMSE: 66.2370376586914, Train R^2: 0.9741325830472083\n",
      "Epoch [490/500], Loss: 4387.34423828125, Train RMSE: 66.20494079589844, Train R^2: 0.9741576463776546\n",
      "Epoch [491/500], Loss: 4383.09375, Train RMSE: 66.17135620117188, Train R^2: 0.9741838560304924\n",
      "Epoch [492/500], Loss: 4378.64794921875, Train RMSE: 66.13932800292969, Train R^2: 0.9742088367524347\n",
      "Epoch [493/500], Loss: 4374.4111328125, Train RMSE: 66.10700988769531, Train R^2: 0.9742340358111027\n",
      "Epoch [494/500], Loss: 4370.1376953125, Train RMSE: 66.07442474365234, Train R^2: 0.9742594363495496\n",
      "Epoch [495/500], Loss: 4365.8291015625, Train RMSE: 66.04338073730469, Train R^2: 0.9742836124369902\n",
      "Epoch [496/500], Loss: 4361.728515625, Train RMSE: 66.01541900634766, Train R^2: 0.97430538440687\n",
      "Epoch [497/500], Loss: 4358.0361328125, Train RMSE: 65.97715759277344, Train R^2: 0.9743351669268369\n",
      "Epoch [498/500], Loss: 4352.98486328125, Train RMSE: 65.94601440429688, Train R^2: 0.9743593901347722\n",
      "Epoch [499/500], Loss: 4348.87646484375, Train RMSE: 65.91488647460938, Train R^2: 0.9743835875980886\n",
      "Epoch [500/500], Loss: 4344.7724609375, Train RMSE: 65.8868637084961, Train R^2: 0.9744053630433599\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=4)\n",
    "data_normalized_pca = pca.fit_transform(data_normalized)\n",
    "print(data_normalized_pca.shape)\n",
    "\n",
    "class Net_PCA(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net_PCA, self).__init__()\n",
    "        self.fc1 = nn.Linear(4, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        intermediate_features = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(intermediate_features))  # 提取中间层特征\n",
    "        x = self.fc3(x)\n",
    "        return x, intermediate_features\n",
    "\n",
    "\n",
    "# 假设data是包含特征数据的数组，labels是包含对应标签的数组\n",
    "data_train_pca, data_test_pca, labels_train_pca, labels_test_pca = train_test_split(data_normalized_pca, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "data_train_tensor_pca = torch.tensor(data_train_pca, dtype=torch.float32).clone().detach()\n",
    "labels_train_tensor_pca = torch.tensor(labels_train_pca, dtype=torch.float32).clone().detach()\n",
    "data_test_tensor_pca = torch.tensor(data_test_pca, dtype=torch.float32).clone().detach()\n",
    "labels_test_tensor_pca = torch.tensor(labels_test_pca, dtype=torch.float32).clone().detach()\n",
    "\n",
    "# 准备数据\n",
    "train_dataset_pca = CustomDataset(data_train_pca, labels_train_pca)\n",
    "train_dataloader_pca = DataLoader(train_dataset_pca, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# 存储最后一次训练的隐藏层特征\n",
    "last_hidden_features = None\n",
    "\n",
    "# 设置超参数\n",
    "\n",
    "learning_rate = 0.001\n",
    "batch_size = 57\n",
    "num_epochs = 500\n",
    "\n",
    "# 初始化模型和损失函数\n",
    "model_pca = Net_PCA()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model_pca.parameters(), lr=learning_rate)\n",
    "\n",
    "# 训练神经网络模型\n",
    "total_step = len(train_dataloader_pca)\n",
    "for epoch in range(num_epochs):\n",
    "    for inputs, targets in train_dataloader_pca:\n",
    "        outputs, intermediate_features = model_pca(inputs)\n",
    "        loss = criterion(outputs, targets.unsqueeze(1))  # 将目标值展开为列向量进行比较\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "        # 计算训练集上的预测精度差\n",
    "        train_outputs,_ = model_pca(data_train_tensor_pca)\n",
    "        train_rmse = mean_squared_error(labels_train_tensor_pca, train_outputs.detach().numpy(), squared=False)\n",
    "        train_r2 = r2_score(labels_train_tensor_pca, train_outputs.detach().numpy())\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item()}, Train RMSE: {train_rmse}, Train R^2: {train_r2}\")\n",
    "        # 提取最后一次训练的隐藏层特征\n",
    "    last_hidden_features = intermediate_features.detach().numpy().astype(np.float64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE: 30.724145442702717\n",
      "Test R2: -2.1175091576354363\n"
     ]
    }
   ],
   "source": [
    "# 直接通过神经网路进行预测\n",
    "model_path_pca = \"model_pca.pth\"\n",
    "# 将模型的参数保存到文件中\n",
    "torch.save(model_pca.state_dict(), model_path_pca)\n",
    "model2 = Net_PCA()\n",
    "model2.load_state_dict(torch.load(model_path_pca))\n",
    "\n",
    "test_inputs = data_test_tensor_pca\n",
    "test_outputs,_ = model2(test_inputs)\n",
    "test_outputs = test_outputs.detach().numpy()\n",
    "test_outputs = np.squeeze(test_outputs)\n",
    "labels_test = np.squeeze(labels_test_pca)\n",
    "\n",
    "test_rmse = mean_squared_error(labels_test, test_outputs, squared=False)\n",
    "test_r2 = r2_score(labels_test_pca, test_outputs)\n",
    "print(\"Test RMSE:\", test_rmse)\n",
    "print(\"Test R2:\",test_r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
